---
title: "Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores"
date: 2018-01-14T11:47:23-06:00
author: "Yanyu Liang"
tags: ["predict disease risk", 'ld pruning']
categories: ["research paper - method"]
draft: false
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
---


<div id="TOC">
<ul>
<li><a href="#meta-data-of-reading">Meta data of reading</a></li>
<li><a href="#in-brief">In brief</a></li>
<li><a href="#the-model">The model</a></li>
<li><a href="#some-derivations">Some derivations</a></li>
</ul>
</div>

<div id="meta-data-of-reading" class="section level1">
<h1>Meta data of reading</h1>
<ul>
<li><strong>Journal</strong>: AJHG</li>
<li><strong>Year</strong>: 2015</li>
<li><strong>DOI</strong>: 10.1016/j.ajhg.2015.09.001</li>
</ul>
<p><span class="math display">\[
\newcommand\E{\text{E}}
\newcommand\var{\text{Var}}
\newcommand\diag{\text{diag}}
\]</span></p>
</div>
<div id="in-brief" class="section level1">
<h1>In brief</h1>
<p>This paper proposed a method to estimate disease risk using GWAS summary statistics. The previous method is to do LD pruning and p-value thresholding but it potentially drop out informative SNPs and leads to lack of explained heritability. This paper proposed a Bayesian polygenic risk scores (PRS), LDpred, that estimates the posterior mean causal effect sizes from GWAS results by assigning prior on genetic architecture of the disease and the LD information from a reference panel.</p>
</div>
<div id="the-model" class="section level1">
<h1>The model</h1>
Assume that phenotype <span class="math inline">\(Y\)</span>, genotype <span class="math inline">\(X\)</span> are both standardized.
<span class="math display">\[\begin{align*}
  Y &amp;= \sum_{i = 1}^M X_i \beta_i + \epsilon \\
  \hat\beta_i &amp;= X_i&#39; Y / N
\end{align*}\]</span>
<p>, where <span class="math inline">\(\hat\beta_i\)</span> is <strong>marginal</strong> effect estimate.</p>
<p>If <span class="math inline">\(z_i\)</span> is provided instead, <span class="math inline">\(\hat\beta_i = s_i(z_i / \sqrt{N})\)</span>, where <span class="math inline">\(s_i\)</span> is the sign of <span class="math inline">\(z_i\)</span>.</p>
<div id="unadjusted-prs" class="section level2">
<h2>Unadjusted PRS</h2>
<span class="math display">\[\begin{align*}
  S_i &amp;= \sum_{j = 1}^M X_{ji} \hat\beta_j
\end{align*}\]</span>
<p>, where <span class="math inline">\(S_i\)</span> is the predicted diseases risk of <span class="math inline">\(i\)</span>th individual. Note that here all SNPs are used to predict the outcome and marginal effects are used which implicitly assume that every loci is not correlated with each other.</p>
</div>
<div id="pt" class="section level2">
<h2>P+T</h2>
<p>In practice, LD pruning and p-value thresholding improve the accuracy. For instance, LD pruning with <span class="math inline">\(r^2 &gt; 0.2\)</span> and p-value thresholding with various values over a grid to optimize the performance (validation data should be available).</p>
</div>
<div id="bpred" class="section level2">
<h2>Bpred</h2>
In the sense of minimizing prediction error variance, the posterior mean prediction is optimal linear prediction (see <a href="https://en.wikipedia.org/wiki/Bayes_estimator#Minimum_mean_square_error_estimation">Bayes estimator on wikipedia</a>). Namely,
<span class="math display">\[\begin{align}
  \E(Y | \tilde\beta, \hat{D}) &amp;= \sum_{i = 1}^M X_i&#39; \E(\beta_i | \tilde\beta, \hat{D}) \label{eq:posterior-mean-estimate}
\end{align}\]</span>
<p>, where <span class="math inline">\(\tilde\beta\)</span> is the marginal least-squares estimate and <span class="math inline">\(\hat{D}\)</span> is observed LD in the training data.</p>
Under standardized <span class="math inline">\(X\)</span>, <span class="math inline">\(\var(Y) = h_g^2 \Theta + (1 - h_g^2) I\)</span> because
<span class="math display">\[\begin{align*}
  \var(Y) &amp;:= \var_{\beta, \epsilon}(Y) \\
  &amp;= \var_{\beta, \epsilon}(X\beta + \epsilon) \\
  &amp;= \var_{\beta}(X\beta) + \var_{\epsilon}(\epsilon) \quad \text{, since $\beta$, $\epsilon$ are independent} \\
  &amp;= h_g^2 (XX&#39; / M) + (1 - h_g^2) I \quad\text{, here it seems that one assumption is $\beta$ iid}\\
  &amp;= h_g^2 \Theta + (1 - h_g^2) I
\end{align*}\]</span>
If all samples are independent and marks are unlinked with <span class="math inline">\(\beta_i \sim_{iid} \mathcal{N}(0, h_g^2 / M)\)</span> (Gaussian infinitesimal prior), then
<span class="math display">\[\begin{align*}
  \E(\beta_i | \tilde{\beta}) &amp;= \E(\beta_i | \tilde\beta_i) = \frac{h_g^2}{h_g^2 + M / N} \tilde\beta_i \\
  \text{cor}(\hat{Y}_\text{PRS}, Y) &amp;= \frac{h_g^2}{h_g^2 + M / N} h_g^2
\end{align*}\]</span>
If a Gaussian mixture is assumed instead, namely
<span class="math display">\[\begin{align*}
  \beta_i &amp;\sim \begin{cases} \mathcal{N}(0, h_g^2 / (h_g^2 + Mp/N)) &amp; \text{prob } p \\ 0 &amp; \text{prob } (1 - p)\end{cases}
\end{align*}\]</span>
, then
<span class="math display">\[\begin{align*}
  \E(\beta_i | \tilde\beta) &amp;= \frac{h_g^2}{h_g^2 + Mp / N} \bar{p}_i \tilde\beta_i
\end{align*}\]</span>
</div>
<div id="ldpred" class="section level2">
<h2>LDpred</h2>
If allowing loci to be linked but distant loci are not linked, under Gaussian infinitesimal prior
<span class="math display">\[\begin{align*}
  \E(\beta^l | \tilde\beta^l, D) &amp;\approx (\frac{M}{Nh_g^2}I + D_l)^{-1} \tilde\beta^l
\end{align*}\]</span>
<p>For Gaussian mixture prior, the posterior mean is obtained by MCMC Gibbs sampler.</p>
</div>
</div>
<div id="some-derivations" class="section level1">
<h1>Some derivations</h1>
<p>This section sketches some derivations of the results listed above. The term ‘unlinked’ means that the single-locus model captures the proper effect which is the same as the multi-loci model. The term ‘infinitesimal’ means that the effect <span class="math inline">\(\beta_i\)</span> with measure zero at zero.</p>
<div id="unlinked-markers-and-infinitesimal-effect" class="section level2">
<h2>Unlinked markers and infinitesimal effect</h2>
Suppose <span class="math inline">\(\beta_i \sim \mathcal{N}(0, h^2 / M)\)</span> (then variance of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(h^2\)</span>).
<span class="math display">\[\begin{align*}
  \tilde\beta_i | \beta_i &amp;\sim \mathcal{N}(\beta_i, \frac{1}{N}(1 - h^2 / M)) \\
  &amp;\quad\text{, since &#39;unlinked&#39;, one loci explains $h^2/M$ variance}\\
  \beta_i | \tilde\beta_i &amp;\sim \mathcal{N}(\frac{1}{1 + M / h^2N}\tilde\beta_i, \frac{1}{N}\frac{1}{1 + M / h^2N}) \\
  &amp;\quad\text{, derived from prior and conditional with large $M$}
\end{align*}\]</span>
</div>
<div id="unlinked-markers-and-non-infinitesimal-effect" class="section level2">
<h2>Unlinked markers and non-infinitesimal effect</h2>
Suppose
<span class="math display">\[\begin{align}
  \beta_i &amp;\sim \begin{cases}
    \delta_0(\beta_i) &amp; \text{, with prob $1 - p$} \\
    \mathcal{N}(0, h^2 / Mp)(\beta_i) &amp; \text{, with prob $p$}
  \end{cases} \label{eq:auxiliary}
\end{align}\]</span>
<p>, where <span class="math inline">\(\var(\beta_i)\)</span> is kept to be <span class="math inline">\(h^2 / M\)</span></p>
Let’s first introduce auxiliary variable <span class="math inline">\(Z_i \sim \text{Ber}(p)\)</span> and
<span class="math display">\[\begin{align*}
  f(\beta_i| Z_i) &amp;= (1 - Z_i) \delta_0(\beta_i) + Z_i\mathcal{N}(0, h^2/Mp)(\beta_i)
\end{align*}\]</span>
, then <span class="math inline">\(f(\beta_i)\)</span> is kept to be . The independences of variables is <embed src="/notebook/posts/bjarni-ajhg-2015_files/figure-html/unnamed-chunk-1-1.pdf" width="288" style="display: block; margin: auto;" type="application/pdf" /> Similarly,
<span class="math display">\[\begin{align*}
  \tilde\beta_i | \beta_i &amp;\sim \mathcal{N}(\beta_i, \frac{1}{N}(1 - h^2 / M)) \\
  &amp;\quad\text{, since $\var(\beta_i)$ is kept the same as before} \\
  f(\beta_i | \tilde\beta_i, Z_i = 0) &amp;\propto f(\beta_i | Z_i = 0) \times f(\tilde\beta_i | \beta_i) \\
  &amp;\propto \delta_0(\beta_i) \times \mathcal{N}(\beta_i, \sigma_1^2)(\tilde\beta_i) \\
  &amp;\propto \delta_0(\beta_i) \\
  f(\beta_i | \tilde\beta_i, Z_i = 1) &amp;\propto \mathcal{N}(0, \sigma_2^2)(\beta_i) \times \mathcal{N}(\beta_i, \sigma_1^2)(\tilde\beta_i) \\
  &amp;\propto \mathcal{N}(k \tilde\beta_i, k / N)
\end{align*}\]</span>
<p>, where <span class="math inline">\(\sigma_1^2 = \frac{1}{N}(1 - h^2 / M), \sigma_2^2 = \var(\beta_i), k = \sigma_2^2 / (\sigma_1^2 + \sigma_2^2)\)</span>.</p>
The posterior mean is
<span class="math display">\[\begin{align*}
  \E(\beta_i | \tilde\beta_i) &amp;= \E_{Z_i | \tilde\beta_i}[\E(\beta_i | \tilde\beta_i, Z_i)] \\
  &amp;= \E_{Z_i | \tilde\beta_i}[Z_i \times k\tilde\beta_i + (1 - Z_i) \times 0] \\
  &amp;= k\bar{p}_i\tilde\beta_i \quad\text{, where $\bar{p}_i = \E(Z_i | \tilde\beta_i)$}
\end{align*}\]</span>
</div>
<div id="linked-markers-and-infinitesimal-effect" class="section level2">
<h2>Linked markers and infinitesimal effect</h2>
Suppose the <span class="math inline">\(i\)</span>th region contains <span class="math inline">\(N_i\)</span> SNPs and the SNPs are linked with each other within the region but no correlation outside the region. Then, the marginal effect <span class="math inline">\(\tilde\beta_i \in \mathbb{R}^{M_i}\)</span> conditional on <span class="math inline">\(\beta_i\)</span> is
<span class="math display">\[\begin{align*}
  \E(\tilde\beta_i | \beta_i) &amp;= D_i \beta_i \\
  \var(\tilde\beta_i | \beta_i) &amp;= \frac{1}{N} (1 - h_i^2) D_i  \\
  \text{, where} &amp; \\
  D_i &amp;= X_i&#39;X_i / N \\
  \tilde\beta_i &amp;= X_i&#39;Y / N \\
  h_i^2 &amp;\text{is} \text{ phenotype variance explained by the $i$th region}
\end{align*}\]</span>
<p>, which can be deduced by replacing <span class="math inline">\(Y\)</span> with <span class="math inline">\(X_i\beta_i + \epsilon_i\)</span>.</p>
Then,
<span class="math display">\[\begin{align*}
  \tilde\beta_i | \beta_i &amp;\sim \mathcal{N}(D_i\beta_i, \Sigma_1) \\
  \beta_i &amp;\sim \mathcal{N}(0, \Sigma_2) \\
  \text{, where} &amp; \\
  \Sigma_1 &amp;= \frac{1}{N} (1 - h_i^2) D_i \\
  \Sigma_2 &amp;= (h_i^2 / M)I
\end{align*}\]</span>
, so that we can get the posterior distribution
<span class="math display">\[\begin{align*}
  \beta_i | \tilde\beta_i &amp;\sim \mathcal{N}(A\tilde\beta_i, \Sigma) \\
  \text{where, }&amp; \\
  \Sigma &amp;= (\Sigma_2^{-1} + D_i\Sigma_1^{-1}D_i)^{-1} = \frac{1}{N}(\frac{1}{1 - h_i^2}D_i + \frac{M}{Nh_i^2}I)^{-1}\\
  A &amp;= \Sigma D_i \Sigma_1^{-1} \\
  \text{when } &amp; h^2 \approx 0 \\
  A &amp;\approx (\frac{1}{1 - h_i^2}D_i + \frac{M}{Nh_i^2}I)^{-1}
\end{align*}\]</span>
</div>
<div id="linked-markers-and-non-infinitesimal-effect" class="section level2">
<h2>Linked markers and non-infinitesimal effect</h2>
The goal is to obtain posterior mean and the paper proposed to use a Gibbs sampler by sampling <span class="math inline">\(\beta_i | \tilde\beta, \beta_{-i}\)</span>. Specifically, the sampling can be decomposed as <span class="math inline">\(Z_i | \tilde\beta, \beta_{-i}\)</span> and <span class="math inline">\(\beta_i | \tilde\beta, \beta_{-i}, Z_i\)</span>. These two variables can be approximately sampled as follow
<span class="math display">\[\begin{align*}
  \Pr(Z_i | \tilde\beta, \beta_{-i}) &amp;\approx \Pr(Z_i | \tilde\beta_i, \beta_{-i}) \\
  f(\beta_i | \tilde\beta, \beta_{-i}, Z_i = 1) &amp;\approx f(\beta_i | \tilde\beta_i, \beta_{-i}, Z_i = 1) \\
\end{align*}\]</span>
<blockquote>
<p>Comment: At first, I did not get the idea of this approximation. After reading their <a href="https://github.com/bvilhjal/ldpred/blob/master/ldpred/LDpred.py">code</a> on Gibbs sampler, I kind of get the point.</p>
</blockquote>
<p>Consider two-loci case in term of the graphical model <embed src="/notebook/posts/bjarni-ajhg-2015_files/figure-html/unnamed-chunk-2-1.pdf" width="288" style="display: block; margin: auto;" type="application/pdf" /></p>
Since <span class="math inline">\(\tilde\beta_i\)</span> is collidar node, <span class="math inline">\(f(\beta_i|\tilde\beta, \beta_{-i}) \ne f(\beta_i | \tilde\beta_i, \beta_{-i})\)</span>. To obtain <span class="math inline">\(f(\beta_i | \tilde\beta, \beta_{-i})\)</span>
<span class="math display">\[\begin{align*}
  f(\beta_i | \tilde\beta, \beta_{-i}, Z_i = 1) &amp;= \int f(\tilde\beta | \beta) \prod_{j \ne i} f(\beta_j) \mathcal{N}(0, \sigma^2)(\beta_i) d\beta_i \\
  &amp;= \prod_{j \ne i} f(\beta_j) \frac{1}{\sqrt{2\pi|\Sigma|}}\int \exp\{-\frac{1}{2}(\tilde\beta - D\beta)&#39; \Sigma^{-1} (\tilde\beta - D\beta)\} \exp\{-\frac{1}{2}\beta_i^2\sigma^{-2}\} d\beta_i \\
  &amp;= \prod_{j \ne i} f(\beta_j) \frac{1}{\sqrt{2\pi|\Sigma|}} \times \sqrt{2\pi(\omega + \hat\beta&#39;\Omega\hat\beta)} \int \exp\{\hat\beta&#39;X\hat\beta)\} \quad\text{, where $X = \Omega - \Omega(\Omega + \Sigma_2)^{-1}\Omega$}
\end{align*}\]</span>
<p>, where <span class="math inline">\(\hat\beta = \tilde\beta - \beta|_{\beta_i = 0}\)</span> and <span class="math inline">\(\Omega = \Sigma^{-1}, \omega = \sigma^{-2}, \Sigma_2 = \diag(1, ..., \omega, 1, ..., 1)\)</span> at the <span class="math inline">\(i\)</span>th entry. To compute <span class="math inline">\(|\Sigma|\)</span> is computationally intensive. To avoid it, we need to work with univariate normal instead. Therefore, the paper proposed to use <span class="math inline">\(f(\beta_i|\tilde\beta_i, \beta_{-i})\)</span> to approximate it since <span class="math inline">\(\tilde\beta_i | \beta\)</span> is simply <span class="math inline">\(\mathcal{N}(\beta_i, (1 - h^2) / N)\)</span> (directly get from <span class="math inline">\(\tilde\beta|\beta\)</span>).</p>
</div>
</div>

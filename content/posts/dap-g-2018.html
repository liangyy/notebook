---
title: "Dap-G"
date: 2018-07-12T10:04:16-05:00
author: "Yanyu Liang"
tags: ["special-series-rotation"]
categories: [""]
draft: false
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
---


<div id="TOC">
<ul>
<li><a href="#meta-data-of-reading">Meta data of reading</a></li>
<li><a href="#model">Model</a></li>
<li><a href="#inference-procedure">Inference procedure</a></li>
<li><a href="#computation">Computation</a></li>
<li><a href="#dap-g">DAP-G</a></li>
</ul>
</div>

<div id="meta-data-of-reading" class="section level1">
<h1>Meta data of reading</h1>
<div id="reference-1" class="section level2">
<h2>Reference 1</h2>
<ul>
<li><strong>Journal</strong>: AJHG</li>
<li><strong>Year</strong>: 2016</li>
<li><strong>DOI</strong>: <a href="https://doi.org/10.1016/j.ajhg.2016.03.029" class="uri">https://doi.org/10.1016/j.ajhg.2016.03.029</a></li>
</ul>
</div>
<div id="reference-2" class="section level2">
<h2>Reference 2:</h2>
<ul>
<li><strong>Journal</strong>: BioRxiv</li>
<li><strong>Year</strong>: 2018</li>
<li><strong>DOI</strong>: <a href="https://doi.org/10.1101/316471" class="uri">https://doi.org/10.1101/316471</a></li>
</ul>
</div>
</div>
<div id="model" class="section level1">
<h1>Model</h1>
<span class="math display">\[\begin{align*}
  \mathbf{y} &amp;= \mu \mathbf{1} + \sum_{i = 1}^p \beta_i \mathbf{g}_i + \mathbf{e}, \mathbf{e} \sim N(0, \sigma^2 I) \\
  \gamma_i &amp;= \begin{cases}
    1 &amp; \text{, if $\beta_i =\ne 0$} \\
    0 &amp; \text{, otherwise}
  \end{cases} \\
  \log\frac{\Pr(\gamma_i = 1)}{\Pr(\gamma_i = 0)} &amp;= \alpha_0 + \sum_{k = 1}^q \alpha_k d_{ik}
\end{align*}\]</span>
<p>, where <span class="math inline">\(\mathbf{d}_i\)</span> is annotation of locus <span class="math inline">\(i\)</span>.</p>
<p>It is a generative model. To simulate <span class="math inline">\(\mathbf{y}\)</span>,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\gamma_i \sim Binom(f(\mathbf{\alpha}, \mathbf{d}_i))\)</span> (by the logistic model)</li>
<li><span class="math inline">\(\beta_i \sim N(0, \sigma^2)\)</span> (it is unclear to me how <span class="math inline">\(\sigma^2\)</span> is picked and also not sure whether it matters or not)</li>
<li><span class="math inline">\(\mathbf{y} \sim N(g(\mu \mathbf{1} + \sum_i \beta_i \mathbf{g}_i), \sigma^2 I)\)</span></li>
</ol>
</div>
<div id="inference-procedure" class="section level1">
<h1>Inference procedure</h1>
<ol style="list-style-type: decimal">
<li>Obtain <span class="math inline">\(\hat\alpha\)</span> via MLE (<span class="math inline">\(\Pr(\mathbf{y} | G, \alpha)\)</span>)</li>
<li>Obtain candidate locus (<span class="math inline">\(\Pr(\mathbf{\gamma}_l = 0 | \mathbf{y}_l, G_l, \hat\alpha)\)</span> is small enough)</li>
<li>Fine-mapping on candidate locus (compute PIP, <em>i.e.</em> <span class="math inline">\(\Pr(\gamma_{l_i} = 1 | \mathbf{y}_l, G_l, \hat\alpha)\)</span>)</li>
</ol>
</div>
<div id="computation" class="section level1">
<h1>Computation</h1>
<div id="challenge-1" class="section level2">
<h2>Challenge 1</h2>
<p>Step 1 can be done by EM, where in each iteration, <span class="math inline">\(\Pr(\mathbf{\gamma}_l | \mathbf{y}_l, G_l, \alpha)\)</span> should be evaluated.</p>
<span class="math display">\[\begin{align*}
  \Pr(\mathbf{\gamma}_l = \gamma| \mathbf{y}_l, G_l, \alpha) &amp;= \frac{\Pr(\gamma | \alpha) BF(\gamma)}{\sum_{\gamma&#39;} \Pr(\gamma&#39; | \alpha) BF(\gamma&#39;)} \\
  BF(\gamma) &amp;:= \frac{\Pr(\mathbf{y}_l | G_l, \gamma)}{\Pr(\mathbf{y}_l | G_l, \gamma_l = 0)}
\end{align*}\]</span>
<p>The difficulty is to compute <span class="math inline">\(C:= \sum_{\gamma&#39;} \Pr(\gamma&#39; | \alpha) BF(\gamma&#39;)\)</span>.</p>
<p>This paper proposed adaptive DAP algorithm to compute it. Essentially, it approximates the summation with exponential number of terms by a summation with linear number of terms plus an approximated error.</p>
<p>The idea is that, for <span class="math inline">\(\gamma: \|\gamma\| = s\)</span>, only a subset <span class="math inline">\(\gamma \in \Omega_s\)</span> contribute substantially to the summation. Furthermore, let <span class="math inline">\(C_s := \sum_{\|\gamma&#39;\| = s} \Pr(\gamma&#39; | \alpha) BF(\gamma&#39;)\)</span>, <span class="math inline">\(C_s^* := \sum_{\gamma&#39; \in \Omega_s} \Pr(\gamma&#39; | \alpha) BF(\gamma&#39;)\)</span>, <span class="math inline">\(C_s^*\)</span> can be built upon <span class="math inline">\(C_{s - 1}^*\)</span>.</p>
</div>
<div id="challenge-2" class="section level2">
<h2>Challenge 2</h2>
<p>The procedure proposed above is designed for a locus with small number of QTNs (causal variants). Since it will be computationally intensive if the number of causal variants is much bigger than this scale. So, it is suitable for cis-eQTL analysis.</p>
<p>For GWAS, such assumption is impractical. The idea is to approximate <span class="math inline">\(\Pr(\mathbf{\gamma}_l | \mathbf{y}_l, G_l, \alpha) \approx \prod_{k = 1}^K \Pr(\mathbf{\gamma}_{[k]} | \mathbf{y}_l, G_l, \alpha)\)</span>, where <span class="math inline">\(\mathbf{\gamma}_{[k]}: k = 1, \cdots, K\)</span> is a partition of <span class="math inline">\(\mathbf{\gamma}_l\)</span>. With this approximation, the number of QTNs is smaller in each chunk <span class="math inline">\(k\)</span> so that DAP algorithm can work efficiently.</p>
</div>
</div>
<div id="dap-g" class="section level1">
<h1>DAP-G</h1>
<p>In the second paper, it proposes the signal cluster concept to make the output of fine-mapping result more interpretable. Also, it proposes the summary statistic-based inference approach. In particular, the required summary statistics for inference are <span class="math inline">\(R, \hat{b}, se(\hat{b}), n, SST\)</span>, from which <span class="math inline">\(G&#39;y, G&#39;G\)</span> can be recovered.</p>
</div>

---
title: "PennCNV: An integrated hidden Markov model designed for high-resolution copy number variation detection in whole-genome SNP genotyping data"
date: 2018-04-18T11:21:18-05:00
author: "Yanyu Liang"
tags: ["hmm"]
categories: ["research paper - method"]
draft: false
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
---


<div id="TOC">
<ul>
<li><a href="#meta-data-of-reading">Meta data of reading</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#observed-signal">Observed signal</a></li>
<li><a href="#emission-probabilities">Emission probabilities</a></li>
<li><a href="#transition-probabilities">Transition probabilities</a></li>
<li><a href="#baum-welch-algorithm">Baum-Welch algorithm</a></li>
<li><a href="#family-based-decoding">Family-based decoding</a></li>
</ul>
</div>

<p><span class="math inline">\(\newcommand\E{\text{E}}\)</span> <span class="math inline">\(\newcommand\nocr{\nonumber\\}\)</span></p>
<div id="meta-data-of-reading" class="section level1">
<h1>Meta data of reading</h1>
<ul>
<li><strong>Journal</strong>: Genome research</li>
<li><strong>Year</strong>: 2007</li>
<li><strong>DOI</strong>: 10.1101/gr.6861907</li>
</ul>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>This paper calls CNV from genotype microarray data. It is a nice example of how HMM is built up based on how the data is generated (emission) and the underlying process works (transition). For the computation part, since the emission and transition probability is parameterized by some standard distribution and it is different from what is always taught in class (transition matrix and emission matrix), I will briefly discuss the idea behind the Baum-Welch algorithm in this more general set up. Hopefully, this discussion will help to understand B-W algorithm better and from a more general perspective.</p>
<p>Furthermore, this paper also discussed how to incorporate family-based information into the framework. Intuitively, family information constraint the solution probabilistically in the such a way that children CNV event is the result of inheritance and <em>de novo</em> event so that there should be dependency between parent state and child state. So, the Viterbi algorithm needs to be run on sequences in the family jointly.</p>
</div>
<div id="observed-signal" class="section level1">
<h1>Observed signal</h1>
<p>The genotyping array gives the following signals:</p>
<ol style="list-style-type: decimal">
<li><strong>B allele frequency</strong>: This is kind of an extension of genotype. It provides the estimate of the ratio of allele count at the locus</li>
<li><strong>Log R ratio</strong>: This is a measure of the normalized signal intensity</li>
</ol>
<p>Intuitively, for a normal region with no CNV, BAF is 0, 1/2, 1 and the intensity should be intermediate. For a CNV event (suppose <span class="math inline">\(k\)</span>), the BAF can take value other than 0, 1/2, 1 and it will be <span class="math inline">\(i/k\)</span> for <span class="math inline">\(i = 0, \cdots, k\)</span> instead. Also, the intensity will be different (though it might not follow nice linear relationship). A nice illustration of how BAF and LRR are informative in terms of CNV state is fig 1.</p>
{{< figure src="/notebook/images/penncnv.png" title="BAF and LRR signals vary among CNV states">}}
</div>
<div id="emission-probabilities" class="section level1">
<h1>Emission probabilities</h1>
Following the idea above, it is straight forward to write down the emission probability. Let BAF be <span class="math inline">\(b\)</span> and LRR be <span class="math inline">\(r\)</span>.
<span class="math display">\[\begin{align*}
  \Pr(r | z) &amp;= \pi_r + (1 - \pi_r) N(r; \mu_{r, z}, s_{r, z}) \\
  \Pr(b | z) &amp;= \pi_b +  (1 - \pi_b) \sum_{g \in G(z)}\Pr(G = g|z)\Pr(b|g)
\end{align*}\]</span>
<p>, where <span class="math inline">\(G(z)\)</span> is the set of all possible genotypes that a CNV state <span class="math inline">\(z\)</span> can have. <span class="math inline">\(G = g | z \sim Binom(N(z), p)\)</span> where <span class="math inline">\(N(z)\)</span> is the number of copies for a locus at state <span class="math inline">\(z\)</span> and <span class="math inline">\(p\)</span> is the population allele frequency of the locus of interest. For <span class="math inline">\(\Pr(b|g)\)</span>, ideally <span class="math inline">\(b \sim N(\mu_{b, g}, s_{b, g})\)</span>. Here, the signal is always modeled as the mixture of uniform and normal, where the uniform mode may correspond to the case where the genotype pattern is not captured by the array (kind of like missing data). This conditional probability is very intuitive so that makes sense to me.</p>
One technical issue of the paper’s emission probability for <span class="math inline">\(b\)</span> is dealing with the fact that <span class="math inline">\(b\)</span> is always between 0 and 1 and the way <span class="math inline">\(b\)</span> is calculated is that <span class="math inline">\(b\)</span> will be truncated as 0 or 1 if it is too big or small. Therefore, we need to use a mixture of point mass and normal to model <span class="math inline">\(b\)</span> when genotype is all A alleles or all B alleles. Specifically, the paper used
<span class="math display">\[\begin{align*}
  \Pr(b | g = 1) &amp;= \begin{cases}
    N(b; \mu_{b, 1}, s_{b, 1}) &amp; \text{, if $b \ne 1$} \\
    1 - \Phi(1; \mu_{b, 1}, s_{b, 1}) &amp; \text{, otherwise}
  \end{cases}
\end{align*}\]</span>
</div>
<div id="transition-probabilities" class="section level1">
<h1>Transition probabilities</h1>
The transition probability is not from first principle but it does take distance effect into account. The form is
<span class="math display">\[\begin{align*}
  \Pr(z_{i} = k | z_{i - 1} = l) &amp;= \begin{cases}
    p_{k, l} (1 - e^{-d_i / D_{k, l}}) &amp; \text{, if $k \ne l$} \\
    1 - \sum_{k&#39; \ne l} p_{k&#39;, l} (1 - e^{-d_i / D_{k&#39;, l}}) &amp; \text{, if $k = l$}
  \end{cases}
\end{align*}\]</span>
<p>, where <span class="math inline">\(D_{k, l}\)</span> is capturing the rate of the CNV along the genome. In the paper, when the the CNV involves LOH, <span class="math inline">\(D\)</span> is set to 100Mb and others are set to 100kb since LOH is much more rare comparing to other events. <span class="math inline">\(p\)</span> matrix is estimated by MLE.</p>
<blockquote>
<p>The underlying assumption of the exponential decay (the CDF of exponential distribution which is waiting time distribution) is that for each infinitesimal region of the genome, the probability of event happens twice is higher order to the length and the rate along the genome is constant. It is sort of what is going on here, considering the CNV event is so rare.</p>
</blockquote>
</div>
<div id="baum-welch-algorithm" class="section level1">
<h1>Baum-Welch algorithm</h1>
<p>B-W algorithm is an EM algorithm so that it is made up of two components, E step and M step. The following presents:</p>
<ol style="list-style-type: decimal">
<li>The complete likelihood of PennCNV</li>
<li>The general update rule of EM</li>
<li>How PennCNV fits into EM framework</li>
</ol>
<div id="complete-likelihood-of-penncnv" class="section level2">
<h2>Complete likelihood of PennCNV</h2>
(Recall that) Let’s treat the CNV state of each locus as hidden variable <span class="math inline">\(z_i\)</span>, BAF as <span class="math inline">\(b_i\)</span>, and LRR as <span class="math inline">\(r_i\)</span>. Then, the complete likelihood <span class="math inline">\(\Pr(b, r, z | \theta)\)</span> is
<span class="math display">\[\begin{align*}
  \Pr(b, r, z | \theta) &amp;= p(z_1) p(b_1|z_1) p(r_1|z_1) \prod_{i=2}^n p(z_{i} | z_{i-1}) p(b_i | z_i) p(r_i|z_i)
\end{align*}\]</span>
<p>Note that we <strong>assume</strong> that conditional on <span class="math inline">\(z_i\)</span>, <span class="math inline">\(r_i\)</span> and <span class="math inline">\(b_i\)</span> are independent. Also, the emissions and transitions can be kept as the most general form as <span class="math inline">\(f(y | x)\)</span> with <span class="math inline">\(\int f(z|x) dz = 1\)</span> and <span class="math inline">\(f(y | x) \ge 0\)</span>.</p>
</div>
<div id="em-algorithm" class="section level2">
<h2>EM algorithm</h2>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Q(\theta, \theta^{(t)}) = \E_{z | r, b, \theta^{(t)}}[\log \Pr(b, r, z | \theta)]\)</span></li>
<li><span class="math inline">\(\theta^{(t+1)} = \arg\max_\theta Q(\theta, \theta^{(t)})\)</span></li>
</ol>
<p>Note that HMM complete likelihood has special form, so that we can avoid integrating <span class="math inline">\(z\)</span>. The next section discusses how it happens.</p>
</div>
<div id="fit-hmm-into-em-algorithm" class="section level2">
<h2>Fit HMM into EM algorithm</h2>
<p>For HMM, the log complete likelihood contains the following three components:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\log \Pr(z_1|\theta)\)</span> as function of discrete variable <span class="math inline">\(z_1\)</span></li>
<li><span class="math inline">\(\log \Pr(o_i|z_i, \theta)\)</span> as function of discrete variable <span class="math inline">\(z_i\)</span></li>
<li><span class="math inline">\(\log \Pr(z_{i+1}|z_i)\)</span> as function of discrete variables <span class="math inline">\(z_i, z_{i+1}\)</span></li>
</ol>
In general, let <span class="math inline">\(f: D \rightarrow \mathbb{R}\)</span> where <span class="math inline">\(D\)</span> is a discrete set. Then E step is to compute <span class="math inline">\(\E_z[f(z)]\)</span>. Making use of the property of <span class="math inline">\(f\)</span>:
<span class="math display">\[\begin{align*}
  f(z) &amp;= \sum_{i \in D} f(i)^{I_{z = i}} \\
  E_z[f(z)] &amp;= E_z[\sum_{i \in D} I_{z = i} f(i)] \\
  &amp;= \sum_{i \in D} f(i) \E_z[I_{z = i}] \\
  &amp;= \sum_{i \in D} f(i) \Pr(z = i)
\end{align*}\]</span>
<p>So, the integration of the three components are:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\E_{z|\theta^{(t)}, r, b}[\log \Pr(z_1|\theta)] = \sum_{j \in D} \log \Pr(j|\theta) \Pr(z_1 = j | \theta^{(t)}, o)\)</span></li>
<li><span class="math inline">\(\E_{z|\theta^{(t)}, r, b}[\log \Pr(o_i | z_i, \theta)] = \sum_{j \in D} \log \Pr(o_i | j, \theta) \Pr(z_i = j | \theta^{(t)}, o)\)</span></li>
<li><span class="math inline">\(\E_{z|\theta^{(t)}, r, b}[\log \Pr(z_{i+1} | z_i, \theta)] \\ = \sum_{(j, k) \in D \times D} \log \Pr(k | j, \theta) \Pr(z_i = j, z_{i+1} = k | \theta^{(t)}, o)\)</span></li>
</ol>
As you can see, the integration over <span class="math inline">\(z\)</span> becomes the one over either <span class="math inline">\(z_i\)</span> or <span class="math inline">\(z_{i}, z_{i+1}\)</span> since only <span class="math inline">\(z_i\)</span> or <span class="math inline">\(z_i, z_{i+1}\)</span> matter when considering the above three components (components are added over <span class="math inline">\(i\)</span> so expectation is not affected). Although it is still not clear how to do the integration but let’s assume that we have an efficient algorithm to do this (black box <span class="math inline">\(f_1, f_2, f_3\)</span>). Then we end up with
<span class="math display">\[\begin{align*}
  Q(\theta, \theta^{(t)}) &amp;= \sum_{j \in D} f_1(j, o) \log \Pr(j|\theta) \\
  &amp;+ \sum_{i=1}^{n} \sum_{j \in D} f_2(j, o) \log \Pr(o_i | j, \theta) \\
  &amp;+ \sum_{i=1}^{n-1} \sum_{(j, k) \in D \times D} f_3(j, k, o) \log \Pr(k | j, \theta)
\end{align*}\]</span>
In terms of the maximization task (M step), what is nice about <span class="math inline">\(Q\)</span> is that
<span class="math display">\[\begin{align*}
  Q((\theta_1, \theta_2, \theta_3), \theta^{(t)}) &amp;= \sum_{j \in D} f_1(j, o) \log \Pr(j|\theta_1) \\
  &amp;+ \sum_{i=1}^{n} \sum_{j \in D} f_2(j, o) \log \Pr(o_i | j, \theta_2) \\
  &amp;+ \sum_{i=1}^{n-1} \sum_{(j, k) \in D \times D} f_3(j, k, o) \log \Pr(k | j, \theta_3)
\end{align*}\]</span>
So,
<span class="math display">\[\begin{align*}
  \arg\max_\theta Q((\theta_1, \theta_2, \theta_3), \theta^{(t)}) &amp;= ( \\
  &amp; \arg\max_{\theta_1} \sum_{j \in D} f_1(j, o) \log \Pr(j|\theta_1) \\
  &amp;, \arg\max_{\theta_2} \sum_{i=1}^{n} \sum_{j \in D} f_2(j, o) \log \Pr(o_i | j, \theta_2) \\
  &amp;, \arg\max_{\theta_3} \sum_{i=1}^{n-1} \sum_{(j, k) \in D \times D} f_3(j, k, o) \log \Pr(k | j, \theta_3) \\
  &amp;)
\end{align*}\]</span>
<!-- Another trick is that we can switch the two summations. For instance,
\begin{align*}
  &\sum_{i=1}^{n} \sum_{j \in D} f_2(j, o) \log \Pr(o_i | j, \theta_2) \\
  = &\sum_{j \in D} \sum_{i=1}^{n} \log \Pr(o_i | j, \theta_2)
\end{align*} -->
So, the problem is reduced to three optimization problems. To see why they are tractable, let’s compute the gradient w.r.t <span class="math inline">\(\theta_i\)</span> of the corresponding Lagrangian.
<span class="math display">\[\begin{align}
  \nabla L_1(\lambda_1, \theta_1) &amp;= \sum_{j \in D} f_1(j, o) \nabla_{\theta_1} \log \Pr(j|\theta_1) + \lambda_1 \sum_{j \in D} \nabla \Pr(j| \theta_1) \label{eq:1}\\
  \nabla L_2(\lambda_2, \theta_2) &amp;= \sum_{i=1}^{n} \sum_{j \in D} f_2(j, o) \nabla_{\theta_2} \log \Pr(o_i | j, \theta_2) \\
  \nabla L_3(\lambda_3, \theta_3) &amp;= \sum_{i=1}^{n-1} \sum_{(j, k) \in D \times D} f_3(j, k, o)  \nabla_{\theta_3} \log \Pr(k | j, \theta_3) + \sum_{j \in D} \lambda_{3,j} \sum_{k \in D} \nabla \Pr(k|j, \theta_3)
\end{align}\]</span>
<p>For , it is similar to the MLE of multinomial data with weights. For  and , the objective function is a bit long but the structure is simple without any intractable sum. Therefore, once we know the black box <span class="math inline">\(f_1, f_2, f_3\)</span>, M step is easily achievable.</p>
Now, let’s focus on how to build up the black box, <em>i.e.</em> to compute <span class="math inline">\(\Pr(z_i = j | \theta, o)\)</span> and <span class="math inline">\(\Pr(z_i = j, z_{i+1} = k | \theta, o)\)</span>.
<span class="math display">\[\begin{align*}
  \Pr(z_i = j | o) &amp;= \frac{\Pr(z_i = j, o)}{\Pr(o)} \\
  \Pr(z_i = j, o) &amp;= \Pr(o_1, \cdots, o_i, z_i = j) \Pr(o_{i+1}, \cdots, o_n | z_i = j) \\
  \Pr(z_i = j, z_{i+1} = k, o) &amp;= \Pr(o_1, \cdots, o_i, z_i = j) \Pr(z_{i+1} = k | z_i = j) \\
  &amp;\times \Pr(o_{i+1}|z_{i+1} = k) \Pr(o_{i+2}, \cdots, o_n | z_{i+1})
\end{align*}\]</span>
<p>Note that <span class="math inline">\(\Pr(o)\)</span> can be solved by forward algorithm/backward algorithm and similarly, <span class="math inline">\(\Pr(o_1, \cdots, o_i, z_i = j)\)</span> and <span class="math inline">\(\Pr(o_{i+1}, \cdots, o_n | z_i = j)\)</span> are easily achievable by forward and backward iteration.</p>
</div>
</div>
<div id="family-based-decoding" class="section level1">
<h1>Family-based decoding</h1>
The decoding problem is to find the best <span class="math inline">\(z\)</span> explained by data. In Viterbi algorithm, the problem reduces to solve <span class="math inline">\(\arg\max_z \Pr(z | o, \theta)\)</span>. But for the child’s locus, the CNV state is determined by parents’ CNV state at the same locus with the probability of <em>de novo</em> event (more precisely, what is really determined/inherited is genotype which is more informative than CNV event). Then, the posterior probability of a trio is
<span class="math display">\[\begin{align*}
  \Pr(z_f, z_m, z_c | o_f, o_m, o_c, \theta) &amp;= [\prod_{i = 1}^n p(z_{f,i}|o_{f,i}) p(z_{m,i} | o_{m,i}) p(z_{c,i} | o_{c,i})] \\
  &amp; [p(z_{f,1})p(z_{m,1})p(z_{c,1}|z_{f,1}, z_{m,1}) \\
  &amp; \prod_{i = 2}^n p(z_{f,i}|z_{f,i-1}) p(z_{m,i} | z_{m, i-1}) p(z_{c,i} | z_{f,i}, z_{m,i})]
\end{align*}\]</span>
<p>, where <span class="math inline">\(p(z_{c,i} | z_{f,i}, z_{m,i})\)</span> is derived from Mendel’s laws with the consideration of probability of <em>de novo</em> event. This family-based posterior probability can also be maximized using Viterbi algorithm but with 125 (<span class="math inline">\(5 \times 5 \times 5\)</span>) states.</p>
</div>

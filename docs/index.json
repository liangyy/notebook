[
{
	"uri": "/notebook/posts/huang-2014-aas/",
	"title": "Joint Analysis of SNP and Gene Expression Data in Genetic Association Studies of Complex Diseases",
	"tags": ["gwas", "eqtl", "complex trait", "integrative analysis"],
	"description": "",
	"content": "Meta data of reading  Journal: The Annals of Applied Statistics Year: 2014 DOI: 10.1214/13-AOAS690  \\[ \\newcommand\\logit{\\text{logit}} \\newcommand\\E{\\text{E}} \\newcommand\\var{\\text{Var}} \\newcommand\\diag{\\text{diag}} \\newcommand\\nocr{\\nonumber\\\\} \\]\n Motivation The goal is to assess the genetic effect of the specific gene on the disease. In this paper, the authors casted the genetic effect as two parts:\nEffect through gene expression of the given gene Other genetic effect (i.e. splicing, but some gene-unrelated mechanisms are also possible as long as it is determined genetically, say enhancer activity)  The paper used genotype data along with paired gene expression data. The variables in the paper were:\nA set of SNPs within the gene (\\(S\\)) Expression level of the given gene (\\(G\\)) Disease status (\\(Y\\))  The causal model is (Figure 1 of the paper):\n The model \\[\\begin{align} \\logit\\{\\Pr(Y_i = 1| S_i, G_i, X_i)\\} \u0026amp;= X_i^T \\alpha + S_i^T \\beta_S + G_i \\beta_G + G_i S_i^T \\gamma \\label{eq:y} \\end{align}\\] The interaction term modeled the combined effect of genotype and gene expression level to phenotype log odds. This term was added because it made biological sense. Since genotype can affect gene expression, such effect was modeled as following:\n\\[\\begin{align} G_i\u0026amp;= X_i^T\\phi + S_i^T\\delta + \\epsilon_i \\label{eq:g} \\end{align}\\] , where \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_G^2)\\).\nThe goal was to test if the total effect captured by genotype and gene expression on \\(Y\\) is non-zero. Namely,\n\\[\\begin{align} H_0: \\beta_S 0, \\beta_G = 0, \\gamma = 0 \\label{eq:h0} \\end{align}\\] This test was referred as the test for total effect of a gene.\n Testing \\(H_0\\) I am not familiar with these hypothesis testing techniques so I can only sketch the general idea and leave the details untouched.\nThe paper discussed the possiblity of using LRT or Wald test to test and they argued that degree of freedom in this case was big so that the power would be limited. Alternatively, the paper proposed to test variance components. Assuming:\n\\[\\begin{align} \\beta_{S_i} \\sim_{iid} \\mathcal{N}(0, \\tau_S) \\nocr \\gamma_i \\sim_{iid} \\mathcal{N}(0, \\tau_I) \\nonumber \\end{align}\\] Namely the becomes a logistic midxed model. Then becomes:\n\\[\\begin{align} H_0: \\tau_S = \\tau_I = 0, \\beta_G = 0 \\nonumber \\end{align}\\] The scores for \\(\\tau_S, \\tau_I\\) and \\(\\beta_G\\) are:\n\\[\\begin{align} U_{\\tau_S} \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T \\mathbb{S}\\mathbb{S}^T \\{Y - \\hat{\\mu}_0\\} \\nocr U_{\\tau_I} \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T \\mathbb{C}\\mathbb{C}^T \\{Y - \\hat{\\mu}_0\\} \\nocr U_{\\beta_G} \u0026amp;= G^T \\{Y - \\hat{\\mu}_0\\} \\nonumber \\end{align}\\] , where\n \\(\\mathbb{S} = (S_1, ..., S_n)^T\\) \\(G = (G_1, ..., G_n)^T\\) \\(\\mathbb{C} = (C_1, ..., C_n)^T = (G_1S_1, ..., G_nS_n)^T\\) \\(\\hat{\\mu}_{0i} = \\exp(X_i^T \\hat{\\alpha}_0) / \\{1 + \\exp(X_i^T \\hat{\\alpha}_0)\\}\\), and \\(\\hat\\alpha_0\\) is the MLE of null model: \\[\\begin{align} \\logit(\\Pr(Y = 1|S_i, G_i, X_i)) = X_i^T \\alpha \\nonumber \\end{align}\\]  To combine the three scores to test , the authors proposed the following weighted sum as the test statistic:\n\\[\\begin{align} Q \u0026amp;= n^{-1} (a_1U_{\\tau_S} + a_2U_{\\beta_G} + a_3U_{\\tau_I}) \\nocr \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T + a_2GG^T + a_3 \\mathbb{C}\\mathbb{C}^T) \\{Y - \\hat{\\mu}_0\\} \\end{align}\\] , where they proposed to use the inverse of the squared root of the corresponding variance as weights of \\(U_{\\tau_S}, U_{\\beta_G^2}, U_{\\tau_I}\\). The varaince can be computed in closed-form with \\(\\hat{\\mu}_{0i}\\). Let \\(Q = Q(\\hat{alpha})\\) and:\n\\[\\begin{align} D \u0026amp;= \\begin{bmatrix} D_{XX} \u0026amp; D_{XV} \\\\ D_{VX} \u0026amp; D_{VV} \\end{bmatrix} \\nocr \u0026amp;= n^{-1} U^T W U \\nocr \\text{, where} \u0026amp; \\nocr U \u0026amp;= \\begin{bmatrix} U_1 \\\\ \\vdots \\\\ U_n \\end{bmatrix} \\nocr U_i \u0026amp;= (X_i^T, V_i^T) \\nocr V_i \u0026amp;= (\\sqrt{a_1}S_i^T, \\sqrt{a_2} G_i, \\sqrt{a_3} C_i) \\in \\mathbb{R}^{2p + 1} \\nocr W \u0026amp;= \\diag(\\mu_i(1 - \\mu_i)) \\nonumber \\end{align}\\] Under null hypothesis , \\(Q \\xrightarrow{d} Q(0) = \\sum_{j}^{2p+1} (A_l^T \\epsilon)^2\\), where:\n \\(\\epsilon \\sim \\mathcal{N}(0, D)\\) \\(A_l\\) is \\(l\\)th row of \\(A = [-D_{XV}^T D_{XX}^{-1}, I_{2p+1}]\\)  It means that under null hypothesis, \\(Q\\) follows a mixture of \\(chi^2\\) distribution, which can be approximated by scaled \\(chi^2\\) as \\(Q \\sim \\kappa \\chi_{\\nu}^2\\), where \\(\\kappa = \\var(Q) / [2\\E(Q)]\\) and \\(\\nu = 2[E(Q)]^2 / \\var(Q)\\) (the expression was given in supplementary).\nFurthermore, some other hypothesis would be:\n \\(Y\\) depends on \\(G\\), \\(S\\) but not their interaction term \\(Y\\) depends on \\(S\\) only  Denote the previously derived \\(Q\\) as \\(Q_{SGC}\\), the test statistic for the above to cases are:\n \\(Q_{GS} = \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T + a_2GG^T) \\{Y - \\hat{\\mu}_0\\}\\) \\(Q_{S} = \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T) \\{Y - \\hat{\\mu}_0\\}\\)  If the model was wrongly specified, then the power of test would be hurted. So, it was necessary to consider the three situations simultanously in the test. The paper proposed a omnibus test to consider these three situations together. Namely the following procedure:\nCompute p-values for the three situations Obtain observed minimum p-value Compare the observed value to its null distribution  , where null distribution of the minimum p-value is not available analytically because of the complicated correlation among the three statistics. Therefore, they used resampling perturbuation instead.\n "
},
{
	"uri": "/notebook/posts/sherlock-2013-ajhg/",
	"title": "Sherlock: Detecting Gene-Disease Associations by Matching Patterns of Expression QTL and GWAS",
	"tags": ["gwas", "eqtl", "integrative analysis", "target gene"],
	"description": "",
	"content": " Meta data of reading  Journal: The American Journal of Human Genetics Year: 2013 DOI: 10.1016/j.ajhg.2013.03.022  "
},
{
	"uri": "/notebook/posts/yang-2012-ng/",
	"title": "Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits",
	"tags": ["gwas", "integrative analysis", "complex trait", "linkage disequilibrium", "conditional analysis"],
	"description": "",
	"content": " Meta data of reading  Journal: Nature Genetics Year: 2012 DOI: 10.1038/ng.2213  $$ \\newcommand\\independent{\\perp\\!\\!\\!\\!\\perp} \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\cr} \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\cor{\\text{Cor}} \\newcommand\\numberthis{\\addtocounter{equation}{1}\\tag{\\theequation}} $$\nMotivation The GWAS procedure is to use single-SNP model to test association and select the SNP with strongest signal to represent the genomic region (~ 2Mb) and the genetic variation is computed based on this SNP only. The paper pointed out the underlying assumptions of this procedure:\n Implicit assumptions, often untested, are that the detected association at the top SNP captures the maximum amount of variation in the region by its LD with an unknown causal variant and that other SNPs in the vicinity show association because they are correlated with the top SNP.\n They pointed out 2 reasons why this assumption may fail:\n Suppose there is only one causal variant, a single tagging SNP may not capture all of its variation It is possible that there are more than one causal variant  So, one-SNP-per-locus procedure may underestimate the underlying causal genetic variation. Some studies have performed conditional analysis to find the secondary SNP inside the locus. The paper proposed a systematic approach to perform conditional analysis by combining GWAS meta-analysis and LD correlation from the same population.\nMulti-SNP model and joint effect The multi-SNP model is:\n$$\\begin{align} \\vec{y} \u0026= X\\vec{b} + \\vec{e} \\nonumber \\end{align}$$ , where $X \\in \\mathbb{R}^{n \\times N}, \\vec{b} \\in \\mathbb{R}^N, \\vec{e} \\in \\mathbb{R}^n$. The least squares solution is:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y \\label{eq:sej} \\cr \\var(\\hat{b}) \u0026= \\sigma^2_{J} (X'X)^{-1} \\label{eq:varj} \\end{align}$$ Note that \\eqref{eq:varj} is an N-by-N (co)variance matrix. It is derived as follows:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y = (X'X)^{-1} X'(Xb + e) \\nocr \u0026= b + (X'X)^{-1} X'e \\nocr \\var(\\hat{b}_j) \u0026= \\var([(X'X)^{-1} X'e]_j) \\text{, where $[\\cdot]$ takes the $i$th row}\\nocr \u0026= \\var([(X'X)^{-1}]_j X'e) \\nocr \u0026= [(X'X)^{-1}_j X'] \\odot [(X'X)^{-1}_j X'] \\var(e) \\label{eq:inter1} \\end{align}$$ Note that in \\eqref{eq:inter1}, we have $[(X'X)^{-1}_j X'] \\in \\mathbb{R}^{1 \\times n}$ and $\\var(e) \\in \\mathbb{R}^{n \\times 1}$ which are vectors. But notice that $e_1, \u0026hellip;, e_n$ are iid. The expression can be simplified as:\n$$\\begin{align} \\eqref{eq:inter1} \u0026= t_j X' X t_j' \\sigma^2_J \\nonumber \\end{align}$$ , where $t_j := [(X'X)^{-1}]_j$. Similarly,\n$$\\begin{align} \\cov(\\hat{b}_i, \\hat{b}_j) \u0026= t_i X' X t_j' \\sigma^2_J \\nonumber \\end{align}$$ So,\n$$\\begin{align} \\var(\\hat{b}) \u0026= \\sigma^2_J \\begin{bmatrix} t_1 \\cr \\vdots \\cr t_N \\end{bmatrix} X'X \\begin{bmatrix} t_1 \u0026 \\cdots \u0026 t_N \\end{bmatrix} \\nocr \u0026= \\sigma^2_J (X'X)^{-1} (X'X) (X'X)^{-T} \\nocr \u0026= \\sigma^2_J (X'X)^{-1} \\text{, with the fact that $X'X$ is symmetric} \\nonumber \\end{align}$$ , which is the result of \\eqref{eq:varj}.\nSingle-SNP model and marginal effect In practice, only single-SNP model results are available. The single-SNP model is as follow:\n$$\\begin{align} y \u0026= x_j \\beta_j + e \\nonumber \\end{align}$$ So, the LS estimator is:\n$$\\begin{align} \\hat\\beta \u0026= D^{-1} X'y \\label{eq:se} \\cr \\var(\\hat\\beta) \u0026= \\sigma^2_M D^{-1} \\label{eq:varm} \\end{align}$$ , where $D = diag(\\vec{d}), d_i = X_i' X_i$ and $X_i$ is the $i$th column of $X$. The derivation of \\eqref{eq:varm} is as follow:\n$$\\begin{align} \\var(\\hat\\beta_i) \u0026= \\var((X_i' X_i)^{-1} X_i' (X_i b_i + e)) \\nocr \u0026= \\var(b_i + (X_i' X_i)^{-1} X_i' e) \\nocr \u0026= \\var((X_i' X_i)^{-1} X_i' e) \\nocr \u0026= \\frac{X_i' \\odot X_i'}{(X_i' X_i)^2} \\var(e) \\nocr \u0026= \\frac{X_i' X_i}{(X_i' X_i)^2} \\sigma^2_M \\nocr \u0026= \\sigma^2_M (X_i' X_i)^{-1} \\nocr \\end{align}$$ Here, we treat single-SNP model as the truth. Note that we treat SNPs independently with each other (namely $\\cov(\\hat\\beta_i, \\hat\\beta_j) = 0$). Different SNPs do not have to share the same residual variance, so a more precise expression is $\\var(\\hat\\beta_i) = \\sigma^2_{M(i)} (X_i' X_i)^{-1}$\nInferring joint effect from single effect From \\eqref{eq:se}, we have $X'y = D \\hat\\beta$. Under multiple SNP model, the proportion of variance explained by all SNPs is:\n$$\\begin{align} R_J^2 \u0026= \\frac{\\cov(\\hat{y}, y)}{\\var(y)} \\nocr \u0026= \\frac{\\hat{b}' X' y}{y'y} \\nocr \u0026= \\frac{\\hat{b}' D \\hat\\beta}{y'y} \\nonumber \\end{align}$$ Then, we can derive:\n$$\\begin{align} \\hat\\sigma^2_J \u0026= \\frac{(1 - R_J^2) y'y}{n - N} \\nocr \u0026= \\frac{y'y - \\hat{b}' D \\hat\\beta}{n - N} \\end{align}$$ Similarly,\n$$\\begin{align} R_{M(j)}^2 \u0026= \\frac{\\hat{y}_j' y}{y'y} \\nocr \u0026= \\frac{X_j' \\hat\\beta_j y}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j X_j'y}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j D_j \\hat\\beta_j}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j^2 D_j}{y'y} \\nocr \\hat\\sigma^2_{M(j)} \u0026= \\frac{(1 - R_{M(j)}) y'y}{n - 1} \\nocr \u0026= \\frac{y'y - \\hat\\beta_j^2 D_j}{n - 1} \\nonumber \\end{align}$$ From \\eqref{eq:varm}, we have $\\var(\\hat\\beta_j) = \\hat\\sigma^2_{M(j)} / D_j$, so we get:\n$$\\begin{align} y'y = D_j \\var(\\hat\\beta_j) (n - 1) + D_j \\hat\\beta_j^2 \\label{eq:yy} \\end{align}$$ This expression provides a way to obtain $y\u0026rsquo;y$ with $\\hat\\beta_j$ and $\\hat{\\var}(\\hat\\beta_j)$ (w/o knowing individual level data $y$). In practice, the paper used the median of inferred $y\u0026rsquo;y$ obtained from $j = 1, \u0026hellip;, N$.\n Side note   The reason why the above analysis is performed is to obtain joint model statistic from the single-SNP model without querying individual level data $y$. One useful relation is: $\\hat\\sigma^2 = \\frac{(1 - R^2) y\u0026rsquo;y}{N - n}$. $R^2$ is computable since it is just the observed proportion of covariance between predictor and response in the overall variance of response. For the single-SNP case, $\\hat\\sigma^2$ is available via \\eqref{eq:varm}.\n In meta-analysis, $X$ is not available as well. But since $X\u0026rsquo;X$ is the (co)variance matrix of SNP genotypes, it can be approximated by the LD score from a matched reference population. The paper used $W$ to denote such population, where $w_{ij} = -2f_j, 1 - 2 f_j, 2 - 2 f_j$ to denote genotypes: two major alleles, heterozygous, two minor alleles respectively. Under this set up, $\\E(w_j) = 0, \\var(w_j) = 2f_j(1 - 2 f_j)$. Therefore, we have:\n$$\\begin{align} \\frac{\\sum_i x_{ij} x_{ik}}{\\sqrt{\\sum_i x_{ij}^2 \\sum_i x_{ik}^2}} \u0026\\approx \\frac{\\sum_i w_{ij} w_{ik}}{\\sqrt{\\sum_i w_{ij}^2 \\sum_i w_{ik}^2}} \\nocr \\Rightarrow (X'X)_{jk} \u0026= \\sum_i x_{ij} x_{ik} \\nocr \u0026\\approx \\sum_i w_{ij} w_{ik} \\sqrt{\\frac{D_j D_k}{D_{W(j)}D_{W(k)}}} := B_{jk} \\nocr \\Rightarrow B \u0026:= D^{1/2}D_W^{-1/2}W'W D_W^{-1/2} D^{1/2} \\approx X'X \\label{eq:app} \\end{align}$$ where $D, D_W$ is the diagonal matrix with diagonal entries of $X\u0026rsquo;X, W\u0026rsquo;W$. $D_{j}$ is not available without $X$, but it can be approximated by $2p_j(1 - p_j)n$. From \\eqref{eq:sej}, \\eqref{eq:se}, \\eqref{eq:app}, we have:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y = (X'X)^{-1} D \\hat\\beta \\nocr \u0026\\approx B^{-1} D \\hat\\beta := \\tilde{b} \\nonumber \\end{align}$$ Similar to \\eqref{eq:varj},\n$$\\begin{align} \\var(\\tilde{b}) \u0026= \\sigma^2_J B^{-1} \\end{align}$$ In the paper, distant SNP pairs were assigned zero correlation instead the observed one in $W$. The paper pointed out an additional complexity in practice, that is SNPs may have different effective sample sizes due to imputation failures. Therefore, the paper suggested to estimate the effective sample for each SNP and use the adjusted sample size to compute $B_{jk}$. The procedure is:\n From \\eqref{eq:yy}, we obtain $y\u0026rsquo;y$ by taking the median Obtain $\\hat{n}_j$ using \\eqref{eq:yy}: $$\\begin{align} \\hat{n}_j \u0026amp;= y\u0026rsquo;y / D_j \\var(\\hat\\beta_j) - \\hat\\beta_j^2 / \\var(\\hat\\beta_j) + 1 \\nonumber \\end{align}$$ Compute $B_{jk}$ and $D_j$ using the adjusted sample size: $$\\begin{align} B_{jk} \u0026amp;= 2 \\min(\\hat{n}_j, \\hat{n}_k) \\sqrt{\\frac{p_j(1 - p_j)p_k(1 - p_k)}{D_{W(j)}D_{W(k)}}} (W\u0026rsquo;W)_{jk} \\nocr W_j \u0026amp;= 2p_j(1 - p_j) \\hat{n}_j \\nonumber \\end{align}$$  Obtain p-value of marginal effect under multi-SNP model In brief, the above derivation provides a way to infer joint effect distribution from single-SNP model summary statistic. Namely, we get:\n$$\\begin{align} (\\tilde{b} - b) \\sim \\mathcal{N}(0, \\var(\\tilde{b}) \\nonumber \\end{align}$$ The marginal distribution for each SNP\u0026rsquo;s effect size, $\\tilde{b}_i$, is:\n$$\\begin{align} (\\tilde{b}_i - b_i) \\sim \\mathcal(N, \\var(\\tilde{b}_i)) \\nonumber \\end{align}$$ Therefore, we can construct a test as follow:\n $H_0$: $b_i$ is zero $H_1$: $b_i$ is not zero  Under the null, $\\tilde{b}_i \\sim \\mathcal{N}(0, \\var(\\tilde{b}_i))$. Then $\\mathbb{P}_{H_0}(|b_i| \u0026gt; |\\tilde{b}_i| ) = 2(1 - \\Phi(|\\tilde{b}_i|))$, which is the marginal effect of $i$th SNP under multi-SNP model.\nConditional analysis The logic of this part is not very intuitive to me, but after some struggling, I end up with the following things.\nFirst of all, the conditional analysis takes a two step procedure to estimate $\\hat{b}_2 | \\hat{b}_1$. That is:\n Do $y \\sim X_1$ and obtain $\\bar{b}_1 = (X_1'X_1)^{-1} X_1'y$ Compute $\\tilde{y} = y - X_1 \\bar{b}_1$ Do $\\tilde{y} \\sim X_2$ and obtain $\\hat{b}_2 | \\hat{b}_1 = (X_2\u0026rsquo; X_2)^{-1} X_2\u0026rsquo; \\tilde{y}$, which matches the equation 15 in the text  The variance of $\\hat{b}_2 | \\hat{b}_1$ stuck me for a while because it is unclear how to define $\\hat\\sigma^2_C$. It is still not so clear to me but what I get is the following:\n$$\\begin{align} y \u0026= X_1 b_1 + X_2 b_2 + e \\label{eq:y}\\cr \\hat{b}_2 | \\hat{b}_1 \u0026= M_2^{-1} X_2'y - M_2^{-1} M_{21} M_1^{-1} X_1'y \\nocr \u0026\\text{, where $M_{ij} = X_i' X_j$ and $M_{ii} = M_i$} \\nocr \u0026= M_2^{-1} M_{21} b_1 + M_2^{-1}M_2b_2 + M_2^{-1}X_2'e \\nocr \u0026- M_2^{-1}M_{21}M_1^{-1}M_1 b_1 - M_2^{-1}M_{21}M_1^{-1}M_{12}b_2 - M_2^{-1}M_{21}M_1^{-1}X_1'e \\nocr \u0026= (M_2^{-1}M_2 - M_2^{-1}M_{21}M_1^{-1}M_{12})b_2 \\nocr \u0026+ (M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1') e \\nocr \\var(\\hat{b}_2 | \\hat{b}_1) \u0026= \\var((M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1') e) \\nocr \u0026= (M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1')(M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1')' \\sigma^2_J \\nocr \u0026= M_2^{-1}X_2' (I - X_1 M_1^{-1} X_1')(I - X_1 M_1^{-1} X_1')' X_2M_2^{-1} \\sigma^2_J\\nocr \u0026= M_2^{-1}X_2' (I - 2X_1M_1^{-1}X_1' + X_1M_1^{-1}X_1'X_1M_1^{-1}X_1') X_2M_2^{-1} \\sigma^2_J\\nocr \u0026= M_2^{-1}X_2' (I - X_1M_1^{-1}X_1') X_2M_2^{-1}\\sigma^2_J \\nocr \u0026= [M_2^{-1} - M_2^{-1}M_{21}M_1^{-1}M_{12}M_2^{-1}]\\sigma^2_J \\nocr \\end{align}$$ So, it seems to me that $\\sigma^2_C$ and $\\sigma^2_J$ are interchangeable if $X = [X_1, X_2]$. If such condition is not satisfied, \\eqref{eq:y} is not the multi-SNP model, so it is better to denote the residual variance as $\\sigma^2_C$. The paper computed $\\hat\\sigma^2_C$ using the following equation:\n$$\\begin{align} \\hat\\sigma^2_C \u0026= \\frac{y'y - \\hat{b}_1' D_1 \\hat{\\beta}_1 - (\\hat{b}_2|\\hat{b}_1)' D_2\\hat\\beta_2}{n - N_1 - N_2} \\end{align}$$ Similar to previous derivation, the individual level statistics can be replaced by $D, \\beta, B$. Note that $M_{12}, M_{21}, M_1, M_2$ can be approximated by $B$.\nResults in brief The paper performed the analysis using GIANT GWAS and two reference genotype data. If two SNPs were in low LD, the result was similar to singe-SNP model. For positively correlated SNPs (modest LD), single-SNP model tended to overestimate the effect size. While multi-SNP model gave smaller effect size, they still reached genome-wide significance. For negatively correlated SNPs, single-SNP model tended to miss one of the signal because the signal was masked by the other one. Multi-SNP model was more powerful in this case. They found 36 loci with multiple signals with 38 leading SNPs and 49 additional SNPs. The result is robust to the choice of reference sample. Conditional analysis was also performed to identify secondary associations in the loci. The analysis was also applied to case-control study where $y$ is OR instead of quantitative trait.\n"
},
{
	"uri": "/notebook/posts/mendelian-randomization/",
	"title": "Mendelian randomization and instrumental variable regression",
	"tags": ["mendelian randomization", "causality"],
	"description": "",
	"content": " $$ \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\iv{\\text{IV}} $$\nMeta data of reading  Links:  https://www.bauer.uh.edu/rsusmel/phd/ec1-8.pdf http://fmwww.bc.edu/EC-C/F2012/228/EC228.f2012.nn15.pdf http://mathworld.wolfram.com/Lindeberg-FellerCentralLimitTheorem.html http://mathworld.wolfram.com/LindebergCondition.html  Year: NA DOI: NA  The problem and the idea of MR Suppose we have phenotype $Y$, gene expression $X$, and genotype $Z$. The goal is to see if $X$ and $Y$ has some causal relationship. Since there are some unknown confounders, the residual of $Y \\sim X$ is correlated with $X$. Therefore, the OLS estimator of effect size $\\hat{\\beta}_{xy}$ is biased.\nTo account for such drawback, $Z$ is introduced as instrumental variable (IV) since genotype is pre-determined so that there should not be confounders that can affect genotype. Therefore, the residual of $Y \\sim Z$ should not be correlated with $Z$. So, OLS estimator is unbiased. The estimator constructed by $\\hat{\\beta}_{zy}$ and $\\hat{\\beta}_{zx}$ is simply:\n$$\\begin{aligned} \\hat{\\beta}_{xy} \u0026amp;= \\frac{\\hat{\\beta}_{zy}}{\\hat{\\beta}_{zx}} \\end{aligned}$$\nConsistency and the derivation of the variance The following is derived from https://www.bauer.uh.edu/rsusmel/phd/ec1-8.pdf.\nFormally, IV estimator is defined as $b_{\\iv} = (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;y$. Then, we have:\n$$\\begin{aligned} b_{\\iv} \u0026amp;= (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;y \\cr \u0026amp;= (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;(X \\beta_{xy} + \\epsilon) \\cr \u0026amp;= \\beta_{xy} + (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;\\epsilon \\cr \u0026amp;\\xrightarrow{p} \\beta_{xy} \\end{aligned}$$\nsince $Z$ is independent to error.\nThe variance of $b_{\\iv}$ is inspired by Lindeberg-Feller CLT:\n$$\\begin{aligned} \\sqrt{N} (b_{\\iv} - \\beta_{xy}) \u0026amp;= \\sqrt{N} (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;\\epsilon \\cr \u0026amp;= (Z\u0026rsquo;X / N)^{-1} \\sqrt{N} (Z\u0026rsquo;\\epsilon / N) \\cr (Z\u0026rsquo;\\epsilon / N) \\sqrt{N} \u0026amp;\\xrightarrow{d} \\mathcal{N}(0, \\sigma^2 \\var(Z)) \\quad\\text{, by L-F CLT} \\cr \\sqrt{N} (b_{\\iv} - \\beta_{xy}) \u0026amp;\\xrightarrow{d} \\mathcal{N}(0, \\sigma^2 \\cov(Z, X)^{-1} \\var(Z) \\cov(Z, X)^{-1}) \\end{aligned}$$\nThe last line is kind of heuristic to me but I cannot find a justification for it (anyway \u0026hellip;). Note that $\\sigma^2 := \\var(\\epsilon)$ and it turns out that $\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_i (y_i - b_{\\iv} x_i)^2$ is an unbiased estimator of this term. So, $\\hat{\\var}(b_{\\iv}) = \\hat{\\sigma}^2 \\hat{\\cov}(Z, X)^{-1} \\hat{\\var}(Z) \\hat{\\cov}(Z, X)^{-1}$.\nThe general procedure of MR This section describes the MR procedure as stated in this paper (see post).\n$b_{\\iv}$ can be computed by two-step least squares (2SLS), simply $\\hat{b}_{xy} = \\hat{b}_{zy} / \\hat{b}_{zx}$. By the result of the above section, we have:\n$$\\begin{aligned} \\var(\\hat{b}_{xy}) \u0026amp;= \\frac{\\text{unexplained variance by 2SLS}}{N} \\times \\frac{\\var(Z)}{\\cov(Z, X)^{2}} \\cr \u0026amp;= \\frac{\\text{unexplained variance by 2SLS}}{N\\var(X) \\rho^2_{xz}} \\text{, Since } \\rho^2_{xz} := \\frac{\\cov(X, Z)^2}{\\var(Z)\\var(X)} \\cr \u0026amp;= \\frac{\\var(Y) (1 - R_{xy}^2)}{N \\var(X) R_{zx}^2} \\end{aligned}$$\n, which justifies the result in the paper (equation 2).\n"
},
{
	"uri": "/notebook/posts/zhu-2017-ng/",
	"title": "Integration of summary data from GWAS and eQTL studies predicts complex trait gene targets",
	"tags": ["gwas", "integrative analysis", "eqtl", "target gene", "mendelian randomization", "causality", "complex trait", "linkage disequilibrium"],
	"description": "",
	"content": " $$ \\newcommand\\independent{\\perp\\!\\!\\!\\!\\perp} \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\cr} \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\cor{\\text{Cor}} $$\nMeta data of reading  Journal: Nature Genetics Year: 2016 DOI: 10.1038/ng.3538  Instrumental variable and Mendelian randomization analysis The paper mentioned it in the background section along with Mendelian randomization (MR). I googled this term and found this site. Also, wiki page is informative.\nIt appears in regression analysis where $y$ is the responsive variable and $x$ is the explanatory variable. The model is simply $Y = X \\beta + \\epsilon$. Suppose $Z$ correlates with $X$ and $Z$ is independent to $Y$ given $X$, then $Z$ correlates with $Y$ as well despite such conditional independence.\nIn some analysis, people sort of take the advantage of $Z$ (in this context $Z$ is called instrumental variable). In regression model, the underlying assumption is that $\\epsilon$ is independent to $X$. But it is hard to achieve if there is some other unknown variable $U$ that affect both $X$ and $Y$. In this case, error term \u0026ldquo;absorbs\u0026rdquo; the dependencies in $U$ which leads to the violence of $X \\independent \\epsilon$ (the following graph is an example).\ngraph LR; Z((Z)) --- X((X)) X --- Y((Y)) U((U)) --- Y X --- U E((error)) --- Y  In this case, $Y \\sim X$ cannot distinguish $U$ and $\\epsilon$ but $Y \\sim Z$ does not have such problem since the effect of $U$ is captured by $X$ and $\\epsilon$ is disentangled.\nThe idea of MR can be illustrated by the following graph:\ngraph LR; Z(variant) --- X(gene expression) X --- Y(phenotype) U(unknown factors) --- Y X --- U E(error) --- Y  Here, the unknown factors can be environmental and regression model cannot tease apart error and them. But since variants are fixed, therefore we can use genetic variant as the instrumental variable to infer the dependency between gene expression and phenotype. It is the idea of MR.\nMotivation MR analysis requires matched genotype and expression profile data and its power is limited by the strength of the dependency between genotype and phenotype along with the one between gene expression and phenotype. So, it is not useful in practice.\nThis paper tends to use summary statistic of GWAS and eQTL instead, which may not as strong as individual level data, but it is easy to achieve a much larger sample size.\nMethod This paper proposed a summary data-based MR method (SMR). In intuitively, it estimates effect of $Z$ (genotype) on $Y$ (phenotype), $b_{zy}$ and effect of $Z$ on $X$, $b_{zx}$ (note that the former is GWAS and the latter is eQTL mapping). Then $b_{xy}$ is simply $\\frac{b_{zy}}{b_{zx}}$. This approach, same as MR, estimates the effect of gene expression on phenotype which is only mediated by genetic component (in other word, free of non-genetic confounders). But the caveat is that the method cannot distinguish causality and pleiotropic effect (mediated by genetic confounders).\nIt appears to me that the derivation of MR (or instrumental variable regression) is not intuitive so I leave the note of MR part for another post here. Note that in MR, the same $Z$ is used for the estimation of $\\hat{b}_{zx}, \\hat{b}_{zy}$. SMR, instead, uses different $Z$, namely $Y \\sim Z_1$ and $X \\sim Z_2$ and $\\hat{b}_{xy} = \\hat{b}_{zy} / \\hat{\\beta}_{zx}$. Since $Z_1 \\independent Z_2$, $\\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) = 0$. By Delta method, we have:\n$$\\begin{aligned} \\var(\\hat{b}_{xy}) \u0026amp;\\approx \\begin{bmatrix} \\frac{1}{\\beta_{zx}} \u0026amp; -\\frac{b_{zy}}{\\beta_{zx}^2} \\end{bmatrix} \\begin{bmatrix} \\var(\\hat{b}_{zy}) \u0026amp; \\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) \\cr \\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) \u0026amp; \\var(\\hat{\\beta}_{zx}) \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\beta_{zx}} \\cr -\\frac{b_{zy}}{\\beta_{zx}^2} \\end{bmatrix} \\cr \u0026amp;= \\frac{b_{zy}^2}{\\beta_{zx}^2} \\bigg[ \\frac{\\var(\\hat\\beta_{zx})}{\\beta_{zx}^2} + \\frac{\\var(\\hat{b}_{zy})}{b_{zy}^2} - 2\\frac{\\cov(\\hat\\beta_{zx}, \\hat{b}_{zy})}{\\beta_{zx}b_{zy}} \\bigg] \\end{aligned}$$\nThen the $\\chi^2$ statistic is:\n$$\\begin{aligned} T_{\\text{SMR}} \u0026amp;= \\hat{b}_{xy} / \\var(\\hat{b}_{xy}) \\cr \u0026amp;\\approx \\bigg(\\frac{\\hat{b}_{zy}}{b_{zy}}\\bigg)^2 \\bigg(\\frac{\\beta_{zx}}{\\hat\\beta_{zx}}\\bigg)^2 \\bigg/ \\bigg[\\frac{(\\frac{\\hat{b}_{zy}}{b_{zy}})^2}{z_{zy}^2} + \\frac{(\\frac{\\hat\\beta_{zx}}{\\beta_{zx}})^2}{z_{zx}^2}\\bigg] \\cr \u0026amp;\\approx \\frac{1}{1 / z_{zy}^2 + 1 / z_{zx}^2} \\cr \u0026amp;= \\frac{z_{zy}^2 z_{zx}^2}{z_{zy}^2 + z_{zx}^2} \\end{aligned}$$\n, which is described in equation 5.\nFurthermore, some data sets only report z-score without effect size. In this case, the inference can still be done but effect size is needed to estimate $b_{xy}$. In fact, the effect size can be recovered if the allele frequency is known (see the result at supplementary notes page 9 bottom and page 10 top equations). This result is surprising to me, so I scratch the derivation below.\nWithout loss of generality, let\u0026rsquo;s consider $\\hat{b}_{zy}$. $\\hat\\beta_{zx}$ follows the same rule.\n First \u0026ldquo;=\u0026rdquo;:  $$\\begin{aligned} \\hat{b}_{zy} \u0026= (Z'Z)^{-1} (Z'y) \\cr \u0026= (Z'Z)^{-1} (Z b_{zy} + \\epsilon) \\cr \u0026= b_{zy} + (Z'Z)^{-1} (Z' \\epsilon) \\cr \u0026= b_{zy} + \\frac{\\sum_j (z_j - \\bar{z}) \\epsilon_j}{\\sum_i (z_i - \\bar{z})^2} \\end{aligned}$$ Here $Z$ is the normalized version of the original $Z$ for simplicity. Since the expression evolves \u0026ldquo;divid\u0026rdquo;, taking \u0026ldquo;variance\u0026rdquo; on both sides seems tedious. Alternatively, we can apply Delta method.\n$$\\begin{align} \\hat{A} \u0026= \\frac{\\sum_j (z_j - \\bar{z}) \\epsilon_j}{n} \\nocr \\hat{B} \u0026= \\frac{\\sum_i (z_i - \\bar{z})^2}{n} \\nocr \\sqrt{n} (\\hat{A} - 0) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\var(z)\\var(\\epsilon)) \\text{, since $z \\independent \\epsilon$} \\nocr \\sqrt{n} (\\hat{B} - \\var(z)) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\tau^2) \\nocr \\sqrt{n} (\\frac{\\hat{A}}{\\hat{B}} - 0) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\nabla g(A, B)' \\Sigma \\nabla g(A, B)') \\nocr \\text{, where } \\nabla g(A, B) \u0026= \\begin{bmatrix} 1/B \\cr 0 \\end{bmatrix} \\nocr \\Sigma \u0026= \\begin{bmatrix} \\var(z) \\var(\\epsilon) \u0026 \\cov(A, B) \\nocr \\cov(A, B) \u0026 \\tau^2 \\end{bmatrix} \\nocr \\text{therefore, } \\nabla g(A, B)' \\Sigma \\nabla g(A, B)' \u0026= \\frac{\\var(z) \\var(\\epsilon)}{B^2} \\nocr \u0026= \\frac{\\var(\\epsilon)}{\\var(z)} \\nocr \\text{thus, } \\var(\\hat{b}_{zy}) \u0026= \\var(\\frac{\\hat{A}}{\\hat{B}}) \\nocr \u0026= \\frac{\\var(\\epsilon)}{n \\var(z)} \\label{eq:varb} \\end{align}$$ So that we obtain the first \u0026ldquo;equal sign\u0026rdquo;: $SE = \\sqrt{\\frac{\\sigma_\\epsilon^2}{n \\var(z)}}$\n Second \u0026ldquo;=\u0026rdquo;:  $$\\begin{aligned} \\E(z) \u0026= 0 \\times (1 - p)^2 + 1 \\times 2p(1 - p) + 2 \\times p^2 = 2p\\cr \\E(z^2) \u0026= 0 \\times (1 - p)^2 + 1 \\times 2p(1 - p) + 4 \\times p^2 = 2p(1 + p)\\cr \\var(z) \u0026= \\E(z^2) - \\E(z)^2 = 2p(1 - p) \\cr \\E(\\epsilon) \u0026= \\E(y - \\tilde{z} b_{zy}) = \\E(y) - b_{zy} \\E(\\tilde{z}) = 0 \\text{, since $\\E(y) = \\E(\\tilde{z}) = 0$}\\cr \\E(\\epsilon^2) \u0026= \\E((y - \\tilde{z} b_{zy})^2) = \\E(y^2) + b_{zy}^2 \\E(\\tilde{z}^2) - 2 b_{zy} \\E(y \\tilde{z}) \\cr \u0026= \\E(y^2) + \\E(\\tilde{z}^2) b_{zy}^2 - 2 b_{zy} [ b_{zy} \\E(\\tilde{z}^2) + \\E(\\tilde{z}) \\E(\\epsilon) ] \\cr \u0026= \\E(y^2) - 2p (1 + p) b_{zy}^2 \\cr \\sigma_{\\epsilon}^2 \u0026= \\E(\\epsilon^2) - \\E(\\epsilon)^2 \\cr \u0026= \\E(y^2) - 2p(1 - p) b_{zy}^2 \\cr \u0026= \\var(y) - 2p(1 - p) b_{zy}^2 \\cr \u0026= 1 - 2p(1 - p) b_{zy}^2 \\text{, since $\\var(y) = 1$} \\end{aligned}$$ , where $\\tilde{z} := z - \\E(z)$. Therefore,\n$$\\begin{aligned} SE \u0026= \\sqrt{\\frac{\\sigma_\\epsilon^2}{n \\var(z)}} \\cr \u0026= \\sqrt{\\frac{1 - 2p(1 - p)b}{2p(1 - p)n}} \\end{aligned}$$ , as the equation stated in supplementary notes page 9 (last one).\nWith z-score known, we have:\n$$\\begin{aligned} z_{zy} \u0026= \\frac{\\hat{b}_{zy}}{SE_{zy}} \\cr \\hat{b}_{zy} \u0026= z_{zy} SE_{zy} \\cr \u0026= z_{zy} \\sqrt{\\frac{1 - 2p(1 - p)b_{zy}}{2p(1 - p)n}} \\cr \\Rightarrow \\hat{b}_{zy}^2 \u0026\\approx z_{zy}^2 \\frac{1 - M \\hat{b}_{zy}^2}{Mn} \\text{, where $M = 2p(1 - p)$} \\cr \\Rightarrow \\hat{b}_{zy} \u0026\\approx z_{zy} / \\sqrt{2p (1 - p) (n + z_{zy}^2)} \\end{aligned}$$ , which is the result on page 10 top of supplementary notes.\nAccounting for linkage In practice, variants are correlated with each other because of LD. Therefore, the loci that is used for computation may show association if it is correlated with two causal variants which affect transcription and phenotype respectively. Figure 1b illustrated the scenarios.\n Three possible explanations of an association   It turns out that it is possible to filter out the \u0026ldquo;linkage\u0026rdquo; case. The paper suggested that if there is only one causal variant, the nearby region should have similar association signal. So, they proposed a hypothesis testing procedure to get rid of signal resulted from linkage. The following shows how they came up with the distribution under null hypothesis.\nLet\u0026rsquo;s say there are $k$ loci in LD to the causal loci which is denoted as loci 0. For the causal loci and the $i$th locus, we have (consider $y \\sim z$):\n$$\\begin{align} y \u0026= b_{yz(0)} z_{(0)} + \\epsilon_{(0)} \\nonumber\\cr y \u0026= b_{yz(i)} z_{(i)} + \\epsilon_{(i)} \\nonumber\\cr \\cov(y, z_{(i)}) \u0026= b_{yz(0)} \\cov(z_{(0)}, z_{(i)}) \\nonumber\\cr \\cov(y, z_{(i)}) \u0026= b_{yz(i)} \\var(z_{(i)}) \\nonumber\\cr \u0026 \\text{, by the fact that $\\epsilon_{(j)} \\independent z_{(i)}, j = 0, i$} \\nonumber\\cr \\Rightarrow b_{yz(i)} \u0026= b_{yz(0)} \\frac{\\cov(z_{(0)}, z_{(i)})}{\\var(z_{(i)})} \\nonumber\\cr \u0026= b_{yz(0)} r_{0i} \\sqrt{\\frac{\\var(z_{(0)})}{\\var(z_{(i)})}} \\nonumber\\cr \u0026= b_{yz(0)} r_{0i} \\sqrt{h_0 / h_i} \\label{eq:1} \\end{align}$$ , where $h = 2p (1 - p)$ which is precisely $\\var(z)$. So equation 7 follows. Intuitively, consider the following situation:\ngraph LR; z(z) --- x(x) x --- y(y)  In this case, $b_{zy} = b_{xy} r_{zx} SE_x / SE_y$. Namely, if we know the dependency between $x$ and $z$, so does $x$ and $y$, then we can derive the dependency between $z$ and $y$. $r_{zx}$ indicates to what extent the effect size of $x$ on $y$ can be transferred to the one of $z$ on $y$. $SE_x / SE_y$ is a rescaling term. To match the same scale (namely $y$), $b \\times SE$ should of the same scale for $z \\sim y$ and $x \\sim y$. Therefore, under the null hypothesis, $\\hat{b}_{zy(i)}$ should have the same expected value. To make inference, we need some distribution of $\\vec{b}_{zy}$, so we should know the variance/covariance as well.\n Side note:   I just found that the above derivation has a subtle drawback. It is that suppose we do $\\cov(\\cdot, z_{(0)})$ instead and do it carelessly, it will fail. The problem comes from the term $\\cov(\\epsilon_{(i)}, z_{(0)})$. From the derivation above, it seems to me that $z_{(0)}$ and $z_{(i)}$ are equivalent and interchangeable but it is really not the case. The graph above has pointed out such difference but in an implicit way. A more explicit illustration is the following:\n graph LR; z(z) --- x(x) x --- y(y) e0(e0) -- y e1(e1) -- x   The corresponding equations are:\n $$\\begin{aligned} y \u0026= b_0 x + \\epsilon_0 \\cr x \u0026= a z + \\epsilon' \\cr \\Rightarrow y \u0026= b_0 (a z + \\epsilon') + \\epsilon_0 \\cr \u0026= b_0 a z + b_0 \\epsilon' + \\epsilon_0 \\cr \u0026:= b_1 z + \\epsilon_1 \\text{, where $\\epsilon_1 = b_0 \\epsilon' + \\epsilon_0$} \\end{aligned}$$  , from which it is easy to see that $\\cov(\\epsilon_{(i)}, z_{(0)}) \\neq 0$\n From the result of $\\eqref{eq:1}$, we have:\n$$\\begin{align} b_{xy(i)} \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\nocr \u0026= \\frac{b_{zy(0)}r_{i0}\\sqrt{h_0 / h_i}}{\\beta_{zy(0)}\\gamma_{i0}\\sqrt{\\eta_0 / \\eta_i}} \\nocr \u0026= \\frac{b_{zy(0)}}{\\beta_{zy(0)}} \\label{eq:2}\\cr \u0026= b_{xy(0)} \\nocr \\end{align}$$ , where \\eqref{eq:2} follows from the fact that $r_{0i}$ and $\\gamma_{0i}$ are simply the property of the locus 0 and locus i, so that $r_{0i} = \\gamma_{0i}$. The same logic follows for $h$ and $\\eta$.\nThis result implies that $\\hat{d}_{i} := \\hat{b}_{(i)} - \\hat{b}_{(0)}$ has mean zero. Then, under the null, $(\\hat{d}_{1}, ..., \\hat{d}_k) \\approx \\mathcal{N}(0, R)$ (as stated in the text page 9 top left). $R$ is the variance/covariance matrix with entry $\\cov(\\hat{d}_i, \\hat{d}_j)$.\n$$\\begin{align} \\cov(x - y, z - y) \u0026= \\E((x - y)(z - y)) - \\E(x - y)\\E(z - y) \\nocr \u0026= \\E((x - y)(z - y)) - [\\E(x) - \\E(y)][\\E(z) - \\E(y)] \\nocr \u0026= \\E(xz) - \\E(x)\\E(z) - [\\E(yz) - \\E(y)\\E(z)] \\nocr \u0026- [\\E(xy) - \\E(x)\\E(y)] + \\E(y^2) - \\E(y)^2 \\nocr \u0026= \\cov(x, z) - \\cov(y, z) - \\cov(x, y) + \\var(y) \\nocr \\cov(\\hat{d}_i, \\hat{d}_j) \u0026= \\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}) - \\cov(\\hat{b}_{(i)}, \\hat{b}_{(0)}) \\nocr \u0026- \\cov(\\hat{b}_{(j)}, \\hat{b}_{(0)}) + \\var(\\hat{b}_{(0)}) \\label{eq:d} \\end{align}$$ From \\eqref{eq:d} (as stated in equation 8), we need to compute $\\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}), \\forall i, j = 0, 1, ..., k$. The derivation of this term is sketched in the supplement part 3. The missing part seems unclear for me, so I derive it great details as follow.\nThe unclear thing is how to get $\\E(g(\\hat\\theta))$ using Delta method. For $\\var(g(\\hat\\theta))$, it is straight forward, but not so for the first order term because $\\E(g(\\hat\\theta)) \\approx g(\\theta)$ is what we commonly use in practice. But if we take a closer look at Delta method, we will be sure that we can do more than this (see post about Delta method). That is to use higher order approximation.\n$$\\begin{align} g(X) \u0026= g(\\mu) + g'(\\mu)(X - \\mu) + g''(\\mu) \\frac{(X - \\mu)^2}{2!} + o((X - \\mu)^2) \\nocr \\E(g(X)) \u0026= g(\\mu) + \\E(o(X - \\mu)) \\label{eq:3}\\cr \\E(g(X)) \u0026= g(\\mu) + \\frac{g''(\\mu)}{2} \\E((X - \\mu)^2) + \\E(o((X - \\mu)^2)) \\nocr \u0026\\approx g(\\mu) + \\frac{g''(\\mu)}{2} \\var(X) \\label{eq:4} \\end{align}$$ \\eqref{eq:3} is commonly used approximation and \\eqref{eq:4} is a more \u0026ldquo;accurate\u0026rdquo; one. The corresponding multivariate version is simply:\n$$\\begin{align} g(X) \u0026\\approx g(\\mu) + \\frac{1}{2} \\langle H(g(\\mu)), \\Sigma_X \\rangle \\nocr \\end{align}$$ , where $H(g(X))$ is the Hessian of $g$ evaluated at $\\mu$ and $\\Sigma_X$ is the variance/covariance matrix of $X$ and $\\langle \\cdot, \\cdot \\rangle$ is the matrix inner product. Now, we can apply this rule for deriving $\\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)})$.\n$$\\begin{align} \\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}) \u0026= \\E(\\hat{b}_{(i)} \\hat{b}_{(j)}) - \\E(\\hat{b}_{(i)}) \\E(\\hat{b}_{(j)}) \\nocr \\E(\\hat{b}_{(i)}) \u0026= \\E(\\hat{b}_{xy(i)}) = \\E(\\frac{\\hat{b}_{zy(i)}}{\\hat\\beta_{(zx(i))}}) \\nocr \u0026\\approx \\frac{b_{zy(i)}}{\\beta_{zx(i)}} + \\frac{1}{2} \\langle \\begin{bmatrix} 0 \u0026 -1 / \\beta_{zx(i)}^2 \\cr -1 / \\beta_{zx(i)}^2 \u0026 2b_{zy(i)} / \\beta_{zx(i)}^2 \\end{bmatrix} , \\nocr \u0026 \\begin{bmatrix} \\var(\\hat{b}_{zy(i)}) \u0026 \\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) \\cr \\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) \u0026 \\var(\\hat{\\beta}_{zy(i)}) \\end{bmatrix} \\rangle \\nocr \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} - \\frac{\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}^2} + \\frac{b_{zy(i)}\\var(\\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}^2} \\nocr \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\bigg( 1 + \\frac{\\var(\\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}} - \\frac{\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)})}{b_{zy(i)}\\beta_{zx(i)}} \\bigg) \\label{eq:6} \\end{align}$$ \\eqref{eq:6} is stated in supplement page 10. For an association signal, $\\frac{\\hat\\beta}{\\var(\\hat\\beta)}$ should be big (namely the $\\chi^2$ statistic). Also, $\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) = 0$. Therefore,\n$$\\begin{align} \\E(\\hat{b}_{xy(i)}) \u0026\\approx \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\nonumber \\end{align}$$ This result gives nothing more than the first order approximation. But it does matter for $\\E(\\hat{b}_{xy(i)}\\hat{b}_{xy(j)})$. The intuition is that as more and more terms get involved, the first order approximation becomes worse and worse. So, in general, when too many terms involved, you need to be careful. If the (co)variance or Hessian is crazy, maybe it is a good idea to go beyond first order approximation.\nIt turns out that we need to compute $\\cov(\\hat{b}_{zy(i)}, \\hat{b}_{zy(j)})$ and $\\cov(\\hat\\beta_{zx(i)}, \\hat\\beta_{zx(j)})$. In the supplementary notes, it was stated as We know that the sampling correlation between the estimates of SNP effects equals to the LD correlation between the SNPs. But, again, this result is not intuitive to me. The following is a derivation of this:\nSuppose $z$ has been standardized and the true effect size is $b$. Then $y = b z_0 + \\epsilon$ where $z_0$ is the causal variant and $\\epsilon$ is the error term. We have:\n$$\\begin{align} \\hat{b}_{i} \u0026= z_i' y / n \\nocr \u0026= z_i' (b z_0 + \\epsilon) / n \\nocr \\text{similarly, } \\hat{b}_j \u0026= z_j' (b z_0 + \\epsilon) / n \\nocr \\cov(\\hat{b}_{i}, \\hat{b}_j) \u0026= \\frac{1}{n^2} \\big[ \\E(\\cov(z_i' (b z_0 + \\epsilon), z_j' (b z_0 + \\epsilon)| z_i, z_j)) \\nocr \u0026- \\cov(\\E(z_i' (b z_0 + \\epsilon)| z_i) \\E(z_j' (b z_0 + \\epsilon)| z_j)) \\big] \\nocr \\text{first term on RHS} \u0026= z_i' \\cov(bz_0 + \\epsilon, bz_0 + \\epsilon) z_j \\nocr \u0026= b^2 [\\var(z_{0k}) + \\var(\\epsilon)] \\E(z_i' z_j) \\nocr \u0026= A \\cov(z_{ik}, z_{jk}) \\nocr \u0026\\text{, where $z_{i1}, ..., z_{in}$ are iid and same for $j$} \\nocr \u0026 \\text{, and $A = b^2 \\var(z_0) \\var(\\epsilon)$ which does not depend on $i$ and $j$} \\nocr \\E(z_i'(bz_0 + \\epsilon)|z_i) \u0026= z_i' [b\\E(z_0) + \\E(\\epsilon)] = 0 \\nocr \\therefore \\text{second term on RHS} \u0026= 0 \\nocr \\therefore \\cov(\\hat{b}_{i}, \\hat{b}_j) \u0026= A \\cov(z_{ik}, z_{jk}) \\label{eq:sub1}\\cr \\text{a special case is: } \u0026 \\nocr \\var(\\hat{b}_i) \u0026= A \\var(z_{ik}) \\label{eq:sub2} \\cr \\eqref{eq:sub1}, \\eqref{eq:sub2} \\Rightarrow \\cor(\\hat{b}_i, \\hat{b}_j) \u0026= \\cor(z_{ik}, z_{jk})\\nonumber \\end{align}$$  Side note:   Note that \\eqref{eq:sub2} seems conflict with \\eqref{eq:varb}. But the difference is that \\eqref{eq:varb} is for causal variant effect size estimator and \\eqref{eq:sub2} is for the non-causal variants in LD. Intuitively, the association signal captured by non-causal variant comes from LD and only from LD. That\u0026rsquo;s why correlation of $\\hat{b}$ matches exactly to correlation of $z$.\nThis proof stuck me from a long time. The confusing part is that I cannot formalize the underlying model in a proper way. At first, I tried:\n $$\\begin{align} y \u0026= b_i z_i + \\epsilon_i \\nocr z_i \u0026= a z_j + \\epsilon_0 \\nocr \\Rightarrow y \u0026= a b_i z_j + b_i \\epsilon_0 + \\epsilon_i \\nonumber \\end{align}$$  I failed because it turns out that in this case the $A$ term will depend on the error term of $i$ and $j$. Essentially, this is not the correct underlying model of the problem. Neither $z_i$ nor $z_j$ have the real effect size well defined since they are not the causal variant. To make it physically make sense, it is necessary to consider $z_0$, the causal variant, somewhere in the calculation.\nAlso, a useful equation for deriving $\\cov$ is:\n $$\\begin{align} \\cov(f(X, Y), g(X, Z)) \u0026= \\E(\\cov(f(X, Y), g(X, Z)| Y, Z)) \\nocr \u0026- \\cov(\\E(f(X, Y)| Y), \\E(g(X, Z)| Z)) \\nonumber \\end{align}$$  This equation divides the problem into simpler and easier to deal with chunks, especially when $Y$ and $Z$ introduce dependency, which complicates the calculation. This equation can tease such dependency apart and work on them after simplifying the expression a bit.\n After we obtain the distribution of $\\hat{d}_1, ..., \\hat{d}_k$, the normalized version is $z_i := \\hat{d}_i / \\sqrt{\\var(\\hat{d}_i)}$ with normalized (co)variance. The test statistic is constructed as $T_{HEIDI} = \\sum_i z_i^2. The CDF of $T_{HEIDI}$ can be computed approximately (Satterthwaite or Saddlepoint as stated in the text).\nResults in brief In practice, they used blood eQTL data because of its large sample size. For each gene (only probe), they used the top associated cis-eQTL with HEIDI test to filter out linkage cases. For 5 complex traits, they identified 104 significant genes. Some insightful results are listed below:\n They pointed out that the loci which shows trait-associated gene signal (namely passed the tests in the paper) tends to be found in the future GWAS as the sample size increases. Second, SMR helps to pinpoint functionally relevant genes. In GWAS, many locus are close to multiple genes and SMR can distinguish them by considering whether the loci affects the gene expression. eQTL analysis are tissue-specific, but throughout the paper, they used blood eQTL results. They pointed out that many eQTL are shared across tissues, which benefits most by this strategy. But some tissue-specific eQTL is missed unavoidably. They showed that the signals identified in blood data were consistent with the results in brain data for schizophrenia GWAS (brain signal is weak due to power issue). But this indicates that SMR in unmatched tissue does not give fake signal. They did SMR with $x$ as expression in blood and $y$ as expression in brain for significant locus. They showed pleiotropic association as well. Also, the effect size learned from blood data can explain the same amount of variance in brain expression data as brain eQTL analysis did. These evidences showed that variants shared across tissues can be successfully captured by SMR even unrelated tissue eQTLs were used. Multiple tagging probes for a single gene is common in eQTL analysis. Probes tag the transcript and ideally we want every tag of the same gene gives consistent result, but it is not always the case (which might be significantly different from each other). They pointed out that such in-consistency may come from the fact that probes are tagging different types of transcripts from the same gene. But they failed to provide further evidence for this argument and they found that probes were not enriched in region close to transcription end site which is the place enriched for alternative splicing. HEIDI test assumes one causal variant per locus which may not be true in practice and non-pleiotropic variant may dilute the signal. So, they performed conditional analysis to overcome this issue. Not sure how they did this analysis. But I will read and post the note of the referred paper (see link). Added on 12/19/17: The idea of conditional analysis is to remove the effect of secondary variant in the region. Conditioning on the potential secondary variant, the effect size and HEIDI analysis were performed. This analysis can remove dilution effect caused by non-pleiotropic variant.   Side note \u0026ndash; the hypothesis test in 4)   Suppose $\\beta_{i1}, \\beta_{j2}$ are two effect size of $x \\sim z$, where $i, j$ denote SNP index and $1, 2$ denote probe index. LS estimator is \\hat\\beta_{i1} = $z_i' x_1$ and the same for $j2$ for standardized $z_i, z_j, x_1, x_2$. We have $\\var(\\hat\\beta) = n \\var(x) \\var(z)$. And covariance is:\n $$\\begin{align} \\cov(\\hat\\beta_{1i}, \\hat\\beta_{2j}) \u0026= \\E(\\cov(z_i' x_1, z_j' x_2 | z_i, z_j)) - \\cov(\\E(z_i' x_1 | z_i)\\E(z_j' x_2 | z_j)) \\nocr \u0026= \\E(z_i' z_j) \\cov(x_1, x_2) \\nocr \u0026= n \\cov(z_i, z_j) \\cov(x_1, x_2) \\nocr \u0026= r_{ij} r_x \\sqrt{n \\var(x_1) \\var(z_i) \\times n \\var(x_2) \\var(z_j)} \\nocr \u0026= r_{ij} r_x \\sqrt{\\var(\\hat\\beta_{1i}) \\var(\\hat\\beta_{2j})} \\nonumber \\end{align}$$ Discussion HEIDI cannot distinguish linkage and pleiotropy if the two causal variants are in high LD (in this case, the data is not informative). To distinguish causality and pleiotropy, they proposed that multiple independent SNPs would help but it is a bit unclear how to construct the null distribution. Nonetheless, SMR provided a prioritized list of gene candidates for experimental validation. Furthermore, besides eQTL, other quantitative trait can be used, such as meQTL, pQTL, etc.\n"
},
{
	"uri": "/notebook/posts/first-test/",
	"title": "TEMPLATE",
	"tags": ["tag"],
	"description": "",
	"content": " Meta data of reading  Journal: JOURNAL Year: YEAR DOI: DOI  "
},
{
	"uri": "/notebook/about/",
	"title": "About",
	"tags": [],
	"description": "",
	"content": "github repository\nTheme is derived from hugo-theme-cactus-plus.\nThe page is rendered by blogdown\n"
},
{
	"uri": "/notebook/categories/cat/",
	"title": "Cat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/causality/",
	"title": "Causality",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/complex-trait/",
	"title": "Complex Trait",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/conditional-analysis/",
	"title": "Conditional Analysis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/eqtl/",
	"title": "Eqtl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/gwas/",
	"title": "Gwas",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/integrative-analysis/",
	"title": "Integrative Analysis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/linkage-disequilibrium/",
	"title": "Linkage Disequilibrium",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/mendelian-randomization/",
	"title": "Mendelian Randomization",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/method-note/",
	"title": "Method Note",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/",
	"title": "Note Archive",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/posts/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/research-paper/",
	"title": "Research Paper",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/tag/",
	"title": "Tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/target-gene/",
	"title": "Target Gene",
	"tags": [],
	"description": "",
	"content": ""
}]
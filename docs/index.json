[
{
	"uri": "/notebook/posts/riordan-ajhg-2017/",
	"title": "From Peas to Disease: Modifier Genes, Network Resilience, and the Genetics of Health",
	"tags": ["epistasis", "complex trait"],
	"description": "",
	"content": " Meta data of reading In brief Modifier genes Modifier effects and functional significance From modified phenotypes to modifier genes Features of modifier genes Modifiers as mediators of network resilience Summary   Meta data of reading  Journal: AJHG Year: 2017 DOI: 10.1016/j.ajhg.2017.06.004   In brief This paper reviewed the principles of modifier genetics and assess progress in the studies of modifier genes and their targets in both simple and complex traits. They proposed that the modifier effects come from gene interaction network which depends on genetic background and it is of great use for *prevent, stabilize, and reverse disease and dysfunction.\n Modifier genes The causal gene does not act in isolation so the phenotypic outcome depends not only on identified causal variants but also on some related genetic variants (here “related” often means “related in functional network”). Those “related” genes are called modifier genes and they are often the results of: i) physical interaction; ii）mechanistic contribution (i.e. involved in the same biological pathway); iii) functional compensation (i.e. the modifier gene acts in redundant/alternative pathway of the causal one).\nTo experimentally detect modifier genes, we need the common functional variants with similar environmental factors in two populations with distinct genetic background.\n Modifier effects and functional significance Principles of modifier Genetics  The illustration of modifier gene   The effect of modifier and target genes on phenotype are not additive (i.e. there is genetic interaction between the two). As shown in the above figure, modifier itself would not lead to phenotype. Taking another view, phenotype is the outcome of the integration of not only so called disease-causing genes but also environmental context and genetic background.\n Types of target genes It can be single target gene ot multigenic but the number of modifier-affected genes are largely unknown thanks to the lack of knowledge of genetic architecture and gene regulatory network.\n Types of modifier effects Penetrance: The modifier gene changes the penetrance of the disease-causing variant Expressivity: The modifier gene changes the expressivity of the disease-causing variant Dominance: Dominance measures the genetic dosage required for a phenotype (complete dominant means heterozygous and homozygous have exactly the same phenotype). The modifier gene changes the dominance of the disease-causing variant Pleiotropy: When the modifier gene presents, it induces novel phenotypes   Modifiers of complex traits For single-gene phenotype, modifier study use multigenic background among a population sharing the same target variant. But for complex trait, only single modifier variant can be tested in a genetically clean population for differential phenotype. Note that the modifier effect is also context-dependent.\n  From modified phenotypes to modifier genes To identify genetic modifier in humans Two strategies are i) Look for shared variants between individuals with disease-causing variants without disease; iii) Perform target sequencing on disease gene to identify candidate with causal variant but phenotypically unaffected.\nThrough sequencing, researchers have found the reason of low penetrance of a disease-causing variant in a family where the occurrence of a common variant along with the disease-causing variant reaches 100% penetrance. Also, another research reported that the over-expression of one gene is protective to a LoF variant of disease gene.\nGWAS type of study takes the strategy that collect a population which share the same disease-causing variant but with various phenotypic outcome (severity, age of onset, etc). Also, the same strategy can be used in family-based study (linkage study) where the signal-noise ratio is better.\n To identify genetic modifiers in mice …\n  Features of modifier genes Nature of sequence variation The number of identified modifier variants is small but it has suggested considerable heterogeneity. The following lists several outcomes of modifier variants:\nPartial or complete loss of function (missense mutation or gene deletion) Enhanced function (increased protein stability) Gain of alternative function (novel protein-protein interaction)  Currently discovered modifier variants often occur in protein-coding exon but may act as regulatory variant (UTRs or promoters). In general, any variant that affect gene function by affecting interaction with other genes and networks is modifier variant.\n Nature of modifier functions One interesting question is that whether modifier genes tend to alter a general regulatory process or affect the target gene locally. It turns out that the latter is the case according to current data.\n Mutational buffering Here the authors discussed a general mechanism to suppress the effect of deleterious mutations by chaperones. But there is not mapping study showing that chaperone is modifier gene, but it is clear that the change of chaperone can modify the effect of a causal variant.\nGlobal modifier refers to the genes that regulate the outcome of variant via general mechanisms, such as chaperones, chromatin modifiers, and transcriptional regulators.\nIt seems that global modifiers respond to generalized stress and modifier genes target specific dysfunctions.\n  Modifiers as mediators of network resilience Modifier effect can be seen as an consequence of causal variant to compensate or enlarge the effect of the target variant on phenotype. Another view is that modifier effect indicates the network interaction (it can be molecularly direct interaction or related functionally, i.e. in the related pathway).\nSo, such coordination between target variant and modifier should be context-dependent (i.e. genetic background, environmental factors). The number of deleterious mutations per generation is hundreds in human including disease-causing ones but human is still alive. One possibility is that some genes act as the buffering system (latent modifiers) to counteract these deleterious variants in a genetic background-specific and environment-specific manner.\nThe complexity is that whether the gene is target, modifier, or QTL depends on the nature of the variant, genetic background, and environmental context. Network feature can provide substantial information but it is unavailable right now.\n Summary Modifier effect is pervasive and it works coordinately with target genes. Its occurrence indicates that the functionally related role between the target and modifier in gene network. Modifier effect provides a mechanism in the network to overcome the deleterious variant occurring naturally in individuals.\n "
},
{
	"uri": "/notebook/posts/kammenga-febs-2017/",
	"title": "The background puzzle: how identical mutations in the same gene lead to different disease symptoms",
	"tags": ["epistasis", "complex trait"],
	"description": "",
	"content": " Meta data of reading Terminology Experimental evidence of genetic background effects Environmental effects Stochastic and epigenetic effects Summary   Meta data of reading  Journal: FEBS Year: 2017 DOI: 10.1111/febs.14080   Terminology This paper discusses the current insights about the phenotypic variation caused by gene interaction, epigenetics and stochasticity in model organism.\nPenetrance and expressivity Penetrance (according to wikipedia) is the proportion of individuals with the variant (allele) that also express an associated trait (phenotype).\nExpressivity (according to this note) is the degree to which the trait expression differs among individuals. For instance, there is a polydactyly (extra toes) trait in inbreeding cats. Individual carriers may have different number of extra toes. Therefore expressivity is only meaningful when the trait is quantitatively measurable in some sense. An extreme case is that the trait/disease is binary, in which the expressivity for an individual is the trait/disease state. And its population average is penetrance.\n Genetic background effect and examples It has been found that there are cases where the individual carries the Mendelian disease causing mutation but has no disease symptom. It implies that the incomplete penetrance may depend on the genetic background. The genetic background includes the genes that affect/interact with the disease causing genes.\nAlso, the same disease with the same mutated genes (involves multiple loci), the expressivity is different among loci, which can also result from the various genetic background.\nCystic fibrosis is monogenic disease. 50% patients carry the same disease causing allele but the disease symptoms are highly variable because of the modifier background genes. There is also case where pedigree-specific phenotype is found. In such case, the same exon deletion causes one syndrome in one pedigree but a more severe one in another.\n  Experimental evidence of genetic background effects Single gene mutations depend on multiple background loci The idea is that the expressivity and penetrance of a mutation \\(M\\) is the function of \\(M\\) and the interaction between \\(M\\) and \\(B\\). There are experimental support of \\(M \\times B\\) effect in fruit flies and mice.\n Genome-wide and molecular insights into background interactions Using RNAi researchers have looked for the background interaction in \\(C. elegans\\). The studies indicate that\nThe effect of background interact is small individually but big together. The effect may depend on the expression level of the affected gene   Cryptic genetic variants Cryptic genetic variants are SNVs that show phenotypic effect under atypical conditions. Such studies imply that gene regulatory network may play a role in genetic background effects. Also, disease-related signaling pathway may be enriched for CGVs.\n The contribution of genetic background is ignored by current genetic research The effect of genetic background can confound the result so geneticists tend to remove such confounding factor by using organisms with single genetic background. This strategy is to maximize the reproducibility but it miss the genetic background effect which is of great importance in understanding the phenotypic variability of human diseases.\nThe author points out the general experimental framework to study genetic background effect using model organism. With two populations, \\(M \\times B_1\\) and \\(M \\times B_2\\) where both of them have disease causing mutation but only one of them has phenotype. By crossing the two populations and performing linkage analysis, the locus associated with the background effect can be identified.\nBesides genetic background effects, the phenotypic effects of a mutation can differ in the population with the same genetic background due to environmental effects and stochastic effects.\n  Environmental effects \\(G \\times E\\) interactions also contribute to disease risk. It has been found the cholesterol level and cardiovascular disease have different effects across generations. Also the genetic background and environmental factors (diet) act jointly on the outcome.\n Stochastic and epigenetic effects Chaperon acts as the mutation buffering system which can decrease the penetrance (the function of chaperon can also be affected by environment, i.e. stress). Also, epigenetic effect can lead to different outcomes in individuals with the same genetic background.\n Summary  Illustration of the genetic background effect   Genetic background acts through gene-gene interaction, epigenetics adn stochasticity and it is as important as disease-causing mutations. Research also finds that the mutations in the same gene can increase the severity of many complex psychiatric disorders, which indicates that the effect of genetic background can be pervasive and complex.\n "
},
{
	"uri": "/notebook/posts/epistasis-nrg-2014/",
	"title": "Detecting epistasis in human complex traits",
	"tags": ["epistasis", "gwas", "complex trait"],
	"description": "",
	"content": " Meta data of reading Motivation Methods for detecting epistasis Overview of empirical evidence for epistasis Summary   \\[ \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\\\} \\newcommand\\var{\\text{Var}} \\newcommand\\logit{\\text{logit}} \\]\nMeta data of reading  Journal: Nature Review Genetics Year: 2014 DOI: 10.1038/nrg3747   Motivation Under GWAS framework, the causal variants are found with independent with additive and cumulative effects. But such assumption is arguably unrealistic. So it is necessary to consider the case where variants do not act independently (genetic interaction, epistasis). In this review, the authors refer:\nFunctional effect: The general observation that effect of a particular variant depends on the genotype of another variant Statistical effect: The interaction variance that can be explained by a combination of causal variants that is not due to their independent effects (namely, \\(1 + 1 \\ne 2\\))  , where epistasis is referred to statistical effect.\nThe authors point out that the presence of functional epistasis does not automatically imply the presence of substantial statistical epistasis and vice versa. By considering epistasis or not, there are two types of heritability:\nNarrow-sense heritability (\\(h^2\\)): The estimated proportion of the phenotypic variance of a trait that is attributable to independent, additive genetic effects Broad-sense heritability (\\(H^2\\)): The estimated proportion of the phenotypic variance of a trait that is attributable to all additive and non-additive genetic effects  The authors point out the reason why epistasis is often under debate (its importance). There are two major goals of GWAS:\nIdentify causal variants of a particular trait to understand underlying biology Estimate the effects of causal variants to predict phenotypic outcomes  Functional epistasis can benefit the first goal and statistical epistasis can benefit the second goal.\nThis paper surveys the methodology and software of the detection of epistasis along with the latest empirical evidence for the importance of epistasis and potential utility in searching for genetic interaction.\n Methods for detecting epistasis There have been methods developed to detect whether two or more loci differs from that predicted by their individual effects. Most methods use SNPs for pairwise or higher-order interactions in GWAS data. There are two strategies:\nHypothesis-free: Namely exhaustive search. Test for all possible combinations in all SNPs which involves billions of tests. It is both computationally expensive and statistically challenging Hypothesis-driven: Test for a subset of SNPs and/or epistasis  Regression-based method The idea is to compare the saturated model (\\(L_{S}\\)) and reduced model (\\(L_R\\)) (considering interaction term between SNPs or not). For instance, in case-control study, \\(L_S\\) can be:\n\\[\\begin{align} \\logit(Pr(Y = 1)) \u0026amp;= \\alpha + \\beta x_1 + \\gamma x_2 + \\delta x_1 x_2 \\nonumber \\end{align}\\] , where \\(x_1, x_2\\) are binary variables and \\(\\delta\\) can be parameterized by four numbers according to the four possible configuration of \\(x_1, x_2\\).\nSeveral progresses have been made to overcome the computational load. First, advanced data structure and parallelization have been introduced. Second, approximate tests have been applied, such as F ratio and Kirkwood superposition approximation (for \\(L_S\\) versus \\(L_R\\) tests under the assumption of HWE).\nAlthough the hypothesis-free regression-based methods have become computationally tractable, they still suffer from low power (needs big sample size). Therefore, the practical compromise is to focus on the SNPs with genome-wide association signal. Moreover, the power of the test is a function of: i) interaction effect size; ii) sample size; iii) linkage disquilibrium. So, dense marker data can benefit the analysis as well.\n LD- and haplotype-based methods The idea is to test whether the co-occurrence of SNPs is enriched in cases relative to controls. It is computationally faster and statistically more powerful than regression-based methods. It works well for unlinked loci in rare diseases.\nFor instance, the LD of the pair of SNPs can be compared for cases and controls to see if the difference is significant (LD-based method). Haplotype-based methods adapted from LD-based one with equal power. With GWAS data, the linkage phase should be inferred before performing this test.\nLD-based methods may generate inflated false positives because HWE does not always hold genome-wide. In LD-based method, a Z-score statistic is used from the difference in Pearson correlation in cases and controls. But simulation suggests that Z-score is inflated when two SNPs are highly correlated and/or both have significant marginal effects.\nHaplotype-based methods incorporates a weighted average of the joint effects of two SNPs can control false positives when only one SNPs has marginal effects. This limitation can be overcome by using full logistic regression model (also with correction of covariates). So, a two-step approach where i) genome-wide screening with Z-score statistic (high power); ii) regression-based model to test the most promising interactions (low false positives).\n Bayesian methods The idea is to compute the posterior probability for the SNP to be i) unassociated; ii) associated by marginal effect; iii) associated by joint effect. Combining Bayesian framework and GLM allows tests of SNP interactions with the consideration of covariates, marginal effect, and gene-environment interaction. Also, Bayesian approach can average multiple models when the underlying interaction patterns are unknown.\n Data-filtering methods Variance heterogeneity refers to the case where for a single bi-allelic SNP, the three genotypes have difference conditional phenotic variance. It can be used to select potentially interacting SNPs (since variance heterogeneity is a necessary condition for genetic interaction). But it suffers limited detectable variance heterogeneity.The cautions are:\nBiases introduced by algorithm Publication bias in existing knowledge Context dependence Filtering threshold may change the null distribution of test statistic   Artificial intelligence algorithms …\n Group- and module-based methods The idea is to group SNPs into functional modules before performing tests, which can substantially reduce the statistical burden. A common practice is to group SNPs by gene and derive gene-based variable that factors in SNP-based correlations for regression-based methods or for analogues LD-based approaches. Another strategy is to compute pairwise interactions between groups and derive gene-based interaction p-value by integrating all pairwise p-values. Additionally, with pairwise interaction statistics between groups along with marginal effects, the gene-gene interaction can be analyzed using network analysis algorithms (SNPrank).\nThe authors point out that such gene-based method benefits from imputed genotypes from external LD information since it may capture unobserved causal variant. The implicit assumption behind gene-based method is “no intragenic interaction”.\n Multitrait and multilevel integration Pleiotropic epistasis is statical interaction signals shared in multiple traits. It can be identified by looking for SNP-SNP interaction that are shared across multiple traits or introducing composed “trait” (combining multiple traits) as latent variable in Bayesian framework. For example, a BEAM-derived method uses three latent variables: gene expression, SNPs, and individuals.\n Summary and future directions  Summary of reviewed methods for detecting epistasis   All methods mentioned above take genotyped SNPs and they cannot handle imputed SNPs with uncertainty. But this is useful in meta-analysis of epistasis which needs to be developed. Current methods do not consider sex chromosome.\n  Overview of empirical evidence for epistasis Hypothesis-free studies People have done genome-wide epistasis scanning using WTCCC data. It turns out that the majority of the statistical interactions were in MHC region affecting T2D or RA. Such signals may be caused by haplotype effects where statistical interaction pair together tags a causal variant in proximity.\nThe result also indicated that many epistasis effect has multiplicative pattern, namely the effect of marginal additive term is bigger than the expected, and such effect can be removed by changing the scale of the trait measured (scale effect).\nBut scale effect happens in two marginally non-significant SNPs, which indicates that epistasis analysis may increase the power of marginal effect analysis. It is possible when two SNPs has small effect but their interaction term has big effect. Taking this idea, researchers used additive \\(\\times\\) additive model to scan the genome and identified similar results as the above one.\nFor traits as gene expression which have bigger genetic effect, researchers used BSGS data and identified 501 epistasis effect in discovery stage but only 30 could be replicated. The issue came from the unobserved causal variants (as discussed previously), which can drive the epistasis signal. The problem cannot be fixed even after filtering on LD. One important conclusion was that even corrected for power the attributable additive effects was still far more bigger than non-additive ones. The authors thought that since the current studies show few evidence of replicated non-additive effects in genome-wide analysis, the non-additive effect with large effect size is unlikely to exist.\n Hypothesis-driven studies The hypothesis-driven studies can reduce the statistical burden substantially. The authors point out that the hypothesis-driven results also suffer from lack of replication.\nThe authors list several successful examples. In Alzheimer’s disease, FYN and RNF219 decrease the risk only if APOE4 mutant exists. Here the strategy is to limit the search to genetic effect that affects endophenotypes. Endophenotype is referred as the heritable traits that are genetically correlated with disease traits. They are often traits (such as the level of a metabolite or transcript) that can be measured in all individuals (both diseased and healthy) and that can potentially provide a predictor of disease status.\nTwo interacting SNPs in HLA-DR2 affects the risk of multiple sclerosis toghether. The epistasis effect has been confirmed experimentally.\nRegarding the pattern of interaction. If variant A increases risk only when variant B exists, the underlying mechanism might be that there are two redundant pathways (A in one and B in the other). Another pattern is that variant A increases risk only when variant B absents. It corresponds to the case where A and B are in the same pathway. Therefore, one strategy to propose the testing pairs is to make use of knowledge on biological function.\nAlternatively, one can only test on variants with large marginal effect. There are cases where this strategy works. But it can also fail (no significant results for T2D, BMI, serum uric acid levels under current results).\nThe authors mention the pitfall of the current epistasis studies. They point out that the analysis is done on observed scale and an interaction that is non-additive in observed scale can be addtive in liability scale since even if variants contribute additively, if the disease occurs under some threshold, it appears to be epistasis under current analysis. It turns out that such effect is largest when disease prevalence is small.\n  Summary Little evidence of statistical epistasis comparing to additive model and the genetic contribution substantially smaller than the additive one, especially for the case of pairwise epistasis between SNPs. However, hypothesis-driven analysis (based on biological function) is more successful than the genome-wide scanning one . It indicates that functional epistasis does exist. But the current progress has made genome-wide scanning (hypothesis-free tests) easily achievable.\nThe future direction is to develope meta-analysis method and to consider multilocus epistasis.\n "
},
{
	"uri": "/notebook/posts/variance-component-test-glmm-1997/",
	"title": "Variance component testing in generalised linear models with random effects",
	"tags": [""],
	"description": "",
	"content": " Meta data of reading   Meta data of reading  Journal: Biometrika Year: 1997 DOI: 10.1093/biomet/84.2.309   "
},
{
	"uri": "/notebook/posts/optimal-skat-2012/",
	"title": "Optimal tests for rare variant effects in sequencing association studies",
	"tags": [""],
	"description": "",
	"content": " Meta data of reading Motivation The approach   \\[ \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\\\} \\newcommand\\var{\\text{Var}} \\newcommand\\diag{\\text{diag}} \\]\nMeta data of reading  Journal: Biostatistics Year: 2012 DOI: 10.1093/biostatistics/kxs014   Motivation Rare variants have very low MAF so GWAS doesn’t work under current sample size. To increase power, the strategy is to aggregate variants according to biological knowledge (or assumptions). There are two main types of tests: burden tests and non-burden tests. Burden test is region-based and it suffers from the assumption that all rare variants have the same direction of the effect and the power is low if there are too many non-causal variants. The non-burden test, sequence kernel association test (SKAT) does not make such assumption but it is less powerful than burden test if the assumption is almost true. The motivation of this paper is to propose a general version of hypothesis test which includes burden/non-burden tests as special cases. It provides a way to maximize the power no matter how the true model looks like.\n The approach They proposed to construct a class of test that is an arbitrary linear combination of burden test and SKAT statistics. The optimal test could be identified by maximizing the power within the class.\nThe goal is to test whether the rare variants in a single region are associated with a complex trait. Suppose sample size is \\(n\\) and the number of SNVs is \\(p\\). For \\(i\\)th subject, \\(y_i\\) is phenotype and \\(G_i = (g_{i1}, ..., g_{ip})\\) is genotype where \\(g_{ij} = 0, 1, 2\\). \\(X_i = (x_{i1}, ..., x_{iq})\\) is the covariates that are needed to be adjusted for. For continuous/categorical phenotypes, they used generalized linear model where \\(\\E(y_{i}) = \\mu_i\\) and \\(\\var(y_i) = \\phi \\nu(\\mu_i)\\) with a link function\n\\[\\begin{align} g(\\mu_i) \u0026amp;= X_i \\alpha + G_i \\beta \\label{eq:glm} \\end{align}\\] \\(\\nu(\\cdot)\\) is a variance function and \\(\\alpha, \\beta\\) are the vectors of regression coefficients for covariates and rare variants. With \\(\\eqref{eq:glm}\\), the test for association can be constructed as \\(H_0: \\beta = (\\beta_1, ..., \\beta_p)\u0026#39; = \\vec{0}\\). But such test suffers from large degree of freedom (d.f.) (\\(p\\)). To reduce d.f., additional assumptions were made.\nBurden-based tests The popular burden-based tests reduce the d.f. by making the assumptions that each \\(\\beta_j\\) is a function of the MAFs such that \\(\\beta_j = w(m_j) = w_j\\beta_0\\), where \\(m_j\\) is the MAF of the \\(j\\)th variant. With this assumption, \\(\\eqref{eq:glm}\\) becomes:\n\\[\\begin{align} g(\\mu_i) \u0026amp;= X_i \\alpha + \\beta_0 \\sum_{j = 1}^p w_j g_{ij} \\label{eq:burden} \\end{align}\\] Then the test \\(H_0: \\beta_0 = 0\\) has 1 d.f. The test was referred as weighted counting burden test (WBT).\n SKAT SKAT takes a different approach to reduce d.f. It assumes that each \\(\\beta_j\\) independently follows an arbitrary distribution with mean zero and variance \\(w_j^2 \\psi\\), where \\(w_j\\) is a fixed number that may depend on MAF. With this assumption, the null hypothesis \\(H_0: \\beta = \\vec{0}\\) is equivalent to \\(H_0: \\psi\\) under variance component test in GLMM. Suppose\n\\(X\\) is \\(n \\times q\\) \\(G\\) is \\(n \\times p\\) \\(W = \\diag(w_1, ..., w_p)\\)  \\(K = GWWG\u0026#39;\\) is \\(n \\times n\\) is an weighted linear kernel matrix. SKAT paper proposed to use a class of flexible weight functions of MAF, \\(w_j = \\text{Beta}(m_j, a_1, a_2)\\) where \\(a_1, a_2\\) were pre-specified and \\(m_j\\) were estimated using the sample MAF of the \\(j\\)th variant.\nDefine the working vector by \\(y^* = X\\alpha + \\Delta (y - \\mu)\\), where \\(\\Delta = \\diag\\{g\u0026#39;(\\mu_i))\\), and the variance matrix by \\(V = \\diag(\\phi \\nu(\\mu_i)[g\u0026#39;(\\mu_i)]\\}\\). The estimates under the null were: \\(\\tilde{y} = X\\hat\\alpha + \\hat\\Delta (y - \\hat\\mu)\\), \\(\\hat\\Delta = \\diag\\{g\u0026#39;(\\hat\\mu_i))\\), and \\(\\hat{V} = \\diag(\\phi \\nu(\\hat\\mu_i)[g\u0026#39;(\\hat\\mu_i)]\\}\\). Where all estimates were obtained under the null hypothesis. It turned out that the score test statistic of the variance component \\(\\psi\\) is\n\\[\\begin{align} Q \u0026amp;= (\\tilde{y} - X\\alpha)\u0026#39; \\hat{V}^{-1} K \\hat{V}^{-1} (\\tilde{y} - X\\hat\\alpha) \\nocr \u0026amp;= (y - \\hat\\mu)\u0026#39; \\hat\\Delta \\hat{V}^{-1} K \\hat{V}^{-1} \\hat\\Delta (y - \\hat\\mu) \\nonumber \\end{align}\\] This result follows from the derivation of the variance component test of GLMM in this paper equation 8.\n This paper proposed The weighted linear kernel was constructed under the assumption that \\(\\beta_j\\)s were independent. But it is not true if the region is of high risk to have deleterious mutations. This paper introduced a new family of kernels. They proposed to allow \\(\\beta\\) to follow a multivariate distribution with exchangable correlation structure. Let the correlation matrix of \\(\\beta\\) be \\(R_\\rho = (1 - \\rho)I + \\rho \\vec{1} \\vec{1}\u0026#39;\\). Then the SKAT test statistic becomes\n\\[\\begin{align} Q_{\\rho} = (y - \\hat\\mu)\u0026#39; \\hat\\Delta \\hat{V}^{-1} K_\\rho \\hat{V}^{-1} \\hat\\Delta (y - \\hat\\mu) \\nonumber \\end{align}\\] This family of statistic contains both WBT and SKAT:\n\\(\\rho = 0\\), it becomes SKAT \\(\\rho = 1\\), it becomes WBT  Then, \\(Q_\\rho = (1 - \\rho) Q_{SKAT} + \\rho Q_{burden}\\). For a fixed \\(\\rho\\), \\(Q_\\rho\\) follows a mixture of \\(\\chi^2\\) distributions. Namely if \\(\\lambda_1, ..., \\lambda_m\\) are eigenvalues of \\(\\hat{V}^{-1/2}K_\\rho\\hat{V}^{-1/2}\\), the null distribution of \\(Q_\\rho\\) can be closely approximated by \\(\\sum \\lambda_j \\chi_{1, j}^2\\), where \\(\\chi_{1, j}^2\\)s are iid \\(\\chi_1^2\\) random variables. The p-values can be obtained by matching moments or inverting characteristic function.\n Optimal test In practice, \\(\\rho\\) is unknown. This paper proposed the test statistic \\(T = \\inf_{0 \\le \\rho \\le 1} p_\\rho\\) to obtain tthe optimal performance. It turns out that \\(Q_\\rho\\) can be approximated by the mixture of two independent random variables (\\((1 - \\rho)\\kappa + \\tau(\\rho)\\eta_0, \\kappa = \\sum_k \\lambda_k \\eta_k + \\xi\\)) which can be further approximated. Let \\(q_{\\min}(\\rho)\\) denote the \\((1 - T)\\)th percentile of the distribution of \\(Q_\\rho\\). Then the p-value of \\(T\\) is\n\\[\\begin{align} 1 - \\Pr(Q_{\\rho_1} \u0026amp;\u0026lt; q_{\\min}(\\rho_1), ..., Q_{\\rho_b} \u0026lt; q_{\\min}(\\rho_b)) \\nocr \u0026amp;= 1 - \\E[\\Pr(\\kappa \u0026lt; \\min\\{(q_\\min(\\rho_nv) - \\tau(\\rho_\\nu)\\eta_0)/(1 - \\rho_\\nu)\\})|\\eta_0] \\end{align}\\] , which can be computed efficiently.\n  "
},
{
	"uri": "/notebook/posts/score-test/",
	"title": "Score Test",
	"tags": ["hypothesis testing", "score test", "neyman-pearson lemma"],
	"description": "",
	"content": " Resources Motivation Univariate case Multivariate case Appendix: Neyman-Pearson lemma An concrete example in biostatistics   \\(\\newcommand\\E{\\text{E}}\\) \\(\\newcommand\\nocr{\\nonumber\\\\}\\)\nResources Score test in wikipedia Meyman-Pearson lemma in wikipedia   Motivation It seems to me that some hypothesis testing methods use score test with which I am not familiar. This note intends to sketch the definition of score test and the intuition behind it with a concrete example in application.\n Univariate case The statistic \\(L\\) is the likelihood function depending on parameter \\(\\theta\\) and data \\(x\\). The score \\(U(\\theta)\\) is:\n\\[\\begin{align} U(\\theta) \u0026amp;= \\frac{\\partial \\log L(\\theta | x)}{\\partial \\theta} \\end{align}\\] Fisher information is:\n\\[\\begin{align} I(\\theta) \u0026amp;= -\\E \\bigg[ \\frac{\\partial^2}{\\partial \\theta^2} \\log L(X; \\theta) | \\theta \\bigg] \\end{align}\\] , where the expectation is taken under \\(X \\sim D(\\theta)\\) with some parametrized distribution \\(D\\).\nThe statistic to test \\(\\mathcal{H}_0: \\theta = \\theta_0\\) is \\(S(\\theta_0) = \\frac{U(\\theta_0)^2}{I(\\theta_0)}\\), which is asymptotically \\(\\chi_1^2\\) under \\(\\mathcal{H}_0\\).\n comment: note that the statistic does not depend on alternative hypothesis which is different from likelihood ratio test.\n  Most powerful test for small deviations Consider the case where we test \\(\\mathcal{H}_0: \\theta = \\theta_0\\) versus \\(\\mathcal{H}_1: \\theta_0 + h\\). By Neymann-Pearson lemma, the most powerful test statistic \\(T\\) is:\n\\[\\begin{align} T \u0026amp;= \\frac{L(\\theta_0 + h | x)}{L(\\theta_0 | x)} \\ge K \\nocr \\Leftrightarrow \\log L(\\theta_0 + h | x) \u0026amp;- \\log L(\\theta_0 | x) \\ge \\log K \\nocr \\log L(\\theta_0 + h | x) \u0026amp;\\approx \\log L(\\theta_0 | x) + h \\times \\bigg( \\frac{\\partial \\log L(\\theta | x)}{\\partial \\theta}_{\\theta = \\theta_0} \\bigg) \\text{, by Taylor expansion} \\nocr \\therefore \\log T \u0026amp;\\approx h \\times U(\\theta_0) \\nonumber \\end{align}\\] Therefore, the score test approximately uses the most powerful test statistic when the deviation is small (\\(\\theta_1 - \\theta_0\\) is small).\n  Multivariate case Suppose \\(\\hat{\\theta}_0\\) is the maximum likelihood estimate of \\(\\theta\\) under null hypothesis. Then\n\\[\\begin{align} U(\\hat\\theta_0)^T I(\\hat\\theta_0)^{-1} U(\\hat\\theta_0) \u0026amp;\\sim \\chi_k^2 \\nocr \\text{, where} U(\\hat\\theta_0) \u0026amp;= \\frac{\\partial \\log L(\\theta | x)}{\\partial \\theta} \\bigg|_{\\theta = \\hat\\theta_0} \\nocr I(\\hat\\theta_0) \u0026amp;= -\\E \\bigg( \\frac{\\partial^2 \\log L(\\theta | x)}{\\partial \\theta \\partial \\theta\u0026#39;} \\bigg|_{\\theta = \\hat\\theta_0} \\bigg) \\nonumber \\end{align}\\] asymptotically under \\(\\mathcal{H}_0\\), where \\(k\\) is the number of constraints imposed by \\(\\mathcal{H}_0\\).\n Appendix: Neyman-Pearson lemma The statement When performing a hypothesis test between two simple hypotheses \\(H_0: \\theta = \\theta_0\\) and \\(H_1: \\theta = \\theta_1\\), the likelihood ratio test which rejects \\(H_0\\) in favour of \\(H_1\\) when\n\\[\\begin{align} \\Gamma(x) \u0026amp;= \\frac{L(x|\\theta_0)}{L(x | \\theta_1)} \\le \\eta \\nocr \\text{, where} \u0026amp; \\Pr(\\Gamma(X) \\le \\eta | H_0) = \\alpha \\nocr \\end{align}\\]  Namely \\(\\eta\\) is defined such that the probability of rejecting \\(H_0\\) under \\(H_0\\) (type I error) is \\(\\alpha\\).\n is the most powerful test at significance level \\(\\alpha\\) for a threshold \\(\\eta\\). If the test is most powerful for all \\(\\theta_1 \\in \\Theta_1\\), it is said to be uniformly most powerful (UMP) for alternatives in the set \\(\\Theta_1\\).\n Comment: The term “powerful” means that the test has smallest type II error.\n  Proof Suppose \\(R_{NP}\\) is the rejection area of Neyman-Pearson test. Namely,\n\\[\\begin{align} R_{NP} \u0026amp;= \\{ x: \\Gamma(x) \\le \\eta \\} \\end{align}\\] By the definition of \\(\\eta\\), we have:\n\\[\\begin{align} \\int_{t \\in R_{NP}} L( t | \\theta_0 ) dt \u0026amp;= \\alpha \\label{eq:rnp} \\end{align}\\] For any other test with significance level \\(\\alpha\\), define \\(R\\) as the rejection area accordingly. Then\n\\[\\begin{align} \\int_{t \\in R} L( t | \\theta_0 ) dt \\le \\alpha \\label{eq:r} \\end{align}\\] The power is 1 - type II error. NP test’s type II error is\n\\[\\begin{align} \\int_{R_{NP}^c} L( t | \\theta_1 ) dt \u0026amp;= \\int_{R_{NP}^c \\cap R} L(t | \\theta_1) dt + \\int_{R_{NP}^c \\cap R^c} L(t | \\theta_1) dt \\nocr \\int_{R_{NP}^c \\cap R} L(t | \\theta_1) dt \u0026amp;\\le \\frac{1}{\\eta} \\int_{R_{NP}^c \\cap R} L(t | \\theta_0) \\text{, by the definition of $R_{NP}$} \\nocr \u0026amp;\\le \\frac{1}{\\eta} \\int_{R^c \\cap R_{NP}} L(t | \\theta_0) dt \\text{, by $\\eqref{eq:rnp} \\eqref{eq:r}$} \\nocr \u0026amp;\\le \\frac{1}{\\eta} \\int_{R^c \\cap R_{NP}} \\eta L(t | \\theta_1) dt \\text{, by the definition of $R_{NP}$} \\nocr \\therefore \\int_{R_{NP}^c} L( t | \\theta_1 ) dt \u0026amp;\\le \\int_{R^c \\cap R_{NP}} L(t | \\theta_1) dt + \\int_{R_{NP}^c \\cap R^c} L(t | \\theta_1) dt \\nocr \u0026amp;= \\int_{R^c} L(t | \\theta_1) dt \\end{align}\\] So, NP test has smallest type II error (namely the largest power).\n  An concrete example in biostatistics The post provides an concrete example of the deviation of score test. Note that the weight was taken as the variance which is a deviation of Fisher information.\n "
},
{
	"uri": "/notebook/posts/t2b-genetic-architecture-2016/",
	"title": "The genetic architecture of type 2 diabetes",
	"tags": ["genetics", "type 2 diabetes", "complex trait", "genetic architecture"],
	"description": "",
	"content": " Meta data of reading In brief   Meta data of reading  Journal: Nature Year: 2016 DOI: 10.1038/nature18642   In brief The idea of this paper is:\n GoT2D: To perform genetic association analysis by considering not only common variants (SNPs) but also rare variants. To get rare variant data, they performed whole genome sequencing on cohort. Furthermore, they included imputed genotypes as well. They estimated the power of the analysis for both common variants and rare variants (SNPs: OR \\(\\ge\\) 1.87, MAF \\(\\ge\\) 5%; rare: OR \\(\\ge\\) 4.7, MAF \\(\\ge\\) 0.5%) T2D-GENES: To perform genetic association analysis by considering coding variants. The data was obtained from whole exome sequencing in five descents. The original sequencing data gave only one signal in one population. The further chip data was designed on the basis of the WES and the integrated data gave more signals. It helped to pinpoint causal variants in previous established T2D GWAS loci.  This paper did comprehensive analysis including:\nAssociation analysis of genome-wide variation Association analysis of coding variation Gene-based analysis Enrichment of rare variation in Mendelian T2D genes Excluding synthetic association (association of common variant is caused by linkage to rare variant by chance) by conditional analysis Nominating functional variants in GWAS loci Modeling disease architecture (contribution of common/rare variants) by simulation (as an example of late-onset disease)   "
},
{
	"uri": "/notebook/posts/simulate_multivariate_normal/",
	"title": "Simulating Multivariate Normal by Univariate Normal",
	"tags": ["simulation", "multivariate normal", "contour"],
	"description": "",
	"content": " Univariate to Multivariate Example Proof   library(knitr) opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE) Univariate to Multivariate \\[\\begin{align} \\vec{y} = \\sum_i \\vec{v}_i \\mathcal{N}_i \\nonumber \\end{align}\\] , where \\(\\mathcal{N}_1, ..., \\mathcal{N}_p\\) are independent standard univariate normal. Then\n\\[\\begin{align} \\vec{y} \\sim \\mathcal{N}(\\vec{0}, VV^T) \\label{eq:mvn} \\end{align}\\] , where \\(V = [\\vec{v}_1, ..., \\vec{v}_p]\\).\n Example In the following example, \\(v_i \\in \\mathbb{R}^2\\) and \\(p = 2\\). The number of simulated samples is \\(B = 10000\\).\nPloting contour in R # draw contour of a multivariate normal distribution library(mvtnorm) drawContour \u0026lt;- function(xgrid, ygrid, mu, sigma) { z \u0026lt;- c() x \u0026lt;- c() y \u0026lt;- c() for (i in 1:length(xgrid)) { for (j in 1:length(ygrid)) { z \u0026lt;- c(z, dmvnorm(c(xgrid[i], ygrid[j]), mean = mu, sigma = sigma)) x \u0026lt;- c(x, xgrid[i]) y \u0026lt;- c(y, ygrid[j]) } } return(data.frame(x = x, y = y, z = z)) }  Simulating and ploting library(MASS) library(ggplot2) # simulation v1 \u0026lt;- c(1, 2) v2 \u0026lt;- c(-1, -1) V \u0026lt;- matrix(c(v1, v2), ncol = 2, byrow = F) B \u0026lt;- 1000 # number of simulated samples p \u0026lt;- 2 # number of vectors n1 \u0026lt;- rnorm(B) n2 \u0026lt;- rnorm(B) n \u0026lt;- matrix(c(n1, n2), ncol = 2, byrow = F) y \u0026lt;- t(V %*% t(n)) df.samples \u0026lt;- data.frame(x = y[, 1], y = y[, 2]) # ploting - contour xgrid \u0026lt;- seq(from = min(y[, 1]) - 0.1, max(y[, 1]) + 0.1, length.out = 100) ygrid \u0026lt;- seq(from = min(y[, 2]) - 0.1, max(y[, 2]) + 0.1, length.out = 100) mu \u0026lt;- rep(0, p) sigma \u0026lt;- V %*% t(V) df \u0026lt;- drawContour(xgrid, ygrid, mu, sigma) ggplot() + geom_contour(data = df, aes(x = x, y = y, z = z, alpha = ..level..)) + geom_point(data = df.samples, aes(x = x, y = y), alpha = 0.1) # ploting - heatmap y.mvn \u0026lt;- mvrnorm(B, rep(0, p), V %*% t(V)) y.combine \u0026lt;- rbind(y, y.mvn) label \u0026lt;- c(rep(\u0026quot;uni -\u0026gt; multi\u0026quot;, nrow(y)), rep(\u0026quot;multivariate normal\u0026quot;, nrow(y.mvn))) df \u0026lt;- data.frame(x = y.combine[, 1], y = y.combine[, 2], label = label) ggplot(df) + geom_bin2d(aes(x = x, y = y)) + facet_grid(. ~ label)   Proof This is one of the definition of multivariate normal distribution according to wikipedia. Here, suppose \\(\\vec{y}\\) is multivariate normal and let’s check the mean and variance is the same as the one in \\(\\eqref{eq:mvn}\\).\n\\[ \\newcommand\\E{\\text{E}} \\newcommand\\var{\\text{Var}} \\newcommand\\cov{\\text{Cov}} \\newcommand\\nocr{\\nonumber\\\\} \\def\\ci{\\perp\\!\\!\\!\\!\\perp} \\]\n\\[\\begin{align} \\E(\\vec{y}) \u0026amp;= \\E(\\sum_i v_i Z_i) \\text{, where $Z_i \\sim \\mathcal{N}(0, 1)$} \\nocr \u0026amp;= \\sum_i \\E(v_i)\\E(Z_i) \\text{, since $v_i \\ci Z_i$} \\nocr \u0026amp;= 0 \\nocr \\cov(y_k, y_j) \u0026amp;= \\cov(\\sum_i v_{ik} Z_i, \\sum_{i\u0026#39;} v_{i\u0026#39;j} Z_{i\u0026#39;}) \\nocr \u0026amp;= \\sum_{i}\\sum_{i\u0026#39;}\\cov(v_{ik}Z_i, v_{i\u0026#39;j}Z_{i\u0026#39;}) \\nocr \u0026amp;= \\sum_i \\cov(v_{ik}Z_i, v_{ij}Z_{i}) \\text{, since $v_i \\ci v_{i\u0026#39;}$ and $Z_i \\ci Z_{i\u0026#39;}$} \\nocr \u0026amp;= \\sum_i v_{ik}v_{ij} \\nocr \\therefore \\cov(\\vec{y}) \u0026amp;= V V^T \\nonumber \\end{align}\\]  "
},
{
	"uri": "/notebook/posts/huang-2014-aas/",
	"title": "Joint Analysis of SNP and Gene Expression Data in Genetic Association Studies of Complex Diseases",
	"tags": ["gwas", "eqtl", "complex trait", "integrative analysis"],
	"description": "",
	"content": " Meta data of reading Motivation The model Testing \\(H_0\\) Assumption and implication of the test Comparing to SNP only model   Meta data of reading  Journal: The Annals of Applied Statistics Year: 2014 DOI: 10.1214/13-AOAS690  \\[ \\newcommand\\logit{\\text{logit}} \\newcommand\\E{\\text{E}} \\newcommand\\var{\\text{Var}} \\newcommand\\cov{\\text{Cov}} \\newcommand\\diag{\\text{diag}} \\newcommand\\nocr{\\nonumber\\\\} \\def\\ci{\\perp\\!\\!\\!\\!\\perp} \\]\n Motivation The goal is to assess the genetic effect of the specific gene on the disease. In this paper, the authors casted the genetic effect as two parts:\nEffect through gene expression of the given gene Other genetic effect (i.e. splicing, but some gene-unrelated mechanisms are also possible as long as it is determined genetically, say enhancer activity)  The paper used genotype data along with paired gene expression data. The variables in the paper were:\nA set of SNPs within the gene (\\(S\\)) Expression level of the given gene (\\(G\\)) Disease status (\\(Y\\))  The causal model is (Figure 1 of the paper):\n The model \\[\\begin{align} \\logit\\{\\Pr(Y_i = 1| S_i, G_i, X_i)\\} \u0026amp;= X_i^T \\alpha + S_i^T \\beta_S + G_i \\beta_G + G_i S_i^T \\gamma \\label{eq:y} \\end{align}\\] The interaction term modeled the combined effect of genotype and gene expression level to phenotype log odds. This term was added because it made biological sense. Since genotype can affect gene expression, such effect was modeled as following:\n\\[\\begin{align} G_i\u0026amp;= X_i^T\\phi + S_i^T\\delta + \\epsilon_i \\label{eq:g} \\end{align}\\] , where \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_G^2)\\).\nThe goal was to test if the total effect captured by genotype and gene expression on \\(Y\\) is non-zero. Namely,\n\\[\\begin{align} H_0: \\beta_S 0, \\beta_G = 0, \\gamma = 0 \\label{eq:h0} \\end{align}\\] This test was referred as the test for total effect of a gene.\n Testing \\(H_0\\) I am not familiar with these hypothesis testing techniques so I can only sketch the general idea and leave the details untouched.\nThe paper discussed the possiblity of using LRT or Wald test to test \\(\\eqref{eq:h0}\\) and they argued that degree of freedom in this case was big so that the power would be limited. Alternatively, the paper proposed to test variance components. Assuming:\n\\[\\begin{align} \\beta_{S_i} \\sim_{iid} \\mathcal{N}(0, \\tau_S) \\nocr \\gamma_i \\sim_{iid} \\mathcal{N}(0, \\tau_I) \\nonumber \\end{align}\\] Namely the \\(\\eqref{eq:y}\\) becomes a logistic midxed model. Then \\(\\eqref{eq:h0}\\) becomes:\n\\[\\begin{align} H_0: \\tau_S = \\tau_I = 0, \\beta_G = 0 \\nonumber \\end{align}\\] The scores for \\(\\tau_S, \\tau_I\\) and \\(\\beta_G\\) are:\n\\[\\begin{align} U_{\\tau_S} \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T \\mathbb{S}\\mathbb{S}^T \\{Y - \\hat{\\mu}_0\\} \\nocr U_{\\tau_I} \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T \\mathbb{C}\\mathbb{C}^T \\{Y - \\hat{\\mu}_0\\} \\nocr U_{\\beta_G} \u0026amp;= G^T \\{Y - \\hat{\\mu}_0\\} \\nonumber \\end{align}\\] , where\n \\(\\mathbb{S} = (S_1, ..., S_n)^T\\) \\(G = (G_1, ..., G_n)^T\\) \\(\\mathbb{C} = (C_1, ..., C_n)^T = (G_1S_1, ..., G_nS_n)^T\\) \\(\\hat{\\mu}_{0i} = \\exp(X_i^T \\hat{\\alpha}_0) / \\{1 + \\exp(X_i^T \\hat{\\alpha}_0)\\}\\), and \\(\\hat\\alpha_0\\) is the MLE of null model: \\[\\begin{align} \\logit(\\Pr(Y = 1|S_i, G_i, X_i)) = X_i^T \\alpha \\nonumber \\end{align}\\]  To combine the three scores to test \\(\\eqref{eq:h0}\\), the authors proposed the following weighted sum as the test statistic:\n\\[\\begin{align} Q \u0026amp;= n^{-1} (a_1U_{\\tau_S} + a_2U_{\\beta_G} + a_3U_{\\tau_I}) \\nocr \u0026amp;= \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T + a_2GG^T + a_3 \\mathbb{C}\\mathbb{C}^T) \\{Y - \\hat{\\mu}_0\\} \\end{align}\\] , where they proposed to use the inverse of the squared root of the corresponding variance as weights of \\(U_{\\tau_S}, U_{\\beta_G^2}, U_{\\tau_I}\\). The varaince can be computed in closed-form with \\(\\hat{\\mu}_{0i}\\). Let \\(Q = Q(\\hat{\\alpha})\\) and:\n\\[\\begin{align} D \u0026amp;= \\begin{bmatrix} D_{XX} \u0026amp; D_{XV} \\\\ D_{VX} \u0026amp; D_{VV} \\end{bmatrix} \\nocr \u0026amp;= n^{-1} U^T W U \\label{eq:d} \\\\ \\text{, where} \u0026amp; \\nocr U \u0026amp;= \\begin{bmatrix} U_1 \\\\ \\vdots \\\\ U_n \\end{bmatrix} \\nocr U_i \u0026amp;= (X_i^T, V_i^T) \\nocr V_i \u0026amp;= (\\sqrt{a_1}S_i^T, \\sqrt{a_2} G_i, \\sqrt{a_3} C_i) \\in \\mathbb{R}^{2p + 1} \\nocr W \u0026amp;= \\diag(\\mu_i(1 - \\mu_i)) \\nonumber \\end{align}\\] Under null hypothesis \\(\\eqref{eq:h0}\\):\n\\[\\begin{align} Q \\xrightarrow{d} Q(0) \u0026amp;= \\sum_{j}^{2p+1} (A_l^T \\epsilon)^2 \\label{eq:null} \\end{align}\\] , where:\n \\(\\epsilon \\sim \\mathcal{N}(0, D)\\) \\(A_l\\) is \\(l\\)th row of \\(A = [-D_{XV}^T D_{XX}^{-1}, I_{2p+1}]\\)  It means that under null hypothesis, \\(Q\\) follows a mixture of \\(\\chi^2\\) distribution, which can be approximated by scaled \\(chi^2\\) as \\(Q \\sim \\kappa \\chi_{\\nu}^2\\), where \\(\\kappa = \\var(Q) / [2\\E(Q)]\\) and \\(\\nu = 2[E(Q)]^2 / \\var(Q)\\) (the expression was given in supplementary).\nFurthermore, some other hypothesis would be:\n \\(Y\\) depends on \\(G\\), \\(S\\) but not their interaction term \\(Y\\) depends on \\(S\\) only  Denote the previously derived \\(Q\\) as \\(Q_{SGC}\\), the test statistic for the above to cases are:\n \\(Q_{GS} = \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T + a_2GG^T) \\{Y - \\hat{\\mu}_0\\}\\) \\(Q_{S} = \\{Y - \\hat{\\mu}_0\\}^T （a_1\\mathbb{S}\\mathbb{S}^T) \\{Y - \\hat{\\mu}_0\\}\\)  If the model was wrongly specified, then the power of test would be hurted. So, it was necessary to consider the three situations simultanously in the test. The paper proposed a omnibus test to consider these three situations together. Namely the following procedure:\nCompute p-values for the three situations Obtain observed minimum p-value Compare the observed value to its null distribution  , where null distribution of the minimum p-value is not available analytically because of the complicated correlation among the three statistics. Therefore, they used re-sampling perturbation instead. The idea is the following. The goal is to generate the distribution of \\(Q\\) under null hypothesis. From \\(\\eqref{eq:null}\\) we know that the randomness of \\(Q\\) under null comes from \\(\\epsilon\\) so what we need is to obtain \\(\\hat{\\epsilon} \\sim \\mathcal{N}(0, D)\\). Under the general re-sampling procedure (wild bootstrap), the paper proposed to use:\n\\[\\begin{align} \\hat\\epsilon \u0026amp;= n^{-1/2} \\sum_{i = 1}^n U_i^T (Y_i - \\hat{\\mu}_i) \\mathcal{N}_i \\end{align}\\] , where \\(\\mathcal{N}_i \\sim \\mathcal{N}(0, 1)\\) is independent to each other. We can check that \\(\\hat\\epsilon\\) has mean \\(0\\) and variance \\(D\\) (see this post on simulating multivariate normal).\n\\[\\begin{align} \\cov(\\hat\\epsilon) \u0026amp;= 1 / n \\times U^T \\diag(Y_i - \\hat{\\mu}_i) \\diag(Y_i - \\hat{\\mu}_i) U \\nocr \u0026amp;= 1 / n \\times U^T \\diag((Y_i - \\hat\\mu_i)^2) U \\nonumber \\end{align}\\] Comparing to \\(\\eqref{eq:d}\\), \\(\\mu_i(1 - \\mu_i)\\) was approximated by \\((Y_i - \\hat\\mu_i)^2\\). Therefore, \\(\\hat\\epsilon\\) was asymptotically the same as \\(\\epsilon\\). So, \\(\\hat{Q}(0)^{(b)}\\) was the sample under null hypothesis. The procedure to obtain omnibus p-value under the null is the following:\nGenerate \\(\\hat{\\epsilon}\\) (Note that \\(\\hat\\epsilon\\) was shared for the three cases to preserve correlation) Obtain \\(\\hat{Q}_{SGC}(0)^{(b)}\\), \\(\\hat{Q}_{SG}(0)^{(b)}\\), and \\(\\hat{Q}_{S}(0)^{(b)}\\) by changing the definition of \\(V_i\\) in \\(A_l\\) Compute \\(\\hat{p}_{SGC}^{(b)}\\), \\(\\hat{p}_{SG}^{(b)}\\), and \\(\\hat{p}_{S}^{(b)}\\) Compute \\(\\hat{p}_{\\min}^{(b)} = \\min\\{\\hat{p}_{SGC}^{(b)}, \\hat{p}_{SG}^{(b)}, \\hat{p}_{S}^{(b)}\\}\\)  The paper pointed out that this perturbation method was computationally more efficient than permutation method, since it did not need to re-calculate \\(Q\\).\n Assumption and implication of the test The authors discussed the interpretation of the null model \\(\\eqref{eq:h0}\\). First of all, they defined the direct effect, indirect effect, total effect (DE, IE, TE) of the SNPs as follow (see supplementary note):\n\\[\\begin{align} \\text{DE} \u0026amp;= \\log[\\text{OR}_{s_1, s_0|x}^{\\text{DE}}(S_0)] \\nocr \u0026amp;= \\logit\\{\\Pr(Y_i(s_1, G_i(s_0)) = 1 | X_i = x)\\} - \\logit\\{\\Pr(Y(s_0, G_i(s_0)) = 1| X_i = x)\\} \\nocr \\text{IE} \u0026amp;= \\log[\\text{OR}_{s_1, s_0|x}^{\\text{IE}}(s_1)] \\nocr \u0026amp;= \\logit\\{\\Pr(Y_i(s_1, G_i(s_1)) = 1 | X_i = x)\\} - \\logit\\{\\Pr(Y_i(s_1, G_i(s_0)) = 1 | X_i = x)\\} \\nocr \\text{TE} \u0026amp;:= \\text{DE} + \\text{IE} \\nocr \u0026amp;= \\log[\\text{OR}_{s_1, s_0|x}^{\\text{IE}}(s_1)] \\nocr \u0026amp;= \\logit\\{\\Pr(Y_i(s_1, G_i(s_1)) = 1 | X_i = x)\\} - \\logit\\{\\Pr(Y_i(s_0, G_i(s_0)) = 1 | X_i = x)\\} \\nonumber \\end{align}\\] \\(Y(s, g), G(s)\\) is the potential outcome if \\(S = s, G = g\\) and it may or may not be observed (but under \\(\\eqref{eq:y} \\eqref{eq:g}\\), the probability is computable). When \\(S_i = s, G_i = g\\), \\(Y_i(s, g) = Y_i\\), namely the observed value (consistency). To identify DE and IE, the authors listed four assumptions:\n\\(Y(s, g) \\ci S | X\\) \\(Y(s, g) \\ci G | X, S\\) \\(G(s) \\ci S | X\\) \\(Y(s, g) \\ci G(s^*) | X\\)  To derive DE, IE, TE, the term \\(\\logit\\{\\Pr(Y(s_a, G_i(s_b)) = 1 | X_i = x)\\}\\) was needed to be computed.\n\\[\\begin{align} \u0026amp; \\logit\\{\\Pr(Y_i(s_a, G_i(s_b)) = 1 | X_i = x)\\} \\nocr \u0026amp;\\approx \\log(\\Pr(Y_i(s_a, G_i(s_b)) = 1 | X_i = x)) \\text{, if $Y$ is rare phenotype} \\nocr \u0026amp;= \\log [\\int \\Pr(Y_i(s_a, g) = 1 | X_i = x, G_i(s_b) = g) \\Pr(G_i(s_b) = g | X_i = x) dg] \\nocr \u0026amp;= \\log [\\int \\Pr(Y_i(s_a, g) = 1 | X_i = x, G_i(s_a) = g, S_i = s_a, G_i = g) \\Pr(G_i(s_b) = g | X_i = x, S_i = s_b) dg] \\text{, by assumptions 1, 2, 3} \\nocr \u0026amp;= \\log [\\int \\Pr(Y_i(s_a, g) = 1 | X_i = x, S_i = s_a, G_i = g) \\Pr(G_i(s_b) = g | X_i = x, S_i = s_b) dg] \\text{, by assumption 4} \\nocr \u0026amp;= \\log [\\int \\Pr(Y_i = 1 | X_i = x, S_i = s_a, G_i = g) \\Pr(G_i = g| X_i = x, S_i = s_b) dg] \\text{, by consistency of $Y(s, g)$ and $G(s)$} \\nocr \u0026amp;\\approx \\log [\\int \\exp(x^T\\alpha + s_a^T \\beta_S, g + \\beta_G, s_a^T g \\gamma) \\mathcal{N}(g| \\mu = x^T \\phi + s_b^T \\delta, \\sigma^2 = \\sigma_G^2) dg] \\text{, by models $\\eqref{eq:y} \\eqref{eq:g}$} \\nocr \u0026amp;= x^T\\alpha + s_a^T\\beta_S + (\\beta+G + s_a^T\\gamma)(x^T\\phi + s_b^T\\delta) + \\frac{1}{2}(\\beta_G + s_a^T\\gamma)^2 \\sigma_G^2 \\label{eq:effect} \\end{align}\\] With \\(\\eqref{eq:effect}\\), IE, DE, and TE were derived accordingly.\n\\[\\begin{align} \\text{TE} \u0026amp;= (s_1 - s_0)^T \\{\\beta_S + \\beta_G \\delta + \\gamma(x^T\\phi + s_0^T\\delta + \\beta_G\\sigma_G^2) + \\delta s_1^T \\gamma\\} + \\frac{1}{2} \\sigma_G^2 (s_1 + s_0)^T\\gamma(s_1 - s_0)^T\\gamma \\label{eq:te} \\\\ \\text{DE} \u0026amp;= (s_1 - s_0)^T[\\beta_S + \\gamma(x^T\\phi + s_0^T \\delta + \\beta_G \\sigma_G^2)] + \\frac{1}{2} \\sigma_G^2 (s_1 + s_0)^T \\gamma (s_1 - s_0)^T \\gamma \\label{eq:de} \\\\ \\text{IE} \u0026amp;= (s_1 - s_0)^T \\delta (\\beta_G + s_1^T \\gamma) \\label{eq:ie} \\end{align}\\] The assumption for TE was substentially simpler than IE and DE’s (since it did not evolve counterfactual term). Under the assumption \\(Y(s) \\ci S | X\\):\n\\[\\begin{align} \\Pr(Y_i(s) = 1 | X_i = x) \u0026amp;:= \\Pr(Y_i(s, G(s)) = 1 | X_i = x) \\nocr \u0026amp;= \\Pr(Y_i(s) = 1 | X_i = x, S_i = s) \\nocr \u0026amp;= \\Pr(Y_i = 1 | X_i = x, S_i = s) \\nocr \\logit(\\Pr(Y_i(s, G(s)) | X_i = x)) \u0026amp;\\approx \\log[\\Pr(Y_i = 1 | X_i = x, S_i = s)] \\nocr \u0026amp;= \\log[\\int \\Pr(Y_i = 1 | X_i = x, S_i = s, G_i = g)\\Pr(G_i = g | X_i = x, S_i = s)dg] \\nocr \u0026amp;= x^T\\alpha + s^T\\beta_S + (\\beta+G + s^T\\gamma)(x^T\\phi + s^T\\delta) + \\frac{1}{2}(\\beta_G + s^T\\gamma)^2 \\sigma_G^2 \\label{eq:effect_te} \\end{align}\\] If \\(S\\) are eQTL SNPs (\\(\\delta \\ne 0\\)):\n\\[\\begin{align} \u0026amp; H_0: \\beta_S = 0, \\beta_G = 0, \\gamma = 0 \\nocr \\Leftrightarrow \u0026amp; H_0: \\text{DE} = 0, \\text{IE} = 0 \\nocr \\Leftrightarrow \u0026amp; H_0: \\text{TE} = 0 \\label{eq:h0_te} \\end{align}\\] Note that \\(\\eqref{eq:h0_te}\\) required only one assumption that is no unmeasured confounding for the effect of eQTL SNPs (\\(S\\)) on the outcome (\\(Y\\)) after adjusting the covariates (\\(X\\)).\nIf \\(S\\) are non-eQTL SNPs (\\(\\delta = 0\\)), the null hypothesis was testing the joint effect of \\(S\\) and \\(G\\) on \\(Y\\).\n Comparing to SNP only model In standard genetic association analysis, the SNP only model was used:\n\\[\\begin{align} \\logit\\{\\Pr(Y_i = 1 | S_i, X_i)\\} \u0026amp;= X_i^T \\alpha^* + S_i^T \\beta_S^* \\label{eq:gwas} \\end{align}\\] The paper compared the hypothesis test (\\(H_0: \\beta_S^* = 0\\)) of this SNP only model with \\(\\eqref{eq:y} \\eqref{eq:g}\\).\nCase 1: The true model is \\([Y | S, G, X]\\) and \\([G | S, X]\\) where there is no \\(S \\times G\\) interaction term, namely \\(\\gamma = 0\\).\n\\[\\begin{align} \\eqref{eq:y} \\eqref{eq:g} \u0026amp;\\Rightarrow \\nocr \\logit\\{\\Pr(Y_i = 1 | S_i, X_i, \\epsilon_i)\\} \u0026amp;= X_i^T(\\alpha + \\beta_G \\phi) + S_i^T(\\beta_S + \\beta_G \\delta) + \\beta_G \\epsilon_i \\nocr \u0026amp;\\approx c[X_i^T(\\alpha + \\beta_G \\phi) + S_i^T(\\beta_S + \\beta_G \\delta)] \\text{, where $c = (1 + 0.35 \\times \\sigma_G^2 \\beta_G^2)^{-1/2}$} \\nocr \\therefore \\beta_S^* \u0026amp;\\approx c(\\beta_S + \\beta_G \\delta) \\end{align}\\] So, testing \\(\\beta_S^* = 0\\) was approximately equivalent to testing \\(\\eqref{eq:h0}\\).\nCase 2: \\(\\gamma \\ne 0\\). Similar to the above derivation, we have:\n\\[\\begin{align} \\logit\\{\\Pr(Y_i = 1 | S_i, X_i)\\} \u0026amp;\\approx c_i^*[X_i^T(\\alpha + \\beta_G \\phi) + S_i^T(\\beta_S + \\beta_G \\delta) + X_i^T \\phi S_i^T \\gamma + S_i^T \\delta S_i^T \\gamma] \\nocr \\text{, where} c_i^* \u0026amp;= \\{1 + 0.35\\sigma_G^2(\\beta_G + S_i^T\\gamma)^2\\}^{-1/2} \\end{align}\\] There was no corresponding term for cross term of \\(X\\) and \\(S\\) in \\(\\eqref{eq:gwas}\\), so it was misspecified. The test was still valid since the null hypothesis were shared, but the power was lost.\n Comment: This result implies that the test \\(\\eqref{eq:h0}\\) under the model \\(\\eqref{eq:y} \\eqref{eq:g}\\) (SGC model) increases power when gene expression and genotype affect the OR of phenotype jointly.\n  "
},
{
	"uri": "/notebook/posts/sherlock-2013-ajhg/",
	"title": "Sherlock: Detecting Gene-Disease Associations by Matching Patterns of Expression QTL and GWAS",
	"tags": ["gwas", "eqtl", "integrative analysis", "target gene"],
	"description": "",
	"content": " Meta data of reading  Journal: The American Journal of Human Genetics Year: 2013 DOI: 10.1016/j.ajhg.2013.03.022  "
},
{
	"uri": "/notebook/posts/yang-2012-ng/",
	"title": "Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits",
	"tags": ["gwas", "integrative analysis", "complex trait", "linkage disequilibrium", "conditional analysis"],
	"description": "",
	"content": " Meta data of reading  Journal: Nature Genetics Year: 2012 DOI: 10.1038/ng.2213  $$ \\newcommand\\independent{\\perp\\!\\!\\!\\!\\perp} \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\cr} \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\cor{\\text{Cor}} \\newcommand\\numberthis{\\addtocounter{equation}{1}\\tag{\\theequation}} $$\nMotivation The GWAS procedure is to use single-SNP model to test association and select the SNP with strongest signal to represent the genomic region (~ 2Mb) and the genetic variation is computed based on this SNP only. The paper pointed out the underlying assumptions of this procedure:\n Implicit assumptions, often untested, are that the detected association at the top SNP captures the maximum amount of variation in the region by its LD with an unknown causal variant and that other SNPs in the vicinity show association because they are correlated with the top SNP.\n They pointed out 2 reasons why this assumption may fail:\n Suppose there is only one causal variant, a single tagging SNP may not capture all of its variation It is possible that there are more than one causal variant  So, one-SNP-per-locus procedure may underestimate the underlying causal genetic variation. Some studies have performed conditional analysis to find the secondary SNP inside the locus. The paper proposed a systematic approach to perform conditional analysis by combining GWAS meta-analysis and LD correlation from the same population.\nMulti-SNP model and joint effect The multi-SNP model is:\n$$\\begin{align} \\vec{y} \u0026= X\\vec{b} + \\vec{e} \\nonumber \\end{align}$$ , where $X \\in \\mathbb{R}^{n \\times N}, \\vec{b} \\in \\mathbb{R}^N, \\vec{e} \\in \\mathbb{R}^n$. The least squares solution is:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y \\label{eq:sej} \\cr \\var(\\hat{b}) \u0026= \\sigma^2_{J} (X'X)^{-1} \\label{eq:varj} \\end{align}$$ Note that \\eqref{eq:varj} is an N-by-N (co)variance matrix. It is derived as follows:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y = (X'X)^{-1} X'(Xb + e) \\nocr \u0026= b + (X'X)^{-1} X'e \\nocr \\var(\\hat{b}_j) \u0026= \\var([(X'X)^{-1} X'e]_j) \\text{, where $[\\cdot]$ takes the $i$th row}\\nocr \u0026= \\var([(X'X)^{-1}]_j X'e) \\nocr \u0026= [(X'X)^{-1}_j X'] \\odot [(X'X)^{-1}_j X'] \\var(e) \\label{eq:inter1} \\end{align}$$ Note that in \\eqref{eq:inter1}, we have $[(X'X)^{-1}_j X'] \\in \\mathbb{R}^{1 \\times n}$ and $\\var(e) \\in \\mathbb{R}^{n \\times 1}$ which are vectors. But notice that $e_1, \u0026hellip;, e_n$ are iid. The expression can be simplified as:\n$$\\begin{align} \\eqref{eq:inter1} \u0026= t_j X' X t_j' \\sigma^2_J \\nonumber \\end{align}$$ , where $t_j := [(X'X)^{-1}]_j$. Similarly,\n$$\\begin{align} \\cov(\\hat{b}_i, \\hat{b}_j) \u0026= t_i X' X t_j' \\sigma^2_J \\nonumber \\end{align}$$ So,\n$$\\begin{align} \\var(\\hat{b}) \u0026= \\sigma^2_J \\begin{bmatrix} t_1 \\cr \\vdots \\cr t_N \\end{bmatrix} X'X \\begin{bmatrix} t_1 \u0026 \\cdots \u0026 t_N \\end{bmatrix} \\nocr \u0026= \\sigma^2_J (X'X)^{-1} (X'X) (X'X)^{-T} \\nocr \u0026= \\sigma^2_J (X'X)^{-1} \\text{, with the fact that $X'X$ is symmetric} \\nonumber \\end{align}$$ , which is the result of \\eqref{eq:varj}.\nSingle-SNP model and marginal effect In practice, only single-SNP model results are available. The single-SNP model is as follow:\n$$\\begin{align} y \u0026= x_j \\beta_j + e \\nonumber \\end{align}$$ So, the LS estimator is:\n$$\\begin{align} \\hat\\beta \u0026= D^{-1} X'y \\label{eq:se} \\cr \\var(\\hat\\beta) \u0026= \\sigma^2_M D^{-1} \\label{eq:varm} \\end{align}$$ , where $D = diag(\\vec{d}), d_i = X_i' X_i$ and $X_i$ is the $i$th column of $X$. The derivation of \\eqref{eq:varm} is as follow:\n$$\\begin{align} \\var(\\hat\\beta_i) \u0026= \\var((X_i' X_i)^{-1} X_i' (X_i b_i + e)) \\nocr \u0026= \\var(b_i + (X_i' X_i)^{-1} X_i' e) \\nocr \u0026= \\var((X_i' X_i)^{-1} X_i' e) \\nocr \u0026= \\frac{X_i' \\odot X_i'}{(X_i' X_i)^2} \\var(e) \\nocr \u0026= \\frac{X_i' X_i}{(X_i' X_i)^2} \\sigma^2_M \\nocr \u0026= \\sigma^2_M (X_i' X_i)^{-1} \\nocr \\end{align}$$ Here, we treat single-SNP model as the truth. Note that we treat SNPs independently with each other (namely $\\cov(\\hat\\beta_i, \\hat\\beta_j) = 0$). Different SNPs do not have to share the same residual variance, so a more precise expression is $\\var(\\hat\\beta_i) = \\sigma^2_{M(i)} (X_i' X_i)^{-1}$\nInferring joint effect from single effect From \\eqref{eq:se}, we have $X'y = D \\hat\\beta$. Under multiple SNP model, the proportion of variance explained by all SNPs is:\n$$\\begin{align} R_J^2 \u0026= \\frac{\\cov(\\hat{y}, y)}{\\var(y)} \\nocr \u0026= \\frac{\\hat{b}' X' y}{y'y} \\nocr \u0026= \\frac{\\hat{b}' D \\hat\\beta}{y'y} \\nonumber \\end{align}$$ Then, we can derive:\n$$\\begin{align} \\hat\\sigma^2_J \u0026= \\frac{(1 - R_J^2) y'y}{n - N} \\nocr \u0026= \\frac{y'y - \\hat{b}' D \\hat\\beta}{n - N} \\end{align}$$ Similarly,\n$$\\begin{align} R_{M(j)}^2 \u0026= \\frac{\\hat{y}_j' y}{y'y} \\nocr \u0026= \\frac{X_j' \\hat\\beta_j y}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j X_j'y}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j D_j \\hat\\beta_j}{y'y} \\nocr \u0026= \\frac{\\hat\\beta_j^2 D_j}{y'y} \\nocr \\hat\\sigma^2_{M(j)} \u0026= \\frac{(1 - R_{M(j)}) y'y}{n - 1} \\nocr \u0026= \\frac{y'y - \\hat\\beta_j^2 D_j}{n - 1} \\nonumber \\end{align}$$ From \\eqref{eq:varm}, we have $\\var(\\hat\\beta_j) = \\hat\\sigma^2_{M(j)} / D_j$, so we get:\n$$\\begin{align} y'y = D_j \\var(\\hat\\beta_j) (n - 1) + D_j \\hat\\beta_j^2 \\label{eq:yy} \\end{align}$$ This expression provides a way to obtain $y\u0026rsquo;y$ with $\\hat\\beta_j$ and $\\hat{\\var}(\\hat\\beta_j)$ (w/o knowing individual level data $y$). In practice, the paper used the median of inferred $y\u0026rsquo;y$ obtained from $j = 1, \u0026hellip;, N$.\n Side note   The reason why the above analysis is performed is to obtain joint model statistic from the single-SNP model without querying individual level data $y$. One useful relation is: $\\hat\\sigma^2 = \\frac{(1 - R^2) y\u0026rsquo;y}{N - n}$. $R^2$ is computable since it is just the observed proportion of covariance between predictor and response in the overall variance of response. For the single-SNP case, $\\hat\\sigma^2$ is available via \\eqref{eq:varm}.\n In meta-analysis, $X$ is not available as well. But since $X\u0026rsquo;X$ is the (co)variance matrix of SNP genotypes, it can be approximated by the LD score from a matched reference population. The paper used $W$ to denote such population, where $w_{ij} = -2f_j, 1 - 2 f_j, 2 - 2 f_j$ to denote genotypes: two major alleles, heterozygous, two minor alleles respectively. Under this set up, $\\E(w_j) = 0, \\var(w_j) = 2f_j(1 - 2 f_j)$. Therefore, we have:\n$$\\begin{align} \\frac{\\sum_i x_{ij} x_{ik}}{\\sqrt{\\sum_i x_{ij}^2 \\sum_i x_{ik}^2}} \u0026\\approx \\frac{\\sum_i w_{ij} w_{ik}}{\\sqrt{\\sum_i w_{ij}^2 \\sum_i w_{ik}^2}} \\nocr \\Rightarrow (X'X)_{jk} \u0026= \\sum_i x_{ij} x_{ik} \\nocr \u0026\\approx \\sum_i w_{ij} w_{ik} \\sqrt{\\frac{D_j D_k}{D_{W(j)}D_{W(k)}}} := B_{jk} \\nocr \\Rightarrow B \u0026:= D^{1/2}D_W^{-1/2}W'W D_W^{-1/2} D^{1/2} \\approx X'X \\label{eq:app} \\end{align}$$ where $D, D_W$ is the diagonal matrix with diagonal entries of $X\u0026rsquo;X, W\u0026rsquo;W$. $D_{j}$ is not available without $X$, but it can be approximated by $2p_j(1 - p_j)n$. From \\eqref{eq:sej}, \\eqref{eq:se}, \\eqref{eq:app}, we have:\n$$\\begin{align} \\hat{b} \u0026= (X'X)^{-1} X'y = (X'X)^{-1} D \\hat\\beta \\nocr \u0026\\approx B^{-1} D \\hat\\beta := \\tilde{b} \\nonumber \\end{align}$$ Similar to \\eqref{eq:varj},\n$$\\begin{align} \\var(\\tilde{b}) \u0026= \\sigma^2_J B^{-1} \\end{align}$$ In the paper, distant SNP pairs were assigned zero correlation instead the observed one in $W$. The paper pointed out an additional complexity in practice, that is SNPs may have different effective sample sizes due to imputation failures. Therefore, the paper suggested to estimate the effective sample for each SNP and use the adjusted sample size to compute $B_{jk}$. The procedure is:\n From \\eqref{eq:yy}, we obtain $y\u0026rsquo;y$ by taking the median Obtain $\\hat{n}_j$ using \\eqref{eq:yy}: $$\\begin{align} \\hat{n}_j \u0026amp;= y\u0026rsquo;y / D_j \\var(\\hat\\beta_j) - \\hat\\beta_j^2 / \\var(\\hat\\beta_j) + 1 \\nonumber \\end{align}$$ Compute $B_{jk}$ and $D_j$ using the adjusted sample size: $$\\begin{align} B_{jk} \u0026amp;= 2 \\min(\\hat{n}_j, \\hat{n}_k) \\sqrt{\\frac{p_j(1 - p_j)p_k(1 - p_k)}{D_{W(j)}D_{W(k)}}} (W\u0026rsquo;W)_{jk} \\nocr W_j \u0026amp;= 2p_j(1 - p_j) \\hat{n}_j \\nonumber \\end{align}$$  Obtain p-value of marginal effect under multi-SNP model In brief, the above derivation provides a way to infer joint effect distribution from single-SNP model summary statistic. Namely, we get:\n$$\\begin{align} (\\tilde{b} - b) \\sim \\mathcal{N}(0, \\var(\\tilde{b}) \\nonumber \\end{align}$$ The marginal distribution for each SNP\u0026rsquo;s effect size, $\\tilde{b}_i$, is:\n$$\\begin{align} (\\tilde{b}_i - b_i) \\sim \\mathcal(N, \\var(\\tilde{b}_i)) \\nonumber \\end{align}$$ Therefore, we can construct a test as follow:\n $H_0$: $b_i$ is zero $H_1$: $b_i$ is not zero  Under the null, $\\tilde{b}_i \\sim \\mathcal{N}(0, \\var(\\tilde{b}_i))$. Then $\\mathbb{P}_{H_0}(|b_i| \u0026gt; |\\tilde{b}_i| ) = 2(1 - \\Phi(|\\tilde{b}_i|))$, which is the marginal effect of $i$th SNP under multi-SNP model.\nConditional analysis The logic of this part is not very intuitive to me, but after some struggling, I end up with the following things.\nFirst of all, the conditional analysis takes a two step procedure to estimate $\\hat{b}_2 | \\hat{b}_1$. That is:\n Do $y \\sim X_1$ and obtain $\\bar{b}_1 = (X_1'X_1)^{-1} X_1'y$ Compute $\\tilde{y} = y - X_1 \\bar{b}_1$ Do $\\tilde{y} \\sim X_2$ and obtain $\\hat{b}_2 | \\hat{b}_1 = (X_2\u0026rsquo; X_2)^{-1} X_2\u0026rsquo; \\tilde{y}$, which matches the equation 15 in the text  The variance of $\\hat{b}_2 | \\hat{b}_1$ stuck me for a while because it is unclear how to define $\\hat\\sigma^2_C$. It is still not so clear to me but what I get is the following:\n$$\\begin{align} y \u0026= X_1 b_1 + X_2 b_2 + e \\label{eq:y}\\cr \\hat{b}_2 | \\hat{b}_1 \u0026= M_2^{-1} X_2'y - M_2^{-1} M_{21} M_1^{-1} X_1'y \\nocr \u0026\\text{, where $M_{ij} = X_i' X_j$ and $M_{ii} = M_i$} \\nocr \u0026= M_2^{-1} M_{21} b_1 + M_2^{-1}M_2b_2 + M_2^{-1}X_2'e \\nocr \u0026- M_2^{-1}M_{21}M_1^{-1}M_1 b_1 - M_2^{-1}M_{21}M_1^{-1}M_{12}b_2 - M_2^{-1}M_{21}M_1^{-1}X_1'e \\nocr \u0026= (M_2^{-1}M_2 - M_2^{-1}M_{21}M_1^{-1}M_{12})b_2 \\nocr \u0026+ (M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1') e \\nocr \\var(\\hat{b}_2 | \\hat{b}_1) \u0026= \\var((M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1') e) \\nocr \u0026= (M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1')(M_2^{-1}X_2' - M_2^{-1}M_{21}M_1^{-1}X_1')' \\sigma^2_J \\nocr \u0026= M_2^{-1}X_2' (I - X_1 M_1^{-1} X_1')(I - X_1 M_1^{-1} X_1')' X_2M_2^{-1} \\sigma^2_J\\nocr \u0026= M_2^{-1}X_2' (I - 2X_1M_1^{-1}X_1' + X_1M_1^{-1}X_1'X_1M_1^{-1}X_1') X_2M_2^{-1} \\sigma^2_J\\nocr \u0026= M_2^{-1}X_2' (I - X_1M_1^{-1}X_1') X_2M_2^{-1}\\sigma^2_J \\nocr \u0026= [M_2^{-1} - M_2^{-1}M_{21}M_1^{-1}M_{12}M_2^{-1}]\\sigma^2_J \\nocr \\end{align}$$ So, it seems to me that $\\sigma^2_C$ and $\\sigma^2_J$ are interchangeable if $X = [X_1, X_2]$. If such condition is not satisfied, \\eqref{eq:y} is not the multi-SNP model, so it is better to denote the residual variance as $\\sigma^2_C$. The paper computed $\\hat\\sigma^2_C$ using the following equation:\n$$\\begin{align} \\hat\\sigma^2_C \u0026= \\frac{y'y - \\hat{b}_1' D_1 \\hat{\\beta}_1 - (\\hat{b}_2|\\hat{b}_1)' D_2\\hat\\beta_2}{n - N_1 - N_2} \\end{align}$$ Similar to previous derivation, the individual level statistics can be replaced by $D, \\beta, B$. Note that $M_{12}, M_{21}, M_1, M_2$ can be approximated by $B$.\nResults in brief The paper performed the analysis using GIANT GWAS and two reference genotype data. If two SNPs were in low LD, the result was similar to singe-SNP model. For positively correlated SNPs (modest LD), single-SNP model tended to overestimate the effect size. While multi-SNP model gave smaller effect size, they still reached genome-wide significance. For negatively correlated SNPs, single-SNP model tended to miss one of the signal because the signal was masked by the other one. Multi-SNP model was more powerful in this case. They found 36 loci with multiple signals with 38 leading SNPs and 49 additional SNPs. The result is robust to the choice of reference sample. Conditional analysis was also performed to identify secondary associations in the loci. The analysis was also applied to case-control study where $y$ is OR instead of quantitative trait.\n"
},
{
	"uri": "/notebook/posts/mendelian-randomization/",
	"title": "Mendelian randomization and instrumental variable regression",
	"tags": ["mendelian randomization", "causality"],
	"description": "",
	"content": " $$ \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\iv{\\text{IV}} $$\nMeta data of reading  Links:  https://www.bauer.uh.edu/rsusmel/phd/ec1-8.pdf http://fmwww.bc.edu/EC-C/F2012/228/EC228.f2012.nn15.pdf http://mathworld.wolfram.com/Lindeberg-FellerCentralLimitTheorem.html http://mathworld.wolfram.com/LindebergCondition.html  Year: NA DOI: NA  The problem and the idea of MR Suppose we have phenotype $Y$, gene expression $X$, and genotype $Z$. The goal is to see if $X$ and $Y$ has some causal relationship. Since there are some unknown confounders, the residual of $Y \\sim X$ is correlated with $X$. Therefore, the OLS estimator of effect size $\\hat{\\beta}_{xy}$ is biased.\nTo account for such drawback, $Z$ is introduced as instrumental variable (IV) since genotype is pre-determined so that there should not be confounders that can affect genotype. Therefore, the residual of $Y \\sim Z$ should not be correlated with $Z$. So, OLS estimator is unbiased. The estimator constructed by $\\hat{\\beta}_{zy}$ and $\\hat{\\beta}_{zx}$ is simply:\n$$\\begin{aligned} \\hat{\\beta}_{xy} \u0026amp;= \\frac{\\hat{\\beta}_{zy}}{\\hat{\\beta}_{zx}} \\end{aligned}$$\nConsistency and the derivation of the variance The following is derived from https://www.bauer.uh.edu/rsusmel/phd/ec1-8.pdf.\nFormally, IV estimator is defined as $b_{\\iv} = (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;y$. Then, we have:\n$$\\begin{aligned} b_{\\iv} \u0026amp;= (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;y \\cr \u0026amp;= (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;(X \\beta_{xy} + \\epsilon) \\cr \u0026amp;= \\beta_{xy} + (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;\\epsilon \\cr \u0026amp;\\xrightarrow{p} \\beta_{xy} \\end{aligned}$$\nsince $Z$ is independent to error.\nThe variance of $b_{\\iv}$ is inspired by Lindeberg-Feller CLT:\n$$\\begin{aligned} \\sqrt{N} (b_{\\iv} - \\beta_{xy}) \u0026amp;= \\sqrt{N} (Z\u0026rsquo;X)^{-1} Z\u0026rsquo;\\epsilon \\cr \u0026amp;= (Z\u0026rsquo;X / N)^{-1} \\sqrt{N} (Z\u0026rsquo;\\epsilon / N) \\cr (Z\u0026rsquo;\\epsilon / N) \\sqrt{N} \u0026amp;\\xrightarrow{d} \\mathcal{N}(0, \\sigma^2 \\var(Z)) \\quad\\text{, by L-F CLT} \\cr \\sqrt{N} (b_{\\iv} - \\beta_{xy}) \u0026amp;\\xrightarrow{d} \\mathcal{N}(0, \\sigma^2 \\cov(Z, X)^{-1} \\var(Z) \\cov(Z, X)^{-1}) \\end{aligned}$$\nThe last line is kind of heuristic to me but I cannot find a justification for it (anyway \u0026hellip;). Note that $\\sigma^2 := \\var(\\epsilon)$ and it turns out that $\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_i (y_i - b_{\\iv} x_i)^2$ is an unbiased estimator of this term. So, $\\hat{\\var}(b_{\\iv}) = \\hat{\\sigma}^2 \\hat{\\cov}(Z, X)^{-1} \\hat{\\var}(Z) \\hat{\\cov}(Z, X)^{-1}$.\nThe general procedure of MR This section describes the MR procedure as stated in this paper (see post).\n$b_{\\iv}$ can be computed by two-step least squares (2SLS), simply $\\hat{b}_{xy} = \\hat{b}_{zy} / \\hat{b}_{zx}$. By the result of the above section, we have:\n$$\\begin{aligned} \\var(\\hat{b}_{xy}) \u0026amp;= \\frac{\\text{unexplained variance by 2SLS}}{N} \\times \\frac{\\var(Z)}{\\cov(Z, X)^{2}} \\cr \u0026amp;= \\frac{\\text{unexplained variance by 2SLS}}{N\\var(X) \\rho^2_{xz}} \\text{, Since } \\rho^2_{xz} := \\frac{\\cov(X, Z)^2}{\\var(Z)\\var(X)} \\cr \u0026amp;= \\frac{\\var(Y) (1 - R_{xy}^2)}{N \\var(X) R_{zx}^2} \\end{aligned}$$\n, which justifies the result in the paper (equation 2).\n"
},
{
	"uri": "/notebook/posts/zhu-2017-ng/",
	"title": "Integration of summary data from GWAS and eQTL studies predicts complex trait gene targets",
	"tags": ["gwas", "integrative analysis", "eqtl", "target gene", "mendelian randomization", "causality", "complex trait", "linkage disequilibrium"],
	"description": "",
	"content": " $$ \\newcommand\\independent{\\perp\\!\\!\\!\\!\\perp} \\newcommand\\E{\\text{E}} \\newcommand\\nocr{\\nonumber\\cr} \\newcommand\\cov{\\text{Cov}} \\newcommand\\var{\\text{Var}} \\newcommand\\cor{\\text{Cor}} $$\nMeta data of reading  Journal: Nature Genetics Year: 2016 DOI: 10.1038/ng.3538  Instrumental variable and Mendelian randomization analysis The paper mentioned it in the background section along with Mendelian randomization (MR). I googled this term and found this site. Also, wiki page is informative.\nIt appears in regression analysis where $y$ is the responsive variable and $x$ is the explanatory variable. The model is simply $Y = X \\beta + \\epsilon$. Suppose $Z$ correlates with $X$ and $Z$ is independent to $Y$ given $X$, then $Z$ correlates with $Y$ as well despite such conditional independence.\nIn some analysis, people sort of take the advantage of $Z$ (in this context $Z$ is called instrumental variable). In regression model, the underlying assumption is that $\\epsilon$ is independent to $X$. But it is hard to achieve if there is some other unknown variable $U$ that affect both $X$ and $Y$. In this case, error term \u0026ldquo;absorbs\u0026rdquo; the dependencies in $U$ which leads to the violence of $X \\independent \\epsilon$ (the following graph is an example).\ngraph LR; Z((Z)) --- X((X)) X --- Y((Y)) U((U)) --- Y X --- U E((error)) --- Y  In this case, $Y \\sim X$ cannot distinguish $U$ and $\\epsilon$ but $Y \\sim Z$ does not have such problem since the effect of $U$ is captured by $X$ and $\\epsilon$ is disentangled.\nThe idea of MR can be illustrated by the following graph:\ngraph LR; Z(variant) --- X(gene expression) X --- Y(phenotype) U(unknown factors) --- Y X --- U E(error) --- Y  Here, the unknown factors can be environmental and regression model cannot tease apart error and them. But since variants are fixed, therefore we can use genetic variant as the instrumental variable to infer the dependency between gene expression and phenotype. It is the idea of MR.\nMotivation MR analysis requires matched genotype and expression profile data and its power is limited by the strength of the dependency between genotype and phenotype along with the one between gene expression and phenotype. So, it is not useful in practice.\nThis paper tends to use summary statistic of GWAS and eQTL instead, which may not as strong as individual level data, but it is easy to achieve a much larger sample size.\nMethod This paper proposed a summary data-based MR method (SMR). In intuitively, it estimates effect of $Z$ (genotype) on $Y$ (phenotype), $b_{zy}$ and effect of $Z$ on $X$, $b_{zx}$ (note that the former is GWAS and the latter is eQTL mapping). Then $b_{xy}$ is simply $\\frac{b_{zy}}{b_{zx}}$. This approach, same as MR, estimates the effect of gene expression on phenotype which is only mediated by genetic component (in other word, free of non-genetic confounders). But the caveat is that the method cannot distinguish causality and pleiotropic effect (mediated by genetic confounders).\nIt appears to me that the derivation of MR (or instrumental variable regression) is not intuitive so I leave the note of MR part for another post here. Note that in MR, the same $Z$ is used for the estimation of $\\hat{b}_{zx}, \\hat{b}_{zy}$. SMR, instead, uses different $Z$, namely $Y \\sim Z_1$ and $X \\sim Z_2$ and $\\hat{b}_{xy} = \\hat{b}_{zy} / \\hat{\\beta}_{zx}$. Since $Z_1 \\independent Z_2$, $\\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) = 0$. By Delta method, we have:\n$$\\begin{aligned} \\var(\\hat{b}_{xy}) \u0026amp;\\approx \\begin{bmatrix} \\frac{1}{\\beta_{zx}} \u0026amp; -\\frac{b_{zy}}{\\beta_{zx}^2} \\end{bmatrix} \\begin{bmatrix} \\var(\\hat{b}_{zy}) \u0026amp; \\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) \\cr \\cov(\\hat{b}_{zy}, \\hat{\\beta}_{zx}) \u0026amp; \\var(\\hat{\\beta}_{zx}) \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\beta_{zx}} \\cr -\\frac{b_{zy}}{\\beta_{zx}^2} \\end{bmatrix} \\cr \u0026amp;= \\frac{b_{zy}^2}{\\beta_{zx}^2} \\bigg[ \\frac{\\var(\\hat\\beta_{zx})}{\\beta_{zx}^2} + \\frac{\\var(\\hat{b}_{zy})}{b_{zy}^2} - 2\\frac{\\cov(\\hat\\beta_{zx}, \\hat{b}_{zy})}{\\beta_{zx}b_{zy}} \\bigg] \\end{aligned}$$\nThen the $\\chi^2$ statistic is:\n$$\\begin{aligned} T_{\\text{SMR}} \u0026amp;= \\hat{b}_{xy} / \\var(\\hat{b}_{xy}) \\cr \u0026amp;\\approx \\bigg(\\frac{\\hat{b}_{zy}}{b_{zy}}\\bigg)^2 \\bigg(\\frac{\\beta_{zx}}{\\hat\\beta_{zx}}\\bigg)^2 \\bigg/ \\bigg[\\frac{(\\frac{\\hat{b}_{zy}}{b_{zy}})^2}{z_{zy}^2} + \\frac{(\\frac{\\hat\\beta_{zx}}{\\beta_{zx}})^2}{z_{zx}^2}\\bigg] \\cr \u0026amp;\\approx \\frac{1}{1 / z_{zy}^2 + 1 / z_{zx}^2} \\cr \u0026amp;= \\frac{z_{zy}^2 z_{zx}^2}{z_{zy}^2 + z_{zx}^2} \\end{aligned}$$\n, which is described in equation 5.\nFurthermore, some data sets only report z-score without effect size. In this case, the inference can still be done but effect size is needed to estimate $b_{xy}$. In fact, the effect size can be recovered if the allele frequency is known (see the result at supplementary notes page 9 bottom and page 10 top equations). This result is surprising to me, so I scratch the derivation below.\nWithout loss of generality, let\u0026rsquo;s consider $\\hat{b}_{zy}$. $\\hat\\beta_{zx}$ follows the same rule.\n First \u0026ldquo;=\u0026rdquo;:  $$\\begin{aligned} \\hat{b}_{zy} \u0026= (Z'Z)^{-1} (Z'y) \\cr \u0026= (Z'Z)^{-1} (Z b_{zy} + \\epsilon) \\cr \u0026= b_{zy} + (Z'Z)^{-1} (Z' \\epsilon) \\cr \u0026= b_{zy} + \\frac{\\sum_j (z_j - \\bar{z}) \\epsilon_j}{\\sum_i (z_i - \\bar{z})^2} \\end{aligned}$$ Here $Z$ is the normalized version of the original $Z$ for simplicity. Since the expression evolves \u0026ldquo;divid\u0026rdquo;, taking \u0026ldquo;variance\u0026rdquo; on both sides seems tedious. Alternatively, we can apply Delta method.\n$$\\begin{align} \\hat{A} \u0026= \\frac{\\sum_j (z_j - \\bar{z}) \\epsilon_j}{n} \\nocr \\hat{B} \u0026= \\frac{\\sum_i (z_i - \\bar{z})^2}{n} \\nocr \\sqrt{n} (\\hat{A} - 0) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\var(z)\\var(\\epsilon)) \\text{, since $z \\independent \\epsilon$} \\nocr \\sqrt{n} (\\hat{B} - \\var(z)) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\tau^2) \\nocr \\sqrt{n} (\\frac{\\hat{A}}{\\hat{B}} - 0) \u0026\\xrightarrow{d} \\mathcal{N}(0, \\nabla g(A, B)' \\Sigma \\nabla g(A, B)') \\nocr \\text{, where } \\nabla g(A, B) \u0026= \\begin{bmatrix} 1/B \\cr 0 \\end{bmatrix} \\nocr \\Sigma \u0026= \\begin{bmatrix} \\var(z) \\var(\\epsilon) \u0026 \\cov(A, B) \\nocr \\cov(A, B) \u0026 \\tau^2 \\end{bmatrix} \\nocr \\text{therefore, } \\nabla g(A, B)' \\Sigma \\nabla g(A, B)' \u0026= \\frac{\\var(z) \\var(\\epsilon)}{B^2} \\nocr \u0026= \\frac{\\var(\\epsilon)}{\\var(z)} \\nocr \\text{thus, } \\var(\\hat{b}_{zy}) \u0026= \\var(\\frac{\\hat{A}}{\\hat{B}}) \\nocr \u0026= \\frac{\\var(\\epsilon)}{n \\var(z)} \\label{eq:varb} \\end{align}$$ So that we obtain the first \u0026ldquo;equal sign\u0026rdquo;: $SE = \\sqrt{\\frac{\\sigma_\\epsilon^2}{n \\var(z)}}$\n Second \u0026ldquo;=\u0026rdquo;:  $$\\begin{aligned} \\E(z) \u0026= 0 \\times (1 - p)^2 + 1 \\times 2p(1 - p) + 2 \\times p^2 = 2p\\cr \\E(z^2) \u0026= 0 \\times (1 - p)^2 + 1 \\times 2p(1 - p) + 4 \\times p^2 = 2p(1 + p)\\cr \\var(z) \u0026= \\E(z^2) - \\E(z)^2 = 2p(1 - p) \\cr \\E(\\epsilon) \u0026= \\E(y - \\tilde{z} b_{zy}) = \\E(y) - b_{zy} \\E(\\tilde{z}) = 0 \\text{, since $\\E(y) = \\E(\\tilde{z}) = 0$}\\cr \\E(\\epsilon^2) \u0026= \\E((y - \\tilde{z} b_{zy})^2) = \\E(y^2) + b_{zy}^2 \\E(\\tilde{z}^2) - 2 b_{zy} \\E(y \\tilde{z}) \\cr \u0026= \\E(y^2) + \\E(\\tilde{z}^2) b_{zy}^2 - 2 b_{zy} [ b_{zy} \\E(\\tilde{z}^2) + \\E(\\tilde{z}) \\E(\\epsilon) ] \\cr \u0026= \\E(y^2) - 2p (1 + p) b_{zy}^2 \\cr \\sigma_{\\epsilon}^2 \u0026= \\E(\\epsilon^2) - \\E(\\epsilon)^2 \\cr \u0026= \\E(y^2) - 2p(1 - p) b_{zy}^2 \\cr \u0026= \\var(y) - 2p(1 - p) b_{zy}^2 \\cr \u0026= 1 - 2p(1 - p) b_{zy}^2 \\text{, since $\\var(y) = 1$} \\end{aligned}$$ , where $\\tilde{z} := z - \\E(z)$. Therefore,\n$$\\begin{aligned} SE \u0026= \\sqrt{\\frac{\\sigma_\\epsilon^2}{n \\var(z)}} \\cr \u0026= \\sqrt{\\frac{1 - 2p(1 - p)b}{2p(1 - p)n}} \\end{aligned}$$ , as the equation stated in supplementary notes page 9 (last one).\nWith z-score known, we have:\n$$\\begin{aligned} z_{zy} \u0026= \\frac{\\hat{b}_{zy}}{SE_{zy}} \\cr \\hat{b}_{zy} \u0026= z_{zy} SE_{zy} \\cr \u0026= z_{zy} \\sqrt{\\frac{1 - 2p(1 - p)b_{zy}}{2p(1 - p)n}} \\cr \\Rightarrow \\hat{b}_{zy}^2 \u0026\\approx z_{zy}^2 \\frac{1 - M \\hat{b}_{zy}^2}{Mn} \\text{, where $M = 2p(1 - p)$} \\cr \\Rightarrow \\hat{b}_{zy} \u0026\\approx z_{zy} / \\sqrt{2p (1 - p) (n + z_{zy}^2)} \\end{aligned}$$ , which is the result on page 10 top of supplementary notes.\nAccounting for linkage In practice, variants are correlated with each other because of LD. Therefore, the loci that is used for computation may show association if it is correlated with two causal variants which affect transcription and phenotype respectively. Figure 1b illustrated the scenarios.\n Three possible explanations of an association   It turns out that it is possible to filter out the \u0026ldquo;linkage\u0026rdquo; case. The paper suggested that if there is only one causal variant, the nearby region should have similar association signal. So, they proposed a hypothesis testing procedure to get rid of signal resulted from linkage. The following shows how they came up with the distribution under null hypothesis.\nLet\u0026rsquo;s say there are $k$ loci in LD to the causal loci which is denoted as loci 0. For the causal loci and the $i$th locus, we have (consider $y \\sim z$):\n$$\\begin{align} y \u0026= b_{yz(0)} z_{(0)} + \\epsilon_{(0)} \\nonumber\\cr y \u0026= b_{yz(i)} z_{(i)} + \\epsilon_{(i)} \\nonumber\\cr \\cov(y, z_{(i)}) \u0026= b_{yz(0)} \\cov(z_{(0)}, z_{(i)}) \\nonumber\\cr \\cov(y, z_{(i)}) \u0026= b_{yz(i)} \\var(z_{(i)}) \\nonumber\\cr \u0026 \\text{, by the fact that $\\epsilon_{(j)} \\independent z_{(i)}, j = 0, i$} \\nonumber\\cr \\Rightarrow b_{yz(i)} \u0026= b_{yz(0)} \\frac{\\cov(z_{(0)}, z_{(i)})}{\\var(z_{(i)})} \\nonumber\\cr \u0026= b_{yz(0)} r_{0i} \\sqrt{\\frac{\\var(z_{(0)})}{\\var(z_{(i)})}} \\nonumber\\cr \u0026= b_{yz(0)} r_{0i} \\sqrt{h_0 / h_i} \\label{eq:1} \\end{align}$$ , where $h = 2p (1 - p)$ which is precisely $\\var(z)$. So equation 7 follows. Intuitively, consider the following situation:\ngraph LR; z(z) --- x(x) x --- y(y)  In this case, $b_{zy} = b_{xy} r_{zx} SE_x / SE_y$. Namely, if we know the dependency between $x$ and $z$, so does $x$ and $y$, then we can derive the dependency between $z$ and $y$. $r_{zx}$ indicates to what extent the effect size of $x$ on $y$ can be transferred to the one of $z$ on $y$. $SE_x / SE_y$ is a rescaling term. To match the same scale (namely $y$), $b \\times SE$ should of the same scale for $z \\sim y$ and $x \\sim y$. Therefore, under the null hypothesis, $\\hat{b}_{zy(i)}$ should have the same expected value. To make inference, we need some distribution of $\\vec{b}_{zy}$, so we should know the variance/covariance as well.\n Side note:   I just found that the above derivation has a subtle drawback. It is that suppose we do $\\cov(\\cdot, z_{(0)})$ instead and do it carelessly, it will fail. The problem comes from the term $\\cov(\\epsilon_{(i)}, z_{(0)})$. From the derivation above, it seems to me that $z_{(0)}$ and $z_{(i)}$ are equivalent and interchangeable but it is really not the case. The graph above has pointed out such difference but in an implicit way. A more explicit illustration is the following:\n graph LR; z(z) --- x(x) x --- y(y) e0(e0) -- y e1(e1) -- x   The corresponding equations are:\n $$\\begin{aligned} y \u0026= b_0 x + \\epsilon_0 \\cr x \u0026= a z + \\epsilon' \\cr \\Rightarrow y \u0026= b_0 (a z + \\epsilon') + \\epsilon_0 \\cr \u0026= b_0 a z + b_0 \\epsilon' + \\epsilon_0 \\cr \u0026:= b_1 z + \\epsilon_1 \\text{, where $\\epsilon_1 = b_0 \\epsilon' + \\epsilon_0$} \\end{aligned}$$  , from which it is easy to see that $\\cov(\\epsilon_{(i)}, z_{(0)}) \\neq 0$\n From the result of $\\eqref{eq:1}$, we have:\n$$\\begin{align} b_{xy(i)} \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\nocr \u0026= \\frac{b_{zy(0)}r_{i0}\\sqrt{h_0 / h_i}}{\\beta_{zy(0)}\\gamma_{i0}\\sqrt{\\eta_0 / \\eta_i}} \\nocr \u0026= \\frac{b_{zy(0)}}{\\beta_{zy(0)}} \\label{eq:2}\\cr \u0026= b_{xy(0)} \\nocr \\end{align}$$ , where \\eqref{eq:2} follows from the fact that $r_{0i}$ and $\\gamma_{0i}$ are simply the property of the locus 0 and locus i, so that $r_{0i} = \\gamma_{0i}$. The same logic follows for $h$ and $\\eta$.\nThis result implies that $\\hat{d}_{i} := \\hat{b}_{(i)} - \\hat{b}_{(0)}$ has mean zero. Then, under the null, $(\\hat{d}_{1}, ..., \\hat{d}_k) \\approx \\mathcal{N}(0, R)$ (as stated in the text page 9 top left). $R$ is the variance/covariance matrix with entry $\\cov(\\hat{d}_i, \\hat{d}_j)$.\n$$\\begin{align} \\cov(x - y, z - y) \u0026= \\E((x - y)(z - y)) - \\E(x - y)\\E(z - y) \\nocr \u0026= \\E((x - y)(z - y)) - [\\E(x) - \\E(y)][\\E(z) - \\E(y)] \\nocr \u0026= \\E(xz) - \\E(x)\\E(z) - [\\E(yz) - \\E(y)\\E(z)] \\nocr \u0026- [\\E(xy) - \\E(x)\\E(y)] + \\E(y^2) - \\E(y)^2 \\nocr \u0026= \\cov(x, z) - \\cov(y, z) - \\cov(x, y) + \\var(y) \\nocr \\cov(\\hat{d}_i, \\hat{d}_j) \u0026= \\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}) - \\cov(\\hat{b}_{(i)}, \\hat{b}_{(0)}) \\nocr \u0026- \\cov(\\hat{b}_{(j)}, \\hat{b}_{(0)}) + \\var(\\hat{b}_{(0)}) \\label{eq:d} \\end{align}$$ From \\eqref{eq:d} (as stated in equation 8), we need to compute $\\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}), \\forall i, j = 0, 1, ..., k$. The derivation of this term is sketched in the supplement part 3. The missing part seems unclear for me, so I derive it great details as follow.\nThe unclear thing is how to get $\\E(g(\\hat\\theta))$ using Delta method. For $\\var(g(\\hat\\theta))$, it is straight forward, but not so for the first order term because $\\E(g(\\hat\\theta)) \\approx g(\\theta)$ is what we commonly use in practice. But if we take a closer look at Delta method, we will be sure that we can do more than this (see post about Delta method). That is to use higher order approximation.\n$$\\begin{align} g(X) \u0026= g(\\mu) + g'(\\mu)(X - \\mu) + g''(\\mu) \\frac{(X - \\mu)^2}{2!} + o((X - \\mu)^2) \\nocr \\E(g(X)) \u0026= g(\\mu) + \\E(o(X - \\mu)) \\label{eq:3}\\cr \\E(g(X)) \u0026= g(\\mu) + \\frac{g''(\\mu)}{2} \\E((X - \\mu)^2) + \\E(o((X - \\mu)^2)) \\nocr \u0026\\approx g(\\mu) + \\frac{g''(\\mu)}{2} \\var(X) \\label{eq:4} \\end{align}$$ \\eqref{eq:3} is commonly used approximation and \\eqref{eq:4} is a more \u0026ldquo;accurate\u0026rdquo; one. The corresponding multivariate version is simply:\n$$\\begin{align} g(X) \u0026\\approx g(\\mu) + \\frac{1}{2} \\langle H(g(\\mu)), \\Sigma_X \\rangle \\nocr \\end{align}$$ , where $H(g(X))$ is the Hessian of $g$ evaluated at $\\mu$ and $\\Sigma_X$ is the variance/covariance matrix of $X$ and $\\langle \\cdot, \\cdot \\rangle$ is the matrix inner product. Now, we can apply this rule for deriving $\\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)})$.\n$$\\begin{align} \\cov(\\hat{b}_{(i)}, \\hat{b}_{(j)}) \u0026= \\E(\\hat{b}_{(i)} \\hat{b}_{(j)}) - \\E(\\hat{b}_{(i)}) \\E(\\hat{b}_{(j)}) \\nocr \\E(\\hat{b}_{(i)}) \u0026= \\E(\\hat{b}_{xy(i)}) = \\E(\\frac{\\hat{b}_{zy(i)}}{\\hat\\beta_{(zx(i))}}) \\nocr \u0026\\approx \\frac{b_{zy(i)}}{\\beta_{zx(i)}} + \\frac{1}{2} \\langle \\begin{bmatrix} 0 \u0026 -1 / \\beta_{zx(i)}^2 \\cr -1 / \\beta_{zx(i)}^2 \u0026 2b_{zy(i)} / \\beta_{zx(i)}^2 \\end{bmatrix} , \\nocr \u0026 \\begin{bmatrix} \\var(\\hat{b}_{zy(i)}) \u0026 \\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) \\cr \\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) \u0026 \\var(\\hat{\\beta}_{zy(i)}) \\end{bmatrix} \\rangle \\nocr \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} - \\frac{\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}^2} + \\frac{b_{zy(i)}\\var(\\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}^2} \\nocr \u0026= \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\bigg( 1 + \\frac{\\var(\\hat{\\beta}_{zy(i)})}{\\beta_{zx(i)}} - \\frac{\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)})}{b_{zy(i)}\\beta_{zx(i)}} \\bigg) \\label{eq:6} \\end{align}$$ \\eqref{eq:6} is stated in supplement page 10. For an association signal, $\\frac{\\hat\\beta}{\\var(\\hat\\beta)}$ should be big (namely the $\\chi^2$ statistic). Also, $\\cov(\\hat{b}_{zy(i)}, \\hat{\\beta}_{zy(i)}) = 0$. Therefore,\n$$\\begin{align} \\E(\\hat{b}_{xy(i)}) \u0026\\approx \\frac{b_{zy(i)}}{\\beta_{zx(i)}} \\nonumber \\end{align}$$ This result gives nothing more than the first order approximation. But it does matter for $\\E(\\hat{b}_{xy(i)}\\hat{b}_{xy(j)})$. The intuition is that as more and more terms get involved, the first order approximation becomes worse and worse. So, in general, when too many terms involved, you need to be careful. If the (co)variance or Hessian is crazy, maybe it is a good idea to go beyond first order approximation.\nIt turns out that we need to compute $\\cov(\\hat{b}_{zy(i)}, \\hat{b}_{zy(j)})$ and $\\cov(\\hat\\beta_{zx(i)}, \\hat\\beta_{zx(j)})$. In the supplementary notes, it was stated as We know that the sampling correlation between the estimates of SNP effects equals to the LD correlation between the SNPs. But, again, this result is not intuitive to me. The following is a derivation of this:\nSuppose $z$ has been standardized and the true effect size is $b$. Then $y = b z_0 + \\epsilon$ where $z_0$ is the causal variant and $\\epsilon$ is the error term. We have:\n$$\\begin{align} \\hat{b}_{i} \u0026= z_i' y / n \\nocr \u0026= z_i' (b z_0 + \\epsilon) / n \\nocr \\text{similarly, } \\hat{b}_j \u0026= z_j' (b z_0 + \\epsilon) / n \\nocr \\cov(\\hat{b}_{i}, \\hat{b}_j) \u0026= \\frac{1}{n^2} \\big[ \\E(\\cov(z_i' (b z_0 + \\epsilon), z_j' (b z_0 + \\epsilon)| z_i, z_j)) \\nocr \u0026- \\cov(\\E(z_i' (b z_0 + \\epsilon)| z_i) \\E(z_j' (b z_0 + \\epsilon)| z_j)) \\big] \\nocr \\text{first term on RHS} \u0026= z_i' \\cov(bz_0 + \\epsilon, bz_0 + \\epsilon) z_j \\nocr \u0026= b^2 [\\var(z_{0k}) + \\var(\\epsilon)] \\E(z_i' z_j) \\nocr \u0026= A \\cov(z_{ik}, z_{jk}) \\nocr \u0026\\text{, where $z_{i1}, ..., z_{in}$ are iid and same for $j$} \\nocr \u0026 \\text{, and $A = b^2 \\var(z_0) \\var(\\epsilon)$ which does not depend on $i$ and $j$} \\nocr \\E(z_i'(bz_0 + \\epsilon)|z_i) \u0026= z_i' [b\\E(z_0) + \\E(\\epsilon)] = 0 \\nocr \\therefore \\text{second term on RHS} \u0026= 0 \\nocr \\therefore \\cov(\\hat{b}_{i}, \\hat{b}_j) \u0026= A \\cov(z_{ik}, z_{jk}) \\label{eq:sub1}\\cr \\text{a special case is: } \u0026 \\nocr \\var(\\hat{b}_i) \u0026= A \\var(z_{ik}) \\label{eq:sub2} \\cr \\eqref{eq:sub1}, \\eqref{eq:sub2} \\Rightarrow \\cor(\\hat{b}_i, \\hat{b}_j) \u0026= \\cor(z_{ik}, z_{jk})\\nonumber \\end{align}$$  Side note:   Note that \\eqref{eq:sub2} seems conflict with \\eqref{eq:varb}. But the difference is that \\eqref{eq:varb} is for causal variant effect size estimator and \\eqref{eq:sub2} is for the non-causal variants in LD. Intuitively, the association signal captured by non-causal variant comes from LD and only from LD. That\u0026rsquo;s why correlation of $\\hat{b}$ matches exactly to correlation of $z$.\nThis proof stuck me from a long time. The confusing part is that I cannot formalize the underlying model in a proper way. At first, I tried:\n $$\\begin{align} y \u0026= b_i z_i + \\epsilon_i \\nocr z_i \u0026= a z_j + \\epsilon_0 \\nocr \\Rightarrow y \u0026= a b_i z_j + b_i \\epsilon_0 + \\epsilon_i \\nonumber \\end{align}$$  I failed because it turns out that in this case the $A$ term will depend on the error term of $i$ and $j$. Essentially, this is not the correct underlying model of the problem. Neither $z_i$ nor $z_j$ have the real effect size well defined since they are not the causal variant. To make it physically make sense, it is necessary to consider $z_0$, the causal variant, somewhere in the calculation.\nAlso, a useful equation for deriving $\\cov$ is:\n $$\\begin{align} \\cov(f(X, Y), g(X, Z)) \u0026= \\E(\\cov(f(X, Y), g(X, Z)| Y, Z)) \\nocr \u0026- \\cov(\\E(f(X, Y)| Y), \\E(g(X, Z)| Z)) \\nonumber \\end{align}$$  This equation divides the problem into simpler and easier to deal with chunks, especially when $Y$ and $Z$ introduce dependency, which complicates the calculation. This equation can tease such dependency apart and work on them after simplifying the expression a bit.\n After we obtain the distribution of $\\hat{d}_1, ..., \\hat{d}_k$, the normalized version is $z_i := \\hat{d}_i / \\sqrt{\\var(\\hat{d}_i)}$ with normalized (co)variance. The test statistic is constructed as $T_{HEIDI} = \\sum_i z_i^2. The CDF of $T_{HEIDI}$ can be computed approximately (Satterthwaite or Saddlepoint as stated in the text).\nResults in brief In practice, they used blood eQTL data because of its large sample size. For each gene (only probe), they used the top associated cis-eQTL with HEIDI test to filter out linkage cases. For 5 complex traits, they identified 104 significant genes. Some insightful results are listed below:\n They pointed out that the loci which shows trait-associated gene signal (namely passed the tests in the paper) tends to be found in the future GWAS as the sample size increases. Second, SMR helps to pinpoint functionally relevant genes. In GWAS, many locus are close to multiple genes and SMR can distinguish them by considering whether the loci affects the gene expression. eQTL analysis are tissue-specific, but throughout the paper, they used blood eQTL results. They pointed out that many eQTL are shared across tissues, which benefits most by this strategy. But some tissue-specific eQTL is missed unavoidably. They showed that the signals identified in blood data were consistent with the results in brain data for schizophrenia GWAS (brain signal is weak due to power issue). But this indicates that SMR in unmatched tissue does not give fake signal. They did SMR with $x$ as expression in blood and $y$ as expression in brain for significant locus. They showed pleiotropic association as well. Also, the effect size learned from blood data can explain the same amount of variance in brain expression data as brain eQTL analysis did. These evidences showed that variants shared across tissues can be successfully captured by SMR even unrelated tissue eQTLs were used. Multiple tagging probes for a single gene is common in eQTL analysis. Probes tag the transcript and ideally we want every tag of the same gene gives consistent result, but it is not always the case (which might be significantly different from each other). They pointed out that such in-consistency may come from the fact that probes are tagging different types of transcripts from the same gene. But they failed to provide further evidence for this argument and they found that probes were not enriched in region close to transcription end site which is the place enriched for alternative splicing. HEIDI test assumes one causal variant per locus which may not be true in practice and non-pleiotropic variant may dilute the signal. So, they performed conditional analysis to overcome this issue. Not sure how they did this analysis. But I will read and post the note of the referred paper (see link). Added on 12/19/17: The idea of conditional analysis is to remove the effect of secondary variant in the region. Conditioning on the potential secondary variant, the effect size and HEIDI analysis were performed. This analysis can remove dilution effect caused by non-pleiotropic variant.   Side note \u0026ndash; the hypothesis test in 4)   Suppose $\\beta_{i1}, \\beta_{j2}$ are two effect size of $x \\sim z$, where $i, j$ denote SNP index and $1, 2$ denote probe index. LS estimator is \\hat\\beta_{i1} = $z_i' x_1$ and the same for $j2$ for standardized $z_i, z_j, x_1, x_2$. We have $\\var(\\hat\\beta) = n \\var(x) \\var(z)$. And covariance is:\n $$\\begin{align} \\cov(\\hat\\beta_{1i}, \\hat\\beta_{2j}) \u0026= \\E(\\cov(z_i' x_1, z_j' x_2 | z_i, z_j)) - \\cov(\\E(z_i' x_1 | z_i)\\E(z_j' x_2 | z_j)) \\nocr \u0026= \\E(z_i' z_j) \\cov(x_1, x_2) \\nocr \u0026= n \\cov(z_i, z_j) \\cov(x_1, x_2) \\nocr \u0026= r_{ij} r_x \\sqrt{n \\var(x_1) \\var(z_i) \\times n \\var(x_2) \\var(z_j)} \\nocr \u0026= r_{ij} r_x \\sqrt{\\var(\\hat\\beta_{1i}) \\var(\\hat\\beta_{2j})} \\nonumber \\end{align}$$ Discussion HEIDI cannot distinguish linkage and pleiotropy if the two causal variants are in high LD (in this case, the data is not informative). To distinguish causality and pleiotropy, they proposed that multiple independent SNPs would help but it is a bit unclear how to construct the null distribution. Nonetheless, SMR provided a prioritized list of gene candidates for experimental validation. Furthermore, besides eQTL, other quantitative trait can be used, such as meQTL, pQTL, etc.\n"
},
{
	"uri": "/notebook/posts/first-test/",
	"title": "TEMPLATE",
	"tags": ["tag"],
	"description": "",
	"content": " Meta data of reading  Journal: JOURNAL Year: YEAR DOI: DOI  "
},
{
	"uri": "/notebook/about/",
	"title": "About",
	"tags": [],
	"description": "",
	"content": "github repository\nTheme is derived from hugo-theme-cactus-plus.\nThe page is rendered by blogdown\n"
},
{
	"uri": "/notebook/tags/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/biology---research-review/",
	"title": "Biology   Research Review",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/cat/",
	"title": "Cat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/causality/",
	"title": "Causality",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/complex-trait/",
	"title": "Complex Trait",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/conditional-analysis/",
	"title": "Conditional Analysis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/contour/",
	"title": "Contour",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/epistasis/",
	"title": "Epistasis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/eqtl/",
	"title": "Eqtl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/genetic-architecture/",
	"title": "Genetic Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/genetics/",
	"title": "Genetics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/gwas/",
	"title": "Gwas",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/hypothesis-testing/",
	"title": "Hypothesis Testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/integrative-analysis/",
	"title": "Integrative Analysis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/linkage-disequilibrium/",
	"title": "Linkage Disequilibrium",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/mendelian-randomization/",
	"title": "Mendelian Randomization",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/method-note/",
	"title": "Method Note",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/multivariate-normal/",
	"title": "Multivariate Normal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/neyman-pearson-lemma/",
	"title": "Neyman Pearson Lemma",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/",
	"title": "Note Archive",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/posts/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/research-paper---biology/",
	"title": "Research Paper   Biology",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/research-paper---method/",
	"title": "Research Paper   Method",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/categories/rscript/",
	"title": "Rscript",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/score-test/",
	"title": "Score Test",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/simulation/",
	"title": "Simulation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/tag/",
	"title": "Tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/target-gene/",
	"title": "Target Gene",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notebook/tags/type-2-diabetes/",
	"title": "Type 2 Diabetes",
	"tags": [],
	"description": "",
	"content": ""
}]